<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">

<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">


  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "e77a7f88"
    });
  daovoice('update');
  </script>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Reinforcement Learning,Artificial Intelligence," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=5.1.2" />






<meta name="description" content="Chapter 1: Introduction人类与环境进行互动，学习环境如何响应我们的行为，并试图通过自身行为影响将来发生的事，这就是一种交互式的学习方式，是人类获取知识的主要来源，同时也是几乎所有学习和智能化理论的基本思想。强化学习正是一种从交互中学习的计算方法，它更侧重于从交互中进行目标导向的学习方式，而不是其他的机器学习方式。">
<meta name="keywords" content="Reinforcement Learning,Artificial Intelligence">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning：An Introduction Chapter 1 学习笔记">
<meta property="og:url" content="http://yoursite.com/2018/09/19/Reinforcement Learning：An Introduction Chapter1 学习笔记/index.html">
<meta property="og:site_name" content="KunBB&#39;s blog">
<meta property="og:description" content="Chapter 1: Introduction人类与环境进行互动，学习环境如何响应我们的行为，并试图通过自身行为影响将来发生的事，这就是一种交互式的学习方式，是人类获取知识的主要来源，同时也是几乎所有学习和智能化理论的基本思想。强化学习正是一种从交互中学习的计算方法，它更侧重于从交互中进行目标导向的学习方式，而不是其他的机器学习方式。">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://raw.githubusercontent.com/KunBB/MarkdownPhotos/master/RLAI_C1/2.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/KunBB/MarkdownPhotos/master/RLAI_C1/1.jpg">
<meta property="og:updated_time" content="2018-09-19T10:25:49.340Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reinforcement Learning：An Introduction Chapter 1 学习笔记">
<meta name="twitter:description" content="Chapter 1: Introduction人类与环境进行互动，学习环境如何响应我们的行为，并试图通过自身行为影响将来发生的事，这就是一种交互式的学习方式，是人类获取知识的主要来源，同时也是几乎所有学习和智能化理论的基本思想。强化学习正是一种从交互中学习的计算方法，它更侧重于从交互中进行目标导向的学习方式，而不是其他的机器学习方式。">
<meta name="twitter:image" content="https://raw.githubusercontent.com/KunBB/MarkdownPhotos/master/RLAI_C1/2.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/09/19/Reinforcement Learning：An Introduction Chapter1 学习笔记/"/>





  <title>Reinforcement Learning：An Introduction Chapter 1 学习笔记 | KunBB's blog</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d76fd55ad2c6a913abb69c04b4c9aadf";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">KunBB's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">The only time you should ever look back is to see how far you've come.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
			
		  
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/19/Reinforcement Learning：An Introduction Chapter1 学习笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yunkun Xu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1505221430290&amp;di=5d6a88d013890d4f520618b024b4aaed&amp;imgtype=0&amp;src=http%3A%2F%2Fimg3.duitang.com%2Fuploads%2Fitem%2F201601%2F03%2F20160103085338_eyBCL.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="KunBB's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Reinforcement Learning：An Introduction Chapter 1 学习笔记</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-19T09:10:00+08:00">
                2018-09-19
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Reinforcement Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2018/09/19/Reinforcement Learning：An Introduction Chapter1 学习笔记/" class="leancloud_visitors" data-flag-title="Reinforcement Learning：An Introduction Chapter 1 学习笔记">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Heat&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
				 <span>℃</span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  3,606
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  13
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><strong>Chapter 1: Introduction</strong><br>人类与环境进行互动，学习环境如何响应我们的行为，并试图通过自身行为影响将来发生的事，这就是一种交互式的学习方式，是人类获取知识的主要来源，同时也是几乎所有学习和智能化理论的基本思想。强化学习正是一种从交互中学习的计算方法，它更侧重于从交互中进行目标导向的学习方式，而不是其他的机器学习方式。<br><a id="more"></a></p>
<h1 id="1-1-Reinforcement-Learning"><a href="#1-1-Reinforcement-Learning" class="headerlink" title="1.1 Reinforcement Learning"></a>1.1 Reinforcement Learning</h1><h2 id="强化学习特征"><a href="#强化学习特征" class="headerlink" title="强化学习特征"></a>强化学习特征</h2><p>强化学习就是学习该做什么，如何将情境映射到动作从而最大化奖励信号。试错搜索（trial-and-error search）和延迟奖励（delayed reward）是强化学习两个最重要的显著特征，另一个重要特征是强化学习并不局限于孤立的子问题，即：<br>· 学习者不会被告知需要采取哪些行动，而是必须通过尝试来发现哪些行动可以产生最大的回报；<br>· 当前行动不仅影响即时奖励，还会影响下一个state，以及后续奖励；<br>· 明确考虑了目标导向的agent与不确定环境交互的整个问题。</p>
<h2 id="强化学习与其他人工智能技术的区别"><a href="#强化学习与其他人工智能技术的区别" class="headerlink" title="强化学习与其他人工智能技术的区别"></a>强化学习与其他人工智能技术的区别</h2><p><strong>监督学习</strong>：是从一组有标记的训练集中进行学习，目的是让系统归纳与推断其响应，使其在训练集中不存在的样例下也能正确做出相应action。监督学习是一种重要的学习方式，但其不足以从交互中学习。在交互问题中获取正确而又代表所有情况的所期望行为的样例是不切实际的。在未知领域，agent必须能够从自身经验中学习才能习得最有益的action。</p>
<p><strong>非监督学习</strong>：通常是寻找隐藏在未标记数据集合中的某种结构。虽然强化学习也不需要带有正确标记的例子，但它的目标是最大化奖励信号，而不是试图找到隐藏的结构。当然，找到agent学习经验中的隐藏结构也是有用的，但这并不是最终目标。</p>
<h2 id="强化学习的挑战"><a href="#强化学习的挑战" class="headerlink" title="强化学习的挑战"></a>强化学习的挑战</h2><p><strong>探索与开发的权衡（trade-off between exploration and exploitation）</strong>。为了获得大量奖励，agent必须更倾向于过去尝试过的行为，并且发现他们能够有效地产生奖励。但是要发现这样的行为，agent必须尝试以前没有尝试过的行为，它必须利用它已经经历的经验来获得奖励，但也必须进行探索，以便在将来做出更好的选择。困难在于，任何探索和开发都有可能会失败，agent必须尝试各种操作，并逐渐倾向于那些看起来最好的操作。在随机任务中，必须多次尝试每一个action以获得对其期望奖励的可靠估计。</p>
<h1 id="1-3-Elements-of-Reinforcement-Learning"><a href="#1-3-Elements-of-Reinforcement-Learning" class="headerlink" title="1.3 Elements of Reinforcement Learning"></a>1.3 Elements of Reinforcement Learning</h1><p><strong>策略（policy）</strong>：策略定义了agent在给定时间内的行为方式。策略是从感知的环境状态到该state下action的映射。通常策略可以是随机的，指定每个action的概率。</p>
<p><strong>奖励信号（reward signal）</strong>：奖励信号定义了强化学习问题的目标，是agent一次action后的反馈，说明了agent某个action对于目标而言是有利的还是有害的。奖励信号是更改策略的基础，如果回报低，下次遇到相同的情况，agent就会采取不同的action。agent唯一的目标是最大化累计获得的奖励。</p>
<p><strong>值函数（value function）</strong>：state的值表示agent以该state为起点，未来可期望的各个state回报的总和。奖励信号表示该state直接意义上的好坏，但值函数表示了以该state为起点，长期运行中的好坏。我们寻求的action应该是带来最高value而非最高reward。</p>
<p><strong>环境模型（model of the environment）</strong>：利用models来解决强化学习的方法为model-based method，反之叫做model-free method。对环境进行建模，不必在真实环境中试验每一action，给定state和action，model会给出下一个state和返回的reward，极大减小了试错搜索的成本，是未来新的发展方向。</p>
<h1 id="1-4-Limitations-and-Scope"><a href="#1-4-Limitations-and-Scope" class="headerlink" title="1.4 Limitations and Scope"></a>1.4 Limitations and Scope</h1><p><strong>进化方法（evolutionary methods）</strong>：如果策略空间较小，或可以被结构化（好的策略容易被检索到），或者有大量时间可以用于搜索，则进化方法是可行的。此外，进化方法在agent无法感知其环境的完整状态的问题上具有优势。</p>
<p><strong>强化学习方法</strong>：进化方法（EM）只看policy的最后结果而不考虑中间的演变的过程。而强化学习方法在与环境的交互中学习，许多情况下，可以利用个体行为相互作用的细节。进化方法忽略了强化学习问题的许多有用结构：EM没有利用所搜索的policy是states到actions的映射这个事实；EM没有注意到agent一生经过了哪些states，选择了哪些actions。虽然某些情况下，该信息可能具有误导性（例如state被误观察），但更一般的，这些信息会带来更高效的搜索。</p>
<h1 id="1-5-An-Extended-Example-Tic-Tac-Toe"><a href="#1-5-An-Extended-Example-Tic-Tac-Toe" class="headerlink" title="1.5 An Extended Example: Tic-Tac-Toe"></a>1.5 An Extended Example: Tic-Tac-Toe</h1><h2 id="优化方法对比"><a href="#优化方法对比" class="headerlink" title="优化方法对比"></a>优化方法对比</h2><p>以“井”字游戏为例说明了传统的AI方法如minimax、dynamic programming、evolutionary method都不太适合即使是这么简单的RL问题。  </p>
<p>经典的博弈论的<strong>minimax</strong>解决方案在这里是不正确的，因为它假定了对手的特定玩法。</p>
<p>用于顺序决策问题的经典优化方法，例如<strong>dynamic programming</strong>，可以为任何对手计算最优解，但需要输入该对手的完整规范，包括对手在每个棋盘状态下进行每次移动的概率。</p>
<p>为了评估策略,<strong>进化方法</strong>使得策略固定并且针对对手玩许多次游戏，或者使用对手的模型模拟许多次游戏。胜利的频率给出了对该策略获胜概率的无偏估计，并且可用于指导下一个策略的选择。<strong>但是每次策略更改都是在许多游戏之后进行的，并且只使用每个游戏的最终结果：在游戏期间发生的事情会被忽略</strong>。例如，如果玩家获胜，那么游戏中的所有行为都会被信任，而不管具体哪些actions对获胜至关重要，甚至可以归功于从未发生过的actions。</p>
<p>相反<strong>值函数方法</strong>允许评估各个states。最后，进化和值函数方法都在搜索策略空间，但学习值函数会利用游戏过程中可用的信息。</p>
<h2 id="value-function方法步骤"><a href="#value-function方法步骤" class="headerlink" title="value function方法步骤"></a>value function方法步骤</h2><ol>
<li>建立数据表，每个数据都代表游戏中的一个可能state，每个数字都是我们从该state获胜概率的最新估计；</li>
<li>假设我们总是玩X，那么连续三个X的value是1，连续三个O的value为0，其他状态的初始值设置为0.5，表示我们有50％的获胜机会；</li>
<li>进行多场游戏，大多数时候我们采用贪婪式方法，选择导致具有最大value的state移动，即具有最高的估计获胜概率。但偶尔也会采取随机下法即探索性动作；<br><img src="https://raw.githubusercontent.com/KunBB/MarkdownPhotos/master/RLAI_C1/2.jpg" alt="Loading..."></li>
<li>在贪婪选择时，使用时间差分法（temporal-di↵erence）更新之前state的value：<br><img src="https://raw.githubusercontent.com/KunBB/MarkdownPhotos/master/RLAI_C1/1.jpg" alt="Loading..."><br>α为步长，S为state，V()为value。</li>
<li>可以通过改变α慢慢趋向于0使得这个方法收敛到一个最优策略；也可以不改变α使得策略不断改变以对抗对手。</li>
</ol>
<h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><p>本小节引出如下几点思考：</p>
<ol>
<li>将先验知识应用到强化学习中可以改善学习效果；</li>
<li>强化学习的动作，除了像这个游戏这种离散的，也可能是连续的，value函数也可能是连续函数；</li>
<li>监督学习为程序提供了从其经验中概括（泛化）的能力。因此当状态集非常大甚至无限时，将监督学习方法与强化学习方法相结合是一个很好的解决途径。ANN和DL并不是唯一的或最好的方法；</li>
<li>如果能够获得或构建一个环境模型，则强化学习效果与效率会更好更高。</li>
</ol>
<h1 id="1-7-Early-History-of-Reinforcement-Learning"><a href="#1-7-Early-History-of-Reinforcement-Learning" class="headerlink" title="1.7 Early History of Reinforcement Learning"></a>1.7 Early History of Reinforcement Learning</h1><p>本小节讲述了RL的三条研究主线：</p>
<ol>
<li>learning with trial and error；</li>
<li>optimal control and its solution using value functions and dynamic programming(planning)；</li>
<li>TD-methods。</li>
</ol>
<h1 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h1><p><strong>Exercise 1.1</strong>：<strong>Self-Play</strong> Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?<br>译：假设上述强化学习算法不是与随机对手比赛，而是双方都在学习。 在这种情况下你认为会发生什么？ 是否会学习选择行动的不同策略？  </p>
<font color="red">答：对于固定的对手来说，算法可能是次优的，对于随机对手而言，算法可能是最优的。对抗式学习和足够的探索相较于一个固定的对手可以产生更为强大的智能体。最后两个智能体应该会达到某种动态平衡，或是某方一直输，另一方一直赢（初始动作或顺序可能影响了学习策略）。</font>

<p><strong>Exercise 1.2</strong>：<strong>Symmetries</strong> Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the learning process described above to take advantage of this? In what ways would this change improve the learning process? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?<br>译：许多井字位置看起来不同，但由于对称性，它们实际上是相同的。我们如何修改上述学习过程以利用这一点？这种变化会以何种方式改善学习过程？ 现在再想一想。假设对手没有利用对称性。在那种情况下，我们应该吗？那么，对称等价的位置是否必须具有相同的值？</p>
<font color="red">答：可以依据4个轴的对称性对状态空间进行约减，即对称移动视为属于相同的状态空间，进而将减少实际的状态数量，加速学习。如果对手没有利用对称性，则其策略会区分“对称”状态，这可能导致我们算法整体性能变差。例如，如果对手在一个状态空间中存在弱点但在另一个状态空间中没有（即使它们是对称的），则对称相似的状态应该具有相同的值是不正确的，因此，这种情况下我们也不应该使用对称性。</font>

<p><strong>Exercise 1.3</strong>：<strong>Greedy Play</strong> Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a nongreedy player? What problems might occur?<br>译：假设强化学习者是贪婪的，也就是说，它总是做出能够把它带到它认为最好的位置的动作。它可能会比一个不贪婪的学习者学的更好或更差吗？可能会出现什么问题？</p>
<font color="red">答：一般而言，贪婪玩家的学的可能会更差。贪婪玩家会追求最大的即时reward，而好的学习策略应该是追求最大的value，即累积回报。如果每一步都追求最好的动作，我们可能永远找不到最优解。贪婪玩家可能会陷入局部最优点。</font>

<p><strong>Exercise 1.4</strong>：<strong>Learning from Exploration</strong> Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a different set of probabilities. What (conceptually) are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?<br>译：假设在所有动作之后发生了学习更新，包括探索性动作。 如果步长参数随时间适当减小（但不是探索的趋势），则状态值将收敛到不同的概率集。 什么（概念上）是我们这样做时计算的两组概率，当我们不这样做时，从探索性动作中学习？ 假设我们继续做出探索性的动作，哪一组概率可能更好学习？ 哪会赢得更多？</p>
<font color="red">答：一个状态的值是从一个状态开始直到获胜的可能性。随着步长的适当减少，且假设探索的概率是固定的，没有从探索中学习的概率集是给定从此采取的最佳行动的每个状态的值，而从探索中学习的概率集是包括主动探索策略在内的每个状态的期望值。使用前者能够更好地学习，因为它避免了算法一味地进行贪婪式的行动，可能到达一个一般来说我们永远不会到达的状态，进而减少了次优的未来状态的偏差（例如，如果你可以在一次移动中赢得一盘棋，但如果你执行另一次移动你的对手获胜，这不会使该状态变坏）。前者会在所有其他条件相同的情况下获得更多胜利。</font>

<p><strong>Exercise 1.5</strong>：<strong>1.5    Other Improvements</strong> Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?<br>译：你能想到其他改善强化学习者的方法吗？你能想出更好的方法来解决所提出的井字游戏问题吗？</p>
<font color="red">答：根据对手行为的变化改变探索率。加大损失的惩罚力度。</font>

<hr>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h1><p>[1] 《reinforcement learning：an introduction》第一章《The Reinforcement Learning Problem》总结：<a href="https://blog.csdn.net/mmc2015/article/details/74931291" target="_blank" rel="external">https://blog.csdn.net/mmc2015/article/details/74931291</a><br>[2] 强化学习经典入门书的读书笔记系列—第一篇：<a href="https://zhuanlan.zhihu.com/p/27133367" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/27133367</a><br>[3] Reinforcement Learning：An Introduction 读书笔记- Chapter 1：<a href="https://blog.csdn.net/PeytonPu/article/details/78450681" target="_blank" rel="external">https://blog.csdn.net/PeytonPu/article/details/78450681</a><br>[4] rl-book-exercises：<a href="https://github.com/jlezama/rl-book-exercises" target="_blank" rel="external">https://github.com/jlezama/rl-book-exercises</a></p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>Donate comment here</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/wechatpay.jpg" alt="Yunkun Xu WeChat Pay"/>
        <p>WeChat Pay</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/alipay.jpg" alt="Yunkun Xu Alipay"/>
        <p>Alipay</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
          
            <a href="/tags/Artificial-Intelligence/" rel="tag"># Artificial Intelligence</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/13/Qt+webservice的多线程实现/" rel="next" title="Qt+webservice的多线程实现">
                <i class="fa fa-chevron-left"></i> Qt+webservice的多线程实现
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zMDg2NC83NDEz"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          
            <img class="site-author-image" itemprop="image"
              src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1505221430290&amp;di=5d6a88d013890d4f520618b024b4aaed&amp;imgtype=0&amp;src=http%3A%2F%2Fimg3.duitang.com%2Fuploads%2Fitem%2F201601%2F03%2F20160103085338_eyBCL.jpeg"
              alt="Yunkun Xu" />
          
            <p class="site-author-name" itemprop="name">Yunkun Xu</p>
            <p class="site-description motion-element" itemprop="description"></p>
        </div>

        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">24</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-1-Reinforcement-Learning"><span class="nav-number">1.</span> <span class="nav-text">1.1 Reinforcement Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#强化学习特征"><span class="nav-number">1.1.</span> <span class="nav-text">强化学习特征</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#强化学习与其他人工智能技术的区别"><span class="nav-number">1.2.</span> <span class="nav-text">强化学习与其他人工智能技术的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#强化学习的挑战"><span class="nav-number">1.3.</span> <span class="nav-text">强化学习的挑战</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-3-Elements-of-Reinforcement-Learning"><span class="nav-number">2.</span> <span class="nav-text">1.3 Elements of Reinforcement Learning</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-4-Limitations-and-Scope"><span class="nav-number">3.</span> <span class="nav-text">1.4 Limitations and Scope</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-5-An-Extended-Example-Tic-Tac-Toe"><span class="nav-number">4.</span> <span class="nav-text">1.5 An Extended Example: Tic-Tac-Toe</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#优化方法对比"><span class="nav-number">4.1.</span> <span class="nav-text">优化方法对比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#value-function方法步骤"><span class="nav-number">4.2.</span> <span class="nav-text">value function方法步骤</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#扩展"><span class="nav-number">4.3.</span> <span class="nav-text">扩展</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-7-Early-History-of-Reinforcement-Learning"><span class="nav-number">5.</span> <span class="nav-text">1.7 Early History of Reinforcement Learning</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Exercise"><span class="nav-number">6.</span> <span class="nav-text">Exercise</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">7.</span> <span class="nav-text">Reference:</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

<div class="powered-by">
 | 
<i class="fa fa-bullseye"></i><span id="busuanzi_container_site_pv">
  本站总访问量<span id="busuanzi_value_site_pv"></span>次
</span>
</div>

<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yunkun Xu</span>

  
</div>





        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  








  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("QhaBjkfW4URLRbhgJHxDwlLS-gzGzoHsz", "lMWD4RO6uM9qzr8tTOVVb70w");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  

  

  
  


  

  

  
  
</body>
</html>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>