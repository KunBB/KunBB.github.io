<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CS231n Lecture 13 Unsupervised Learning and Generative Models]]></title>
    <url>%2F2018%2F10%2F09%2FCS231n%20Lecture%2013%20Unsupervised%20Learning%20and%20Generative%20Models%2F</url>
    <content type="text"><![CDATA[Lecture 13主要讲解了无监督模型和生成模型，其中详细介绍了生成模型中的pixelRNN、pixelCNN、VAE、GAN等图像生成方法。 Unsupervised Learning监督式学习我们都很熟悉了，我们有数据x和标签y，我们在的目的是学习到一个函数可以将数据x映射到标签y，标签可以有很多形式。 无监督学习要做的就是在我们拥有的只是一些没有标签的训练数据的情况下学习一些数据中潜在的隐含结构。 典型的无监督学习有一下几种： 聚类聚类的目标是找到数据中的分组，组内的数据在某种度量方式的比较下是相似的。 降维降维的目标是找出一些轴，在这些轴向上训练数据的方差最大。这些轴向就是数据潜在结构的一部分。我们可以用这些轴来减少数据维度，数据在每个保留下来的维度上都有很大的方差。 学习数据的特征表达在监督式方法中，如分类任务中我们使用了监督式的损失函数，我们有数据标签，我们可以训练一个神经网络，我们可以把激活函数理解为数据的某种未来表征。而在无监督学习中，如自动编码器，损失函数的目标是重构输入数据，然后通过这个重构来学习表征。 密度估计此项任务中我们想要估计数据的内在分布情况。如上图1中我们有一些一维的点，我们尝试用高斯函数来拟合这一密度分布情况。如上图2，是一个二维数据，我们尝试估计密度分布，并且可以为该密度函数建模。 Generative Models生成模型的任务是给定训练数据，从相同的数据分布中生成新的样本。我们有一些训练数据，这些训练数据是由某种分布p-data中生成的，我们想从中学习到一个模型p-model来以相同的分布生成样本，即我们希望p-model能学的和p-data尽可能地相似。 生成模型可以解决密度估计问题。我们可以使用生成式模型来做显式的密度估计，此种情况我们会求解出目标模型p-model。或者我们也可以进行隐式的密度估计，这种情况下我们会习得一个能够从p-model中生成样本的模型而不需要显式地定义它。 生成模型可以从数据分布中创造出我们想要的真实样本。上图左边为生成的图片，中间为生成的人脸，除此之外还可以做超分辨率或者着色之类的任务。另外，我们还可以用关于时间序列数据的生成模型来进行仿真和规划，这样一来就能在强化学习应用中派上用场。同时训练生成式模型也能使得隐式表征的推断成为可能。 生成模型分类： PixelRNN and PixelCNN这些都属于全可见信念网络。目的是对一个密度分布显示建模。我们有图像数据$x$，我们希望对该图像的概率分布或者似然$p(x)$建模。于是我们使用链式法则将这一似然分解为一维分布的乘积。我们有每个像素$x_i$的条件概率，其条件是给定所有下标小于$i$的像素（$x_1$ ~ $x_{i-1}$）。此时图像中所有像素的概率或者联合概率就是所有这些像素点，这些似然的乘积。一旦我们定义好这一似然，为了训练这一模型，我们只要在该定义下最大化我们的训练数据的似然。 那么如果我们观察下右边的像素值概率分布，可以发现这是一个十分复杂的分布。我们之前已经了解到如果想要进行一些复杂的变换，我们可以利用神经网络来实现这一映射。 PixelRNN该方法从左上角像素开始，一个接一个地生成像素，顺序如图所示。序列中每一个对之前像素的依赖关系都会通过LSTM来建模。 缺点：这种方法是顺序生成的，速度会很慢。 PixelCNN我们依然是从角落开始并展开，进而生成整张图片。区别在于，现在使用CNN替代RNN来对所有这些依然关系建模。 现在我们打算在环境区域（图示中间这个特定像素点的附近区域）上使用CNN，生成该区域包围的这个像素。取待生成像素点周围的像素（已生成像素区域内的灰色区域），把他们传递给CNN用来生成下一个像素值。每一个像素位置都有一个神经网络输出，该输出将会是像素的softmax损失值。我们通过最大化训练样本图像的似然来训练模型，在训练的时候取一张训练图像来执行生成过程，每个像素位置都有正确的标注值，即训练图片在该位置的像素值，该值也是我们希望模型输出的值。 Q：这里用到了标签值，为何还是无监督学习？A：我们并没有为了训练图像而收集数据，我们只是将输入数据作用于模型末端函数。 PixelCNN训练要比pixelRNN更快，因为在每一个像素位置，我们想要最大化我们已有的训练数据的似然，我们已经有了所有的值，这些值来自训练数据，所以我们可以训练的更快，但在测试时的生成环节，我们想要从第一个像素生成一个全新的图像，我们并不需要在生成时做任何学习，只需逐一生成像素点，因此生成时间仍然很慢。 PixelRNN和pixelCNN能显示地计算似然$p(x)$，这是一种可以优化的显式密度模型。该方法同时给出了一个很好的评估度量，你可以通过你所能计算的数据的似然来度量出你的样本有多好，同时这些方法能够生成相当不错的样本。 这些方法的主要缺陷在于，由于生成过程是序列化的，因此速度上会很慢。 Variational Autoencoders (VAE)PixelCNN定义了一个易于处理的密度函数，我们可以直接优化训练数据的似然。对于变分自编码器我们将定义了一个不易处理的密度函数，现在我们通过附加的隐变量$z$对密度函数进行建模。我们数据的似然$p(x)$是等式右边的积分形式，即对所有可能的$z$值取期望，我们无法直接优化它，我们只能找出一个似然函数的下界然后再对该下界进行优化。 Autoencoders变分自编码器与自动编码器的无监督模型息息相关。我们不通过自动编码器来生成数据，它是一种利用无标签数据来学习低维特征表示的无监督学习。我们有输入数据$x$，我们想要学习一些特征$z$，我们会有一个编码器进行映射，来实现从该输入数据到特征$z$的映射。该编码器可以有多种不同的形式，常用的是神经网络。最先提出的是非线性层的线性组合，又有了更深的全连接网络，又出现了CNN。我们取得输入数据$x$然后将其映射到某些特征$z$，我们通常将$z$限制在比$x$更小的维度上，由此可以实现降维。降维的目的是希望捕捉到的是重要的特征。 自动编码器将该模型训练成一个能够用来重构原始数据的模型。我们用编码器将输入数据映射到低维的特征$z$（也就是编码器网络的输出），同时我们想获得这些基于输入数据得到的特征，然后用第二个网络也就是解码器网络输出一些跟$x$有相同维度并和$x$相似的东西，也就是重构原始数据。对于解码器，我们一般使用和编码器相同类型的网络（通常与编码器对称）。 流程是我们取得输入数据，把它传给编码器网络（比如一个四层的卷积网络），获取输入数据的特征，把特征传给解码器（四层的解卷积网络），在解码器末端获得重构的数据。 至于为何选用卷积网络作为编码器而用解卷积网络作为解码器。事实上对于解码器来说，这是因为在编码器那里高维的输入被映射到低维的特征，而现在我们需要反其道而行。也就是从低维特征回到高维的重构输入（这里可以参考一下Lecture 11中关于反卷积的讲解）。 为了能够重构输入数据的效果，我们使用类似L2损失函数，让输入数据中的像素与重构数据中的像素相同。这里虽然有损失函数，但我们没有使用任何外部标签来训练模型。 一旦我们训练好模型，我们可以去掉解码器。这么做是为了生成重构的输入信息，同时为了计算损失函数。 使用训练好的编码器实现特征映射，而我们可以利用此特性来初始化一个监督式模型。通过编码器得到输入数据的特征，顶部有一个分类器，如果是分类问题我们可以用它来输出一个类标签，在这里使用了外部标签和标准的损失函数如softmax。 这么做的价值在于我们可以用很多无标签数据来学习到很多普适特征表征。可以用学习到的特征表征来初始化一个监督学习问题，因为在监督学习的时候可能只有很少的有标签训练数据，而少量的数据很难训练模型，可能会出现过拟合等其他一些问题，通过使用上面得到的特征可以很好地初始化网络。 我们已经见识了自动编码器重构数据，学习数据特征，初始化一个监督模型的能力。这些学习到的特征同时也具有能捕捉训练数据中蕴含的变化因素的能力。我们获得了一个含有训练数据中变化因子的隐变量$z$。 那么我们能用自动编码器生成新的图像吗？ Variational Autoencoders这是通过向自编码器中加入随机因子获得的一种模型，这样我们就能从该模型中采样从而生成新的数据。我们有训练数据$x_i$（$i$的范围从1到N），数据$x$是从某种潜在的不可观测的隐式表征$z$中生成的。$z$的元素要捕捉的信息是训练数据中某种变化因子的多少，是某种类似于属性的东西。例如我们想要生成微笑的人脸，$z$代表的就是脸上有几分笑意、眉毛的位置、嘴角上扬的弧度等。 生成过程：从$z$的先验分布中采样，对于每种属性，我们都假设一个我们觉得它应该是一个怎样的先验分布。高斯分布就是一个对$z$中每个元素的一种自然的先验假设。同时我们会通过从在给定z的条件下，$x$的条件概率分布$p(x|z)$中采样。先对$z$采样，也就是对每个隐变量采样，接下来利用它对图像$x$采样。 对于上述采样过程，真实的参数是$\theta^*$，我们有关于先验假设和条件概率分布的参数，我们的目的在于获得一个生成式模型，从而利用他来生成新的数据，真实参数中的这些参数是我们想要估计并得出的。 如何表述上述模型：对上述过程建模，选一个简单的关于$z$的先验分布，例如高斯分布。对于给定$z$的$x$的条件概率分布$p(x|z)$很复杂，所以我们选择用神经网络来对$p(x|z)$进行建模。 我们想要训练好该模型，这样一来就能习得一个对于这些参数的估计。训练网络时，我们需要调用解码器网络，选取隐式特征并将其解码为它所表示的图像。一个直接且自然的训练策略是通过最大化训练数据的似然函数来寻找这些模型的参数。在已经给定隐变量z的情况下，我们需要写出$x$的分布$p$并对所有可能的$z$值取期望，因为z值是连续的所以表达式是一个积分。 现在我们想要最大化它的似然，但发现这一积分很难求解。 Variational Autoencoders: Intractability似然项的第一项是$z$的分布$p(z)$，这里如前所述，它可以被直接设定为高斯分布。对于$p(x|z)$我们之前说要指定一个神经网络解码器，任意给定一个$z$，我们就能获得$p(x|z)$。如果我们想要对每一个z值计算$p(x|z)$是很困难的，我们无法计算该积分。似然难解直接导致了模型的其他项，如后验概率$p(z|x)$也是难解的。 因此我们无法直接进行优化。一个可以让我们训练该模型的办法是，如果在使用解码器网络来定义一个对$p(x|z)$建模的神经网络的同时，额外定义一个编码器$q(z|x)$，将输入$x$编码为$z$，从而得到似然$p(z|x)$。也就是说我们定义该网络来估计出$p(z|x)$，这个后验密度分布项仍然是难解的，我们用该附加网络来估计该后验分布，这将使我们得到一个数据似然的下界，该下界易解也能优化。 与自编码器类似，在变分自编码器中我们想得到一个生成数据的概率模型，将输入数据$x$送入编码器得到一些特征$z$，然后通过解码器网络把z映射到图像$x$。 我们这里也有编码器网络和解码器网络，但是我们要将一切参数随机化。参数是$\phi$的编码器网络$q(z|x)$输出一个均值和一个对角协方差矩阵；解码器网络$p(x|z)$输入$z$，输出均值和关于$x$的对角协方差矩阵。 为了得到给定$x$下的$z$和给定$z$下的$x$，我们会从这些分布（$p$和$q$）中采样。现在我们的编码器和解码器网络所给出的分别是$z$和$x$的条件概率分布，并从这些分布中采样从而获得值。 编码器网路也是一种识别或推断网络，因为是给定$x$下对隐式表征$z$的推断；解码器网络执行生成过程，所以也叫生成网络。 Variational Autoencoders: Solution process既然已经有了编码器和解码器网络，现在我们求解数据似然，这里我们会使用对数似然。如果我们想获得$\log(p(x))$，我们要对其关于$z$取期望，z是采样自分布$q(z|x)$，也就是我们通过编码器网络定义的分布。我们之所以这么做是因为$p(x)$并不依赖于$z$。 根据贝叶斯公式，从这一原始的表达式开始，我们可以将其展开为上式。下面为了求解它，我们可以再乘一个常数。这样一来，我们要做的就是把上式写成三项之和的形式，这三项都是有意义的。仔细观察，会发现第一项是$\log(p(x|z))$关于$z$取期望，接下来有两个KL散度项，用来描述这两个分布有多么相似，也就是$q(z|x)$和$p(z)$有多相似，是对分布函数距离的度量。再仔细观察一下。第一项是$p(x|z)$，是由解码器网络提供的。同时我们能够通过采样计算并估计出这些项的值，而且我们还会看到我们能够通过一种叫做重参数化的技巧来进行一次可微分的采样。 第二个KL项是两个高斯分布之间的KL散度。$q(z|x)$是由我们的编码器网络生成的一个性质很好的高斯分布（由编码器生成的均值和协方差构成）。同样，我们的先验假设$p(z)$也是一个高斯分布。我们有一个关于两个高斯分布的KL散度就等于我们获得了一个很好的闭式解。 第三个KL散度是关于$q(z|x)$和$p(z|x)$。我们知道$p(z|x)$是一个难解的后验概率，这一项依然是个问题。但是KL散度是对两个分布之间距离的度量，从定义上看它总是大于等于0。 因此我们能做的是在这里前两项可以很好地求解，而第三项一定大于等于0。因此，前两项合起来就是一个可以求解的下界，我们就可以对其取梯度并进行优化。 那么为了训练一个变分自编码器，我们需要优化并最大化这一下界，因此我们是在优化对数似然的下界。 另一个对于下界的解释是，第一项是对所有采样的$z$取期望，$z$是$x$经过编码器网络采样得到的，对$z$采样然后再求所有$z$对应的$p(x|z)$。让$p(x|z)$变大，就是最大限度地重构数据。第二项是让KL的散度变小，让我们的近似后验分布和先验分布变得相似，意味着我们想让隐变量$z$遵循我们期望的分布类型和形状。 Q：为何将先验分布即隐变量分布设定为高斯分布？A：我们是在定义某种生成过程，该过程首先要对$z$采样然后对$x$采样。把它假设为高斯分布是因为这是一种合理的先验模型，对于隐变量的属性来说，分布成某种高斯分布式讲得通的，而且这么做可以让我们接下来能够优化模型。 Variational Autoencoders: Training process左上角的公式就是我们要优化及最大化的下界。现在对于前向传播而言我们要按下面的流程处理。 我们有输入数据$x$； 让输入数据传递经过编码器网络，得到$q(z|x)$； 通过$q(z|x)$来计算公式中的KL项； 根据给定$x$的$z$分布对$z$进行采样，由此获得了隐变量的样本； 把$z$继续传给解码器网络，得到$x$在给定$z$条件下的分布的两个参数（均值和方差）； 最终可以在给定$z$的条件下从这个分布中采样获得$x$，并会产生一些样本输出。 在训练的时候，我们就是要获得该分布，而我们的损失项将会是给定$z$条件下对训练像素值取对数。那么我们的损失函数要做的就是最大化被重构的原始输入数据的似然。 现在对于每个小批量输入，我们都要计算这一前向传播过程，取得所有我们所需的项，它们都是可微分的，所以我们接下来把它们全部反向传播回去获得梯度。我们利用梯度不断更新参数，包括编码器和解码器、网络参数$\theta$和$\phi$，从而最大化训练数据的似然。 Variational Autoencoders: Generation process一旦我们训练好VAE，要想生成数据，我们只需要解码器网络。我们在训练阶段就开始对$z$采样，而不用从后验分布中采样。在生成阶段，会从真实的生成过程中采样。先从设定好的先验分布中采样，接下来从这里对数据$x$采样。 在本例中通过在MNIST数据集上训练VAE，我们可以生成这些手写数字样本。我们用$z$表示隐变量，因为是从先验分布的不同部分采样，所以我们可以通过改变$z$来获得不同的可解释的意义。这里可以看到一个关于二维z的数据流形。如果我们有一个二维的$z$然后我们让$z$在某个区间内变化，比如该分布的百分比区间，接下来让$z_1$和$z_2$逐渐变化，从这幅图中可以看到各种不同的$z_1$和$z_2$的组合所生成的图像，它会在所有这些不同的数字之间光滑地过渡变化。 我们对$z$的先验假设是对角的，这样做是为了促使它成为独立的隐变量，这样它才能编码具有可解释性的变量。因此我们就有了$z$的不同维度，他们编码了不同的具有可解释性的变量。 在人脸数据上训练的模型中，随着我们改变$z_1$，从上往下看笑脸的程度在逐渐改变，从最上面的眉头紧锁到下面大的笑脸；接下来改变$z_2$，从左往右看发现人脸的朝向在变化，从一个方向一直向另一个方向变化。 $z$同时也是很好的特征表示，因为$z$编码了这些不同的可解释语义的信息是多少。这样我们就可以利用$q(z|x)$也就是我们训练好的编码器，我们给它一个输入，将图像$x$映射到$z$，并把$z$用作下游任务的特征，比如监督学习，分类任务。 Variational Autoencoders: SummaryVAE实际上是在原来的自编码器上加入了随机成分，那么在使用VAE的时候我们不是直接取得确定的输入$x$然后获得特征$z$最后再重构$x$，而是采用随机分布和采样的思想，这样我们就能生成数据。 为了训练模型VAEs，我们定义了一个难解的密度分布，我们推导出一个下界然后优化下界，下界是变化的，“变分”指的是用近似来解决这些难解的表达式，这是模型被称为变分自动编码器的原因。 VAEs优点：VAEs就生成式模型来说是一种有据可循的方法，它使得查询推断成为可能，如此一来便能够推断出像$q(z|x)$这样的分布，这些东西对其他任务来说会是很有用的特征表征。 VAEs缺点：当我们在最大化似然函数的下界时，不像pixelRNN和pixelCNN那样明确。且它生成的数据中仍然有模糊的成分。 Generative Adversarial Networks (GAN)我们之前的PixelCNN和PixelRNN定义了一个易于处理的密度函数，通过密度函数优化训练数据的似然；VAEs有一个额外定义的隐变量$z$，有了$z$以后获得了很多的有利性质但是我们也有了一个难解的密度函数，对于该函数我们不能直接优化，我们推到了一个似然函数的下界，然后对它进行优化。 现在我们放弃显式地对密度函数建模，我们想要得到的是从分布中采样并获得质量良好的样本。GANs中不再在显式的密度函数上花费精力，而是采用一个博弈论的方法，并且模型将会习得从训练分布中生成数据，而这一实现是基于一对博弈玩家。 在GAN的配置中，我们真正在意的是我们想要能够从一个复杂的高维训练分布中采样。如果想从这样的分布中生成样本，是没有什么直接的来方法可以采用的（该分布十分复杂，我们无法从中采样）。我们将要采用的方法是从一个简单分布中采样，比如符合高斯分布的噪声，然后我们学习一个从这些简单分布直接到我们想要的训练分布的一个变换。那么用什么来表达这一复杂的变换？当然是神经网络。 接下来要做的是取得一些具有某一指定维度的噪声向量作为输入，然后把该向量传给一个生成器网络，之后我们要从训练分布中采样并将结果直接作为输出。对于每一个随机噪声输入，我们都想让它和来自训练分布的样本一一对应。 Generative Adversarial Networks: Training process我们接下来训练这个网络的方式是，我们会把训练过程看做两个玩家博弈的过程，一个玩家是生成器网络（Generator），一个是判别器网络（Discriminator）。生成器网络作为玩家1会试图骗过判别器网络，欺骗的方式是生成一些看起来十分逼真的图像，同时第二个玩家，也就是判别器网络，试图把真实图片和虚假图片区别开，判别器试图正确指出哪些样本是生成器网络生成的。 我们将随机噪声输入到生成器网络，生成器网络将会生成这些图像，我们称之为来自生成器的伪样本，然后我们从训练集中取一些真实图片，我们希望判别器网络能够对每个图片样本做出正确的区分，这是真实样本还是伪样本。 我们的想法是，我们想训练一个性能良好的判别器，如果它能很好的区分真实样本和伪样本，同时如果我们的生成器能够生成一些伪造样本，而这些伪造样本能够很好的骗过判别器，那么我们就获得了一个很好的生成模型，如此一来我们将可以生成一些看起来很像训练集合中的图像的样本。 现在我们现在有两个玩家，需要通过一个MiniMax博弈公式联合训练这两个网络，该MiniMax目标函数就是如图所示的公式，我们的目标是让目标函数在$\theta_g$上取得最小值，$\theta_g$是指生成器网络$g$的参数；同时要在$\theta_d$上取得最大值，$\theta_d$指的是判别器网络的参数。 公式中各项的含义： 第一项是在训练数据的分布上取$\log(D(x))$的期望，$\log(D(x))$是判别器网络在输入为真实数据（训练数据）时的输出，该输出是真实数据从分布p-data中采样的似然概率； 第二项是对$z$取期望，$z$是从$p(z)$中采样获得的，这意味着从生成器网络中采样，同时$D(G(z))$这一项代表了以生成的伪数据为输入判别器网路的输出，也就是判别器网络对于生成网络生成的数据给出的判定结果。 对该过程的解释：我们的判别器的目的是最大化目标函数，也就是在$\theta_d$上取最大值，这样一来$D(x)$就会接近1，也就是使判别结果接近真，因而该值对于真实数据应该相当高，并且$D(G(z))$的值也就是判别器对伪造数据输出就会相应减小，我们希望这一值接近于0。因此如果我们能最大化这一结果，就意味着判别器能够很好的区别真实数据和伪造数据。对于生成器来说，我们希望它最小化该目标函数，也就是让$D(G(z))$接近1，如果$D(G(z))$接近1，那么用1减去它就会很小，判别器网络就会把伪造数据视为真实数据，也就意味着我们的生成器在生成真实样本。 那我们该如何训练这些网络呢。这是一个无监督学习，所以不会人工给每个图片打上标签，但是生成器网络中生成的数据我们标签为0或假，我们的训练集都是真实的图片，被标记为1或真。有了这些以后，对判别器的损失函数而言就会使用这些信息，判别器要做的就是对生成器生成的图片输出0，而对真实图片输出1，这其中没有外部标签。 如何训练：首先对判别器进行梯度上升，从而习得$\theta_d$来最大化该目标函数。接着对生成器进行梯度下降，$\theta_g$进行梯度下降最小化目标函数，此时目标函数只取右边这一项，因为只有这一项与$\theta_g$有关。 以上就是GAN的训练方式，我们交替训练生成器和判别器，每次迭代生成器都试图骗过判别器。但在实践中，有一件事不得不注意。我们定义的生成器目标函数并不能很好地工作，看一下上图损失函数的函数空间，关于$D(G(z))$的损失函数的函数空间，$1-D(G(z))$也就是我们期望对于生成器能够最小化的项，它的函数图像如图所示，我们想要最小化该函数。但我们可以看出该损失函数越向右斜率越大，$D(G(z))$越接近1该函数的斜率越高，这意味着当我们的生成器表现很好时，我们才能获得最大梯度。另一方面，当生成器生成的样本不怎么好时，这时候的梯度相对平坦。这意味着梯度信号主要受到采样良好的区域支配，然而事实上我们想要的是从训练样本中学到知识。 因此，为了提高学习效率，我们接下来要做的是针对梯度定义一个不同的目标函数，去做梯度上升算法。也就是说我们不再最小化判别器正确的概率，而是进行最大化判别器出错的概率，这样就会产生一个关于最大化的目标函数，也就是最大化$\log(D(G(z)))$，可以看出函数图像是原来函数图像的反转。现在我们可以在生成样本质量还不是很好的时候获得一个很高的梯度信号。 联合训练两个网络很有挑战，而且会不稳定。交替训练的方式不可能一次训练两个网络，还有损失函数的函数空间会影响训练的动态过程。所以如何选择目标函数，从而获得更好的损失函数空间来帮助训练并使其更加平稳是一个活跃的研究方向。 在每一个训练迭代期都先训练判别器网络，然后训练生成器网络。 对于判别器网络的k个训练步，先从噪声先验分布$z$中采样得到一个小批量样本，接着从训练数据$x$中采样获得小批量的真实样本，下面要做的将噪声样本传给生成器网络，并在生成器的输出端获得伪造图像。此时我们有了一个小批量伪造图像和小批量真实图像，我们有这些小批量数据在判别器生进行一次梯度计算，接下来利用梯度信息更新判别器参数，按照以上步骤迭代一定的次数来训练判别器。 之后训练生成器，在这一步采样获得一个小批量噪声样本，将它传入生成器，对生成器进行反向传播，来优化目标函数。 交替进行上述两个步骤。 Generative Adversarial Networks: Generation process我们对生成器网络和判别器网络都进行了训练，经过几轮训练后我们就可以取得生成器网络并用它来生成新的图像。我们只需把噪声$z$传给它来生成伪造图像。 Generative Adversarial Networks: Improvement之前GAN生成的图片分辨率低不清晰，后来出现了一些研究提升了生成样本的质量。Alex Radford提出给GANs增加卷积结构用于提升样本质量。 噪声向量$z$通过卷积神经网络一路变换直到输出样本。 本例中我们可以取得两个$z$，也就是两个不同的噪声向量，然后我们在这两个向量之间插值。这里每一行都是从随机噪声z到另一个随机噪声向量$z$的插值过程，这些都是平滑插值产生的图像。 我们更深入分析向量$z$的含义，可以对这些向量做一些数学运算。图中实验所做的是取得一些女性笑脸，一些淡定的女性脸，一些淡定的男性脸样本。取这三种样本的$z$向量的平均。然后取女性笑脸的平均向量减去淡定脸女性的平均向量，再加上淡定男性脸的平均向量，我们最后会得到男性的笑脸样本。 本例同上。这样一来我们就会看到z向量具有这样的可解释性，从而你就可以利用它来生成一些相当酷的样本。 2017年有大量关于GANs的研究工作。上图左边可以看到更好的训练与生成过程，即我们之前说的损失函数的改进，是的训练更稳定，这样一来就能获得几种不同结构的优质生成效果。另外我们还可以实现领域迁移和条件GANs，如上图中我们把源空间的马转换到输出空间的斑马。我们可以先获取一些马的图像并训练一个GAN，将这个GAN训练成输出和输入的马结构相同但细节特征却属于斑马。同样可以颠倒该过程，我们可以把苹果变成橘子。我们还可以通过这种方式做照片增强。上图下面的例子是场景变换，将冬天的图像转换成夏天。右上角的例子是根据文本信息生成图像。右下角是自动填色。 Generative Adversarial Networks: SummaryGANs并不使用显式的密度函数，而是利用样本来隐式表达该函数，GAN通过一种博弈的方法来训练，通过两个玩家的博弈从训练数据的分布中学会生成数据。 GANs的优点是它们可以生成目前最好的样本。 GANs的缺点是训练起来需要更多的技巧，而且训练起来比较不稳定。我们并不是直接优化一个目标函数，如果是这样的话只要后向传播就能轻松训练。事实上，我们需要努力地平衡两个网络的训练，这样就可能造成不稳定。同时我们还会由于不能够进行一些查询推断而受挫，如$p(x)$，$p(z|x)$，也就是在VAE里遇到的同样的问题。 Summary]]></content>
      <categories>
        <category>CS231n</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Deep Leanring</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n Lecture 12 Visualizing and Understanding]]></title>
    <url>%2F2018%2F10%2F07%2FCS231n%20Lecture%2012%20Visualizing%20and%20Understanding%2F</url>
    <content type="text"><![CDATA[Lecture 12主要讲解的是对卷积神经网络的可视化和解释性的研究，从网络层的特征可视化开始到基于梯度提升方法的特征匹配、特征反演，进而衍生出纹理合成、图像生成和风格转移等。网络内部到底是如何运行的？是如何完成自己特定的工作的？他们需要寻找的特征类型是什么？这些中间层的作用是什么？ First Layer：Visualize Filters第一个卷积层由许多个卷积核组成，如AlexNet中有64个卷积核，每一个的size是（3,11,11）。通过卷积核在输入图像上的滑动、与图像块做点积，最后得到了第一个卷积层的输出。 由于我们得到了第一个卷积层的权重和输入图像像素的点积，我们可以通过简单地可视化得到卷积核寻找的东西，并将其看作图像显示出来。如AlexNet中的（3,11,11）的卷积核可以看做有3通道的$11\times11$的图像，并且给定红蓝绿值。因为有64个卷积核，所以我们把它看做由64个小的3通道的$11\times11$图像组成的大图像（如下图所示）。我们可以看出它们都在寻找有向边（如明暗线条），从不同的角度和位置来观察输入图像，我们可以看到完全相反的颜色（如绿色和粉色，蓝色和橙色）。 无论网络结构如何，或是训练数据类型如何，第一个卷积层得到的可视化结果都与上图类似。 如果我们对中间层做相同的可视化操作，但实际上它的可解释性会差很多。比如第二层使用20个（16,7,7）尺寸的卷积核，我们不能直接将其转换成可视化图像。我们可以将（16,7,7）的卷积核平面展开成16个$7\times7$的灰度图像。因为这些卷积核没有直接连接到输入图像，回想一下，第二层的卷积核与第一层的输出相连接，所以这让我们意思到在第一次卷积后，什么类型的激活模式会使第二个激活层的卷积的活性最大化。但这并不是可以解释的，因为我们并不知道那些第一层的卷积在图像像素上呈现出的样子是什么样的。 Last Layer网络的最后一层我们通过大概1000个类的得分来预测图像类别。在最后一层之前一般会有一个全连接层，以AlexNet为例，我们用4096维的特征向量来表示我们的图像，然后将其输入到最后一层来预测分类得分。 另一种用来解决可视化和理解卷积神经网络的方式是尝试去理解神经网络的最后一层到底发生了什么。我们可以提取一些数据集通过我们训练后的卷积神经网络，并为每一个图像标记对应的4096维向量，接着可视化最后一个隐层。 Last Layer：Nearest Neighbors这里我们尝试使用最近邻算法。与之前所讲的逐像素地计算最近邻不同之处在于，这里我们计算的是最后一层卷积层所生成的4096维特征向量的最近邻。（上图左侧是ciffa-10上测试的逐像素计算最近邻，上图中部展示的是计算4096维的最近邻。） 我们可以发现这两种方法非常的不同，因为图像的像素在它的近邻和特征空间之间是非常不同的。然而这些图像的语义内容在特征空间中是相似的。例如，我们在第二行查询的是这只站在图像左侧的大象，但在其最近邻结果中，我们可以发现有站在图像右侧的大象，而它们的像素是几乎完全不同的。通过神经网络学习到的特征空间中，这两个图像彼此之间非常相似，也就是说特征空间的特性是捕捉这些图像的语义内容。 Last Layer：Dimensionality Reduction另一个观察最后一层到底发生了什么的角度是使用降维方法。类似于PCA，有一种更强大的算法t-SNE（t分布领域嵌入）用于可视化特征的非线性降维。 这里展示了在mnist数据集上使用t-SNE降维的应用实例。我们使用t-SNE将mnist中的$28\times28$维原始像素的特征空间（4096维）压缩到2维。上图中的集群对应了mnist数据集中的数据。 于是我们在我们训练的图像分类网络中做相同类型的可视化工作。我们提取大量的图像，并且让他们在卷积神经网络上运行，记录每个图像在最后一隐层的4096维特征向量。然后我们通过t-SNE降维方法把4096维的特征空间压缩到2维特征空间。现在我们在压缩后的2维特征空间中布局网络，并且观察这个2维特征空间中网格中每个位置会出现什么类型的图像。从上图我们可以发现在特征空间中有一些不连续的语义概念，与Nearest Neighbors结果类似，特征上相似的物体会聚集到一起。 Visualizing Activations可视化中间层的权重解释性并不是那么强，但是可视化中间层的激活映射图在某些情况下是具备可解释性的。CONV5的特征向量为$128\times13\times13$，我们将其看做128个的$13\times13$的二维向量，我们可以把每一个$13\times13$的二维向量可视化为灰度图像。这可以告诉我们卷积神经网络要寻找的特征在输入中是什么类型的。 从上图可以看出大部分中间层特征都有很多的干扰，但是这里有一个突出的中间层特征（绿色方框），看起来似乎在激活对应人脸的特征映射图部分。 Maximally Activating Patches（最大化激活块）可视化中间层的另一种非常有用的方法是可视化输入图像中什么类型的图像块可以最大限度地激活不同的特征和不同的神经元。我们选取AlexNet的卷积层，AlexNet的每一个激活量提供$128\times13\times13$的三维数据，其中每一个元素就是一个神经元，我们选取128个$13\times13$二维向量中的一个（如第17个），这里一个二维向量就是一个卷积核和输入数据点积后的运算结果。我们通过卷积神经网络运行很多图像，对于每一个图像记录它们相应部分（第17个）的卷积特征，可以观察到特征映射图的相应部分已经被我们的图像数据集最大地激活。 卷积层中的每个神经元在输入部分都有一些小的感受野，每个神经元的管辖部分并不是整个图像，它们只针对这个图像的子集合。我们要做的是从这个庞大的图像数据集中，可视化来自特定层，特定特征对应的最大激活图像块，我们可以根据这些激活块在这些特定层的激活程度来解决这个问题。 上图是一些来自激活特定神经元的输入图像块的实例，最大化激活图像块的可视化结果。我们从神经网络的每一层选择一个神将元，根据从大型数据集中提取的图像对这些神经元进行排序（根据神经元激活值大小排序），这会使这个神经元被最大程度地激活。上面是低层的，使激活的特征很明显，下面是高层的，具有更广阔的感受野，特征更高级一些。 这可以让我们了解神经元可能在寻找什么特征。如第一行中神经元可能在寻找输入图像中圆形的深色物体。 Occlusion Experiments（遮挡实验）遮挡实验的目的是弄清楚究竟是输入图像的哪个部分导致神经网络做出分类决定。我们将一幅输入图像的某个部分遮挡，将遮挡部分设置为这幅图像的平均像素值。通过神经网络运行该图像，记录遮挡图像的预测概率，然后将这个遮挡图像块划过输入图像的每个位置，并重复之前的操作，最后绘制图像的热力图，热力图显示了我们遮挡不同部位时的预测概率输出。 如果我们遮挡了图像的某个部分，并且导致了神经网络分数值的急剧变化，那么这个遮挡的输入图像部分可能对分类决策起到非常重要的作用。 Saliency Maps（显著图）给出一只狗的输入图像，以及狗的预测类标签，我们想要知道输入图像中的哪部分像素对于分类是重要的。 显著图的方法是计算输入图像像素的预测类别分数值的梯度，这将直接告诉我们在一阶近似意义上对于输入图片的每个像素如果我们进行小小的扰动，那么相应分类的分数值会有多大的变化。 GrabCut是一种分割算法，当将其与显著图结合起来时，我们可以细分出图像中的对象。但这种方法有一些脆弱。 Intermediate Features via backprop(guided)（引导式反向传播）另一种方法称之为引导式反向传播。我们不使用类的分数值，而是选取神经网络中间层的一些神经元，看图像中的哪些部分影响了神经网络内的神经元的分值。仅传播正梯度，仅需跟踪整个神经网络正面积极的影响。可以看出此种方法生成的图像特征更加清晰。 Gradient Ascent（梯度上升）神经网络相当于一个函数告诉我们输入图像的哪一部分影响了神经元的分数值，问题是如果我们移除了图像上的这种依赖性后，什么类型的输入会激活这个神经元。我们现在希望通过梯度上升方法来修正训练的卷积神经网络的权重，并在图像的像素上执行梯度上升来合成图像，以尝试和最大化某些中间神经元和类的分数值。在执行梯度上升的过程中。我们不再优化，神经网络中的权重值保持不变，相反，我们试图改变一些图像的像素，使这个神经元的值或这个类的分数值最大化。除此之外，我们需要一些正则项来防止我们生成的图像过拟合特定网络的特性。 生成图像需要具备两个属性： 最大程度地激活一些分数值或神经元的值； 我们希望这个生成的图像看起来是自然的，即我们想要生成的图像具备在自然图像中的统计数据。正则项强制生成的图像看起来是自然的图像。 我们需要把初始图像初始化为0，或是添加高斯噪声，然后重复进行前向传播并计算当前分数值、通过反向传播计算相对于图像像素的神经元值的梯度、对图像像素执行一个小的梯度下降或上升的更新，以使神经元分数值最大化，直到我们生成了一幅漂亮的图像。 Regularizer一个非常普通的图像正则化想法是惩罚生成图像的L2范数，这从语义上来说并不是那么有意义。 当你在训练的神经网络上（惩罚生成图像的l2范数）时，生成的图片如上图所示，可以看出我们正在尝试最大化左上角哑铃（dumbbell）的生成图像分数值（第一幅图）。 解决问题的另一个角度是通过改进正则化来改善可视化图像。除了L2范数约束外，我们还定期在优化过程中对图像进行高斯模糊处理，同时也将一些低梯度的小的像素值置0。可以看出这是一种投影梯度上升算法，定期投影具备良好属性的生成图像到更好的图像集中。例如，进行高斯模糊处理后，图像获得特殊的平滑性，更容易获得清晰的图像。 现在我们可以不仅对类的最后分数值执行这个程序，也可以对中间神经元。例如，我们可以最大化某个中间层的其中一个神经元的分数值，而不是最大化台球桌（billard table）的分数值。 以上图像都是不同随机初始化图像经过同一程序得到的结果。 我们可以使用这些相同类型的程序来可视化及合成图像即最大限度激活神经网络的中间神经元。然后我们就可以了解到这些中间神经元寻找的东西是什么。例如第四层可能在寻找螺旋状的东西。 一般来说，当你把图片放的越大，神经元的感受野范围也会越大，所以（使用者）应该寻找图像中较大的图像块，神经元则倾向于寻找图像中更大的结构或更复杂的类型。 另外在优化问题中考虑多模态问题，即对每一个类运行聚类算法以使这些类分成不同的模型，然后用接近这些模型其中之一的类进行初始化，可以得到更好的图像。 Fooling Images / Adversarial Examples（愚弄图像/对抗样本）愚弄图像，即选取一些任意的图像，比如我们提取一张大象的图像，然后告诉神经网络最大化这张图像中考拉的分数值，然后我们要做的是改变这个大象的形象，让神经网络将它归类为考拉。你可能希望的是这头大象更像一只考拉，有可爱的耳朵，但是实际上并不是如此。如果你提取这张大象的图像，然后告诉神经网络它是考拉，并且尝试改变大象的图像，你将会发现第二幅图像会被归类为考拉，但是它在我们看来和左边第一幅图是一样的。 第一列和第二列图像在像素上是没有差异的，如果你放大这些差异，并不会真的在这些差异中看到ipod或者考拉的特征，它们就像随机的噪声模式。 （为何会这样？Ian Goodfellow将会在Lecture 16：特邀讲座中讲解。） DeepDream（也属于基于梯度的图像优化。）提取我们输入的图像，通过神经网络运行到某一层，接着进行反向传播并且设置该层的梯度等于激活值，然后反向传播到图像并且不断更新图像。 对于以上步骤的解释：试图放大神经网络在这张图像中检测到的特征，无论那一层上存在什么样的特征，我们设置梯度等于特征值，以使神经网络放大它在图像中所检测到的特征。 这种方法同样可用于最大化图像在该层的L2范数。 A couple of tricks： 计算梯度之前抖动图像，即不是通过神经网络运行完全和原图像相同的图像，而是将图像移动两个像素，并将其他两个像素包裹其中，这是一种正则化方式，以使图像更加平滑； 使用梯度的L1归一化； 有时候修改像素值。 这是一种投影梯度下降，即投影到实际有效图像的空间上。 Feature Inversion（特征反演）选取一张图像，通过神经网络运行该图像，记录其中一个图片的特征值，然后根据它的特征表示重构那个图像，观察那个重构图像我们会发现一些在该特征向量中捕获的图像类型的信息。 我们可以通过梯度上升和正则化来做到。与其最大化某些分数值，不如最小化捕捉到的特征向量之间的距离，并且在生成图像的特征之间尝试合成一个新的与之前计算过的图像特征相匹配的图像。 一个经常见到的正则化是全变差正则化，全变差正则化将左右相邻像素间的差异拼凑成上下相邻，以尝试增加生成图像中特殊的平滑度。 左边是原始图像，我们通过VGG-16运行这个图像，记录这个神经网络某一层的特征，然后尝试生成一个与那层记录的特征相匹配的新图像，这让我们了解到这张图像在神经网络不同层的这些特征的信息存储量。例如当我们尝试基于VGG-16的relu2-2特征重构图像，可以看到图像被完美地重构，即不会真正丢弃该层原始像素的许多信息。但是当我们向上移动到神经网络的更深处，尝试从relu4-3、relu5-1重构图像，可以看到图像的一般空间结构被保留了下来，但是很多低层次的细节并不是原始图像真实的像素值，并且纹理的颜色也和原来不同。这些低层次的细节在神经网络的较高层更容易损失。 这让我们注意到随着图像在神经网络中层数的上升，可能会丢失图像真实像素的低层次信息，并且会试图保留图像的更多语义信息，对于类似颜色和材质的小的变化，它是不变的。 Texture Synthesis（纹理合成）这里没有使用神经网络，而是运用了一种简单算法，即按照扫描线一次一个像素地遍历生成图像。然后根据已生成的像素查看当前像素周围的邻域，并在输入图像的图像块中计算近邻，然后从输入图像中复制像素。 上述非神经网络方法对于简单纹理生成效果很好，但是当纹理较为复杂时，可能会行不通。 Neural Texture Synthesis: Gram Matrix首先选取我们输入的石头纹理，把它传递给卷积神经网络，然后抽取他们在神经网络中某层的卷积特征，这里我们假设卷积特征size为$C\times H\times W$，我们可以将其看做$H\times W$的空间网格，在网格上的每一点都有$C$维特征向量来描述图像在这点的外观。 我们将会使用激活映射图来计算输入纹理图像的映射符，然后选取输入特征的两个不同列，每个特征列都是$C$维的向量，然后通过这两个向量得到$C\times C$的矩阵。这个$C\times C$矩阵告诉我们两个点代表的不同特征的同现关系，如果C*C矩阵中位置索引为$ij$的元素值非常大，这意味着这两个输入向量的位置索引为$i$和$j$的元素值非常大。 这以某种方式捕获了一些二阶统计量，即映射特征图中的哪些特征倾向于在空间的不同位置一起激活。 我们将对$H\times W$网格中所有不同点所对应的特征向量取平均值，那么我们会得到$C\times C$ Gram矩阵，然后使用这些描述符来描述输入图像的纹理结构。 关于Gram矩阵：它丢弃了特征体积中的所有空间信息，因为我们对图像中的每一点所对应的特征向量取平均值，它只是捕获特征间的二阶同现统计量，这最终是一个很好的纹理描述符。并且Gram矩阵的计算效率非常高，如果有$C\times H\times W$三维张量，可以对它进行重新组合得到$C\times HW$，然后乘以它本身的转置矩阵，这些计算都是一次性的。 使用协方差矩阵同样有效，但是计算成本要高。 一旦我们有了纹理在神经网络上的描述符，我们可以通过梯度上升来合成与原始图像纹理相匹配的新的图像。这看起来跟我们之前说的特征重构有些类似，但是这不是试图重构输入图像的全部特征映射，而是尝试重构输入图像的Gram矩阵纹理描述符。 人们用纹理图像来训练VGG网络，并计算网络不同层的Gram矩阵，然后随机初始化新的图像，接着使用梯度上升。即随机选取一张图片，使他通过VGG，计算在各层上的Gram矩阵，并且计算输入图像纹理矩阵和生成图像纹理矩阵之间的L2范数损失，然后进行反向传播，并计算生成图像的像素值梯度，然后根据梯度上升一点点更新图像的像素，不断重复这个过程，即计算Gram矩阵的L2范数损失，反向传播图像梯度，最终会生成与纹理图像相匹配的纹理图像。 上图顶部是四张不同的输入纹理图像，底部是通过Gram矩阵匹配的纹理合成方法合成的新图像，即计算在预训练卷积神经网络中不同层的Gram矩阵。如果我们使用卷积神经网络的较低层，那么通常会得到颜色的斑点，总体的结构并没有被很好的保留下来。下面几行的图像，即在神经网络的较高层计算Gram矩阵，它们倾向于更大力度地重建输入图像，这在合成新图像时可以很好地匹配输入图像一般的空间统计量，但是他们在像素上和真实的输入图像本身有很大的差别。 Neural Style Transfer如果我们将梵高的星空或其他艺术品作为输入纹理图像，然后运行相同的纹理合成算法，那么生成的图像倾向于重构那些艺术品中比较有趣的部分。 当你把Gram矩阵匹配的纹理合成方法与特征匹配的特征反演算法结合在一起，一些有趣的事情将会发生，即风格迁移。 风格迁移中，我们将两张图像作为输入图像。第一步，选取其中一幅图像作为内容图像，它将引导我们的生成图像看起来像什么。同样的，风格图像负责生成图像的纹理和风格。将它们输入到神经网络中以计算Gram矩阵和特征，然后使用随机噪声初始化输出图像，计算Gram矩阵的L2范数损失，以及图像上的像素梯度。不断重复上述步骤，在生成图像上执行梯度上升，以实现最小化内容图像的特征重构损失以及风格图像的Gram矩阵损失。 生成图像时，我们在联合重构最小化内容图像的特征重构损失和风格图像的gram矩阵损失。通过控制两个损失的权重，我们可以控制内容和风格之间在生成图像中所占的比重。 还有很多别的超参数，如在计算Gram矩阵前重新调整风格图像的尺寸大小，这可以让我们控制从特征图像中重构的特征的尺度。 还有很多别的超参数，如在计算Gram矩阵前重新调整风格图像的尺寸大小，这可以让我们控制从特征图像中重构的特征的尺度。 风格迁移算法有一个问题，其算法效率非常低。为了生成图像，我们需要通过预训练神经网络，计算大量的正向传播和反向传播。 解决此问题的方法是用另一个神经网络来进行风格迁移的工作。即在一开始就修改我们想要迁移的风格。在这种情况下不是为我们想要合成的每个图像运行一个单独的优化程序，而是训练一个可以输入内容图像的前馈网络，直接输出风格迁移后的结果。训练前馈神将网络的方法是在训练期间计算相同内容图像和风格图像的损失，然后使用相同梯度来更新前馈神经网络的权重，一旦训练完成，只需在训练好的网络上进行单一的正向传播。以上两种快速风格迁移算法与第一种类似，有一些小的不同之处。 以上这些快速风格迁移算法的一个缺点是在训练新的风格迁移网络时，对于要应该的每个风格实例都需要训练一个神经网络。来自Google的一篇论文提出使用一个训练好的前馈神经网络对输入图像应用许多不同的风格。选取一张内容图像，以及风格图像，然后使用一个神经网络来应用许多不同类型的风格。此算法也能在一个训练好的神经网络上进行混合风格迁移。一旦你在训练这个神经网络时使用了4中不同风格的图像，你可以在测试时指定这些风格的混合。]]></content>
      <categories>
        <category>CS231n</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Deep Leanring</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n Lecture 11 Detection and Segmentation]]></title>
    <url>%2F2018%2F10%2F05%2FCS231n%20Lecture%2011%20Detection%20and%20Segmentation%2F</url>
    <content type="text"><![CDATA[Lecture 11主要讲解的是分割、定位与检测。具体包括语义分割、分类定位、目标检测和实例分割四部分。 语义分割（Semantic Segmentation）语义分割为每一个像素产生一个分类标签，但并不区分同类目标。 Semantic Segmentation Idea: Sliding Window输入一幅图像，将其打碎为许多小的局部图像块。以图中为例，三个包含牛头的小块，对其分别进行分类，判断它的中心像素属于哪类。这个计算复杂度相当的高，因为我们想要标记图像中的每个像素点，我们需要为每个像素准备单独的小块。 当我们希望区分彼此相邻甚至重叠的两块图像块时，这些小块的卷积特征最终通过同样的卷积层，因此，我们可以共享这些图像块的很多计算过程。 Semantic Segmentation Idea: Fully Convolutional通过数个卷积层最后得到的张量数据的大小是$C \times H \times W$，$C$是类的数量，这个张量会对每一个像素的分类进行评分，因此我们可以通过堆叠卷积层一次性完成所有计算。但这种方法存在一个问题，我们的每一个卷积层都保持着与原始输入图像相同的尺寸，这导致计算量非常大。 Semantic Segmentation Idea: downsampling+upsampling常用方法是对输入图像做下采样，之后再做上采样。相较于基于图像的全维度做卷积，我们仅仅对一部分卷积层做原清晰度处理，之后对特征进行下采样（最大池化，或stride&gt;1的卷积）。在后部分网络中我们希望增加清晰度，这样能使我们的输出维持原有的图像尺寸。这种方法更加方便计算，因为我们可以让网络很深，每层的清晰度降低但是有很多层叠加。 上采样（upsampling）：去池化（unpooling）平均池化在下采样中是针对每个池化区域取平均，而在上采样中做的是最近距离（nearest neighbor）去池化。如上图左所示，输出是输入的$2\times2$ stride 2 nearest neighbor去池化结果，在去池化区域中使用输入的当前元素进行填充。如上图右所示为钉床函数的去池化，我们将输入元素放在去池化区域的左上角，并将去池化区域的其他元素置0。 许多神经网络结构是对称的，尤其是下采样的神经网络之后会进行上采样。我们在下采样时用最大池化的同时，会记住池化区域中的最大元素索引，之后我们会执行类似钉床函数去池化的操作，不过这里我们是将输入的元素放在去池化区域中与之前记住的最大元素索引相对应的地方。 将向量去池化可以帮助我们很好地处理细节，存储空间信息（在池化后会丢失这些信息）。 上采样（upsampling）：卷积转置（Transpose Convolution）前面所谈到的各种去池化方法都是使用固定的方程，并不是真的在学习如何上采样。而strided convolution既是一种可学习的层，它可以学习如何下采样，与之对应的我们称之为卷积转置，用于进行上采样，即上采样特征图，又能学习权重。上图所示为3x3卷积的常规操作。 上图所示为3x3的 strided convolution的常规操作。 上图所示为卷积转置操作。卷积转置依次取出特征图的输入元素（标量），我们用这个标量乘以filter，以filter的size复制到输出的相应位置，这样输出便是带有权重的filters的叠加（把输入看做权重）。 如上图所示我们在一维情况下来理解卷积转置。上图中输入为$2\times1$，filter为$3\times1$。我们可以看到输出是对输入做加权，最后对输出中的感受野重叠部分进行叠加。 （上图左侧的矩阵X每行的最后一个$x$应为$z$。） 我们可以把卷积写成矩阵相乘的形式。如上图所示，$\vec x$表示有三个元素的卷积向量，$\vec a$表示有4个元素的输入向量。我们将向量$\vec x$转变成矩阵形式X，X包含很多由不同区域偏移的卷积核$\vec x$。卷积转置的意思是我们乘以相同权重矩阵的转置。 当步长为2时情况会有些不一样。对应的转置矩阵不再是卷积。 关于卷积转置有不理解的地方可以参考下CSDN的这篇[深度学习]转置卷积(Transposed Convolution)或是知乎的如何理解深度学习中的deconvolution networks？。 分类与定位我们不只预测图像的类别（如猫），同时绘制一个bounding box将该类别（猫）包裹进去。与目标检测不同的是分类与定位会提前知道有一个（或更多）物体是我们需要寻找的，且只产生一个bounding box。与前面章节所讲解的图像分类类似，这里我们通过AlexNet最后的FC（$4096\times1000$）得到类别的scores。此外，我们新增另一个FC（$4096\times4$），输出维度4代表了bounding box的$x,y$坐标，宽度和高度。于是网络会产生两个不同的输出，一个是类别的scores，一个是bounding box的四个属性，其中前者是分类问题，后者是回归问题。 训练此网络时，我们有两组损失，使用softmax Loss计算类别的损失，使用L2 Loss计算bounding box的损失，我们对两组损失加权求和得到最终的网络Loss。 姿态估计上图为14关节点的样本示例，来定义人的姿态。 输入这张人像后将输出14组参数，逐一给出这14个关节点的$x，y$坐标，然后用回归损失（L2损失、L1损失等）来评估这14个点的预测表现。 目标检测目标检测主要研究的是，假设我们已有一些类别已知，我们输入一幅图像，每当在图像中出现其中某一类对象时，围绕对象绘制一个框并预测对象所属类别。与分类定位任务的区别在于每一张输入图像对象的数量是不定的。 Object Detection as Regression从上图可以看出每幅图像经过网络后需要输出目标bounding box的四个参数信息，然而三幅图的输出参数数量都是不相同的，因此将目标检测问题等同于回归问题来考虑是十分棘手的。 Object Detection as Classification: Sliding Window滑动窗口方法类似于图像分割方法中将图像切分为一小块一小块，我们将图像块输入到卷积神经网络中，之后网络会对输入的图像块进行分类决策。 此方法的问题在于你该如何选择图像块，因为图像中对象的数量、尺寸与位置都不确定。（一般不会用此方法。） Object Detection as Classification: Region Proposals通常我们使用的是一种称为候选区域的方法。对给定的输入图像，候选区域网络会在目标可能出现的地方绘制上千个框，我们可以将这一步理解为做一些定位操作，也就是寻找图像的边界并且尝试绘制可以包含闭合边界的矩形框。区域选择网络会在输入图像中寻找点状的候选区域，然后给我们一些候选的区域，也就是对象可能出现的区域。 一种常见的区域选择方法是selective search，会给你2000个备选区域（而不是上图的1000），CPU运行2s后输入图像会被切分成2000个可能包含目标体的区域。 我们一般会先使用候选区域网络找到物体可能存在的备选区域，再用卷积神经网络对这些备选区域进行分类。 R-CNN给定输入图像，我们运行一些区域选择网络，去找到备选区域（也称之为兴趣区域ROI），selective search寻找到大约2000个兴趣区域。其中一个问题是这些输入中的区域可能有不同的尺寸，但都要在同一个卷积神经网络中运行做分类。由于全连接层的特性，我们希望所有的输入尺寸一致，因此我们需要处理这些区域，将输入尺寸调整为固定尺寸，使之与下游网络输入相匹配，输入到卷积神经网络中，然后使用SVM基于样本做分类，预测出对应的类别。 另外，R-CNN也可以用于回归预测，校正bounding box的参数，预测边界框四个参数的补偿与修正值。 存在问题： 训练阶段十分耗时（84h），需独立训练2000个区域，且图像特征的存储极占空间； 测试阶段也十分耗时（30s/幅）； 区域选择算法不是一种学习算法，参数固定。 Fast R-CNN现在不再按兴趣区域处理，而是将整幅图像通过一些卷积层，得到整个图像的高分辨率特征映射。然后我们仍会用一些备选区域选择算法，但并不是使用固定算法（如selective search），我们将这些备选区域投影到卷积特征映射（feature map），之后从卷积特征映射中提取属于备选区域的卷积块，而不是直接截取备选区域的像素。通过对整个图像进行处理，我们可以重用很多卷积计算。另外，如果我们在下游有很多全连接层，这些全连接层的输入应该是固定尺寸的，所以我们需要对从卷积映射提取的图像块进行reshape（用可微的方法）。这里我们使用被称之为ROI 池化层的网络层（ROI Pooling layer）。之后数据经过FC得到预测分类结果以及对包围盒的线性回归补偿。 Fast R-CNN训练过程如下：当我们训练这个的时候，在BP阶段我们有一个多任务损失，我们可以基于全局反向传播同时学习。 ROI PoolingROI池化有点像最大池化，其主要作用包括以下两点： 缩小region proposal的尺寸，加速FC层的训练速度； 将数据尺寸reshape为固定的大小，以确保能够进入之后的FC进行训练，实现end-to-end training。 ROI pooling层有两个输入： 一个从具有多个卷积核池化的深度网络中训练得到的feature map矩阵； 一个表示了所有感兴趣区域的ROI矩阵（维度为$N\times5$），$N$表示ROI的数目，第1列表示区域的index，其余4列表示区域左上角和右下角坐标。 如上图所示，原图像尺寸为$3\times640\times480$，输出数据尺寸为$512\times7\times7$。经过卷积层后数据尺寸变为$512\times20\times15$的feature map，感兴趣区域大小为$512\times18\times8$并投影到feature map二维平面中的某一部分，之后我们需要将$18\times8$的区域划分成$7\times7=49$个sections，在每一个sections中取最大元素（此处类似max pooling），最终得到进入FC前尺寸为$512\times7\times7$的数据。（ROI Pooling有不明了的地方可参考这里。） Faster R-CNN用固定的函数计算备选区域成为提升计算性能的瓶颈，因此我们让网络自身去做这个。同样的，我们在卷积层中运行整个输入图像去获取特征映射（feature map）来表示整个高清晰度图像。然后这里有一个分离备选区域的网络工作于卷积特征的上层，在网络内部预测自己的备选区域，当我们有了这些备选区域后，后续工作就和fast R-CNN一样了。 模型最终Loss包含四个Loss，对应了模型需要完成的四个任务。区域选择网络需要完成两件事情： 他们是否是待识别物体； 对包围盒进行校正。 最后FC层网络还需要再做上述的这两件事。 Detection without Proposals: YOLO / SSD该方法尝试将目标检测作为回归问题处理。给出输入图像，将输入图像分成数个（如$7\times7$）网格。在每一个网格中，你可以想象一系列的基本边界框（如图所示，绘制了三个基本边界框）。我们需要对每一个网格和每一个基本边界框预测几种参数，一是预测边界框偏移，从而预测出边界框与目标物体的位置偏差；二是预测目标对应类别的classification scores，每个边界框会对应一个类别分数。 最后我们得到如上图所示的输出：$7\times7\times(5\times B+C)$，我们有B个基本边界框，每个边界框对应5个值，分别对应边界框4个参数的差值和我们的置信度；C对应C个目标类别（包括背景类）。我们可以把这种目标检测看做输入一张图像，输出一个三维张量。 基于候选框的网络，正如faster R-CNN中所使用的，最终会寻找出一系列比较接近的图像网格的边界框，另外一些基于候选框的网络除了预测类别之外，还会对预测框做回归。 在faster R-CNN中我们将目标区域的选择看做是一个端到端的回归问题，然后我们对提出的区域分别进行处理。但是在SSD中，我们只做第一步，通过一个前馈网络一次进行所有的目标检测。 目标检测中各式各样的variables： 实例分割实例分割指的是，给定一张输入图像，我们想预测出一个图像中某个目标的类别和目标的位置。与目标检测不同之处在于实例分割不是预测出每个目标的边框，而是想要预测出每个物体的整个分割区域，即预测输入图像的哪些元素对应着预测物体。这有点像是语义分割和目标检测的融合。 Mask R-CNN我们将整张图像输入到卷积网络和训练好的候选框生成网络中，得到训练好的候选区域后，我们将这些候选区域投影到卷积特征上（到这一步之前都和faster R-CNN类似）。但现在不只是进行分类预测类别或回归预测边界框，我们同时希望对每一个候选区域预测出一个分割区域。如上图所示的下面一个分支会对输入候选框中的像素进行分类，确定这是不是属于某个物体。 可以看出Mask R-CNN是前述几种模型的整合。 Mask R-CNN也能用于姿态估计：通过回归预测人体每一个关节点的坐标，我们可以进行姿态估计。我们可以使用Mask R-CNN做目标识别、姿态估计和实例分割，唯一需要修改的部分是对于每一个候选框额外添加一个分支。]]></content>
      <categories>
        <category>CS231n</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Deep Leanring</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n Lecture 10 Recurrent Neural Networks]]></title>
    <url>%2F2018%2F10%2F01%2FCS231n%20Lecture%2010%20Recurrent%20Neural%20Networks%2F</url>
    <content type="text"><![CDATA[Lecture 10主要讲解了循环神经网络模型的结构、前向传播与反向传播，并由此引出了Attention机制、LSTM（长短期记忆网络）等，除此之外还介绍了图像标注、视觉问答等前沿问题。 RNN网络结构RNN通常用来预测一个时间序列向量。每个RNN网络都有如上图所示这样一个小小的循环核心单元。X为输入，将其传入RNN，RNN有一个内部隐藏态（internal hidden state），这一隐藏态会在RNN每次读取新的输入时更新。当模型下一次读取输入时，隐藏状态同时将结果反馈至模型。 循环神经网络隐藏状态$h_t$更新公式：我们对某种循环关系用函数$f$进行计算。函数$f$依赖于权重$W$，接收输入隐藏态$h_{t-1}$和输入$x_t$，然后输出下一个隐藏态$h_t$。如果我们想要在网络的每一步都产生一些输出，那么我们可以增加全连接层，根据每一步的隐藏态做出决策。（注意，每一时间步的函数$f$和参数$W$都是相同的。） Vanilla RNN 络结构图与更新公式：Vanilla RNN 计算图：$h_0$为初始状态，一般$h_0=0$。由上图可以看出每一时间步都在使用相同的$f$和$W$。由反向传播原理可知，$W$最后的梯度是所有时间步下独立计算出的梯度之和。 Many to many$Y_t$可以是每个时间步的类别得分，$L_t$是对应时间步下的损失（如softmax等），计算$L_t$需要序列在每个时间步下都有与之对应的真实标签。所有$L_t$相加就得到了最终的Loss $L$。在反向传播中我们需要计算$\frac{dL}{dW}$，而$L$又会回溯到每一个时间步的损失，然后每一时间步又会各自计算出当下$\frac{dL_t}{dW}$，其总和就是权重$W$的最终梯度。 Many to one此种结构通常用于情感分析等问题。我们会根据网络的最终$h_T$做出决策，因为其整合了序列中包含的所有情况。 One to many此种结构通常用于自动生成图片描述等问题。 Sequence to sequenceMany-to-one+One-to-many此种结构通常用于机器翻译。编码阶段会接收到一个不定长的输入序列（如一个句子），然后整个句子会被编码器网络最终的隐层状态编码成一个单独的向量。解码阶段每一个时间步下会做出一个预测，将其还原成一个句子。 示例：字符级别语言模型现在有vocabulary：[h, e, l, o]，example training sequence：“hello”。我们希望输入前一个字母后网络能够预测出下一个字母是什么。训练过程如下：我们可以看到第一个时间步输入“h”，应该输出“e”，但预测结果score最高的是“o”。 测试过程如下所示：在测试时，一次输入一个样本字符，将输出的字符反馈给模型（这里先不用去看数值，虽然h后面o的可能性最大，但是这里假设就是e）。 接下来我们就要利用Loss来backpropagation修正整个网路，RNN通过时间进行反向传播过程如下图所示：由上图可以看出如果直接是在全局计算的话，会发现显存或内存不够，而且计算过程十分耗时，因为每计算一次梯度都必须做一次前向计算，遍历所有的训练集，然后反向传播每次也会遍历一遍训练集。 在实际中人们通常采用一种近似方法，我们称之为延时间的截断（Truncated）反向传播方法。在训练模型时，前向计算若干步（如100），仅计算这个子序列的Loss，然后沿子序列反向传播并计算梯度更新参数，如下图所示。重复上述batch的计算过程（进入下一个100步），前向传播与第一个batch无异（需导入上一个batch最后得到的隐藏状态$h_t$），但是在计算梯度时，我们仅根据第二批数据反向传播误差，仅更新该batch中的参数（如下图所示）。Andrej Karpathy写了一个Vanilla RNN简单的示例程序：min-char-rnn.py，顺便给大家推荐该代码作者写的详解 karpathy’blog。 RNN的可解释性[Karpathy, Johnson, and Fei-Fei: Visualizing and Understanding Recurrent Networks, ICLR Workshop 2016]该论文中作者训练了一个字符层级的语言模型循环神经网络，然后从隐藏向量中选取一个元素，通过一个序列过程来看这个隐藏向量的值，试图了解这些不同隐藏状态正在寻找的东西。从向量中选择一个元素，然后让句子继续向前运行通过训练好的模型，接下来每个字符的颜色对应于隐藏矢量在读取序列时的每个时间步长的单个标量元素的大小。 图像标注问题[Karpathy A, Li F F. Deep visual-semantic alignments for generating image descriptions, CVPR 2015]我们希望输入一个图像，然后输出自然语言的图像语义信息。 模型中有一部分卷积神经网络用来处理输入的图像信息，它将产生图像的特征向量，然后输入到循环神经网络语言模型的第一个时间步中，一次一个地生成标题的单词。 我们把输入图像输入到卷积神经网络中，但是我们不是使用这个图像网络模型中最后得到的softmax值，而是使用模型末端的4096维向量，我们用这个向量来概述整个图像的内容。 当我们讨论递归神经网络语言模型时，我们需要知道模型的第一个初始化输入，来告诉它开始生成文字。本论文中作者给它了一些特殊的开始记号。在之前的递归神经网络语言模型中我们已经了解了这些矩阵的计算公式，即把当前时间步的输入以及前一个时间步的隐藏状态结合到下一个时间步的隐藏状态。但我们现在还需要添加图片信息，需要用完全不同的方法来整合这些信息，一个简单的方式是加入第三个权重矩阵，它在每个时间步中添加图像信息来计算下一个隐藏状态（如上图所示）。上面五幅图演示了生成句子的过程。 现在我们需要计算词汇表中所有scores的分布，在这里我们的词汇表是类似所有英语词汇的东西，所以他可能会相当大，我们将从分布中采样并在下一次时间步时当做输入传入。依次重复上述步骤，我们将生成完整的句子。一旦我们采样到特殊停止标记（类似于句号）就停止生成。在训练时，我们在每个标题末尾都放上结束的标志，这样在训练过程中结束标记就出现在序列的末尾。在测试时，它倾向于在完成生成句子后对这些结束标记进行采样。 Attention机制当我们生成这个图片标题的文字时，我们允许模型来引导它们的注意到图像不同的部分。通常的做法是，相比于产生一个单独的向量来整合整个图像，该方法的卷积神经网络倾向于产生由向量构成的网络给每幅图片中特殊的地方都用一个向量表示。当我们在前向传播时，除了在每一时间步中对词汇表进行采样外，还会在图像中想要查看的位置上产生一个分布。图像位置的分布可以看成是一种模型在训练过程中应该关注哪里的张量。因此第一个隐藏状态$h_0$计算在图片位置上的分布$a_1$，它将会回到向量集合，给出一个概要向量$z_1$，把注意力集中在图像的某一部分上。现在这个概要向量得到了反馈，作为神经网络下一时间步的额外输入。接着$h_1$将产生两个输出，一个是我们在词汇表上的分布$d_1$，一个是图像位置的分布$a_2$，整个过程将会继续下去，它在每个时间步都会做这两件不同的事情。 Ps：软注意力机制采用的是加权组合，所有图像位置中的所有特征。硬注意力机制中，我们限制模型在每一步只选择一个位置来观察图片。在硬注意力的情况下选择图像的位置有点复杂，因为这不是一个可微函数，所以需要使用一些比vanilla反向传播算法高级一点的算法，以便能够在那样的情况下训练模型。 视觉问答：RNNs with Attention这是一个多对一的情形（选出正确的答案）。我们的模型需要将自然语言序列作为输入，我们可以设想针对输入问题的每个元素，建立一个递归神经网络，从而将输入问题概括为一个向量。然后我们可以用CNN将图像也概括为一个向量。现在把CNN得出的向量和输入问题的向量结合，通过RNN编程来预测答案的概率分布。有时候会将soft spatial attention结合到视觉问答中，所以在这里可以看到这个模型在试图确定这些问题的答案时它在图像上仍具有spatial attention。 Q：如何将编码图像向量和编码问题向量组合起来？A：最简单的一种做法是将他们连接起来然后粘贴进FC中。有时也会用一些高级的方法，即在这两个向量之间做乘法从而得到更为强大的函数。 Multilayers RNNs如上所示是一个三层循环神经网络结构图，现在将输入传进模型，然后在第一层的RNN中产生一系列的隐藏状态。在我们运行了RNN的一层之后得到了所有的隐藏状态序列。我们可以将这些隐藏状态序列作为第二层RNN的输入序列，以此类推。（RNN一般2-4层就足够了） Vanilla RNN 梯度流上图中可以看出隐藏状态$h_t$的计算过程。计算梯度时，我们会得到$\frac{dLoss}{dh_t}$，最后我们需要计算的是$\frac{dLoss}{dh_{t-1}}$。当我们进行反向传播时，梯度会沿红线反向流动。但是当反向传播流过矩阵乘法门时，实际上是用权重矩阵的转置来做矩阵乘法，这意味着每一次BP经过一个vanilla RNN单元时就需要和权重矩阵相乘一次。当我们把很多网络单元连接成一个序列，梯度反向流动穿过一些这样的层时，都要乘以一个权重矩阵的转置，这意味着最终$h_0$的梯度表达式将会包含很多权重矩阵因子。（再强调一下，每一个单元的$W$是相同的） 为了便于理解，我们可以将权重矩阵简化为标量。假如我们有一些标量，我们不断地对同一个数值与这些标量做乘法，当有几百时间步时，情况将非常糟糕。在标量的情形中，它要么在这些标量绝对值大于1时发生梯度爆炸，要么当这些标量绝对值小于1时发生梯度消失。唯一能够让这不发生的情况是这些标量值刚好是1。延伸到矩阵的情况时，标量的绝对值替换为权重矩阵的最大奇异值。 解决梯度爆炸的方法：梯度截断判断梯度的L2范数是否大于某个值。 解决梯度消失的方法：改变RNN结构引出了下面介绍的LSTM。 长短期记忆网络（LSTM）如上图所示，LSTM每个时间步都维持两个隐藏状态。我们称$h_t$为隐藏状态，$c_t$为单元状态，$c_t$相当于保留在LSTM内部的隐藏状态，不会完全暴露到外部去。首先我们传入两个输入来计算四个门i、f、o、g，使用这些门来更新单元状态$c_t$，然后将这些单元状态作为参数来计算下一时间步中的隐藏状态。 将上一时间步的隐藏状态和当前时间步的输入堆叠在一起，然后乘上一个非常大的权重矩阵w，计算得到四个不同的门向量，每个门向量的大小和隐藏状态一样。 i代表输入门，表示LSTM要接受多少新的输入信息；f是遗忘门，表示要遗忘多少之前的单元记忆（上一时间步的记忆信息）；o是输出门，表示我们要展现多少信息给外部；G是门中门，表示我们有多少信息要写到输入单元中去。四个门中i、f、o都用了sigmoid，这意味着输出值都在0和1之间。G用了tanh函数，这意味着输出都在-1到1之间。 从上图公式中我们可以看出单元状态是经过遗忘门逐项相乘的。遗忘门可以看做是由0和1组成的向量，F中0表示我们忘记这个单元状态中的这个元素值，1说明我们想要记住这个单元状态中的这个值。使用遗忘门来断开部分单元状态的值后，我们接下来需要输入门和门中门逐元素相乘。i是由0和1构成的向量，对于单元状态的每个元素值，i的值为1表示我们想要保留单元状态的那个元素，i的值为0表示我们不想保留单元状态对应的那个元素。门中门中的这些值是当前时间步中我们可能会写入到单元状态中去的候选值。单元状态$c_t$在每个时间步中都可以被加一或减一。也就是说在单元状态的内部，我们可以保留或者遗忘之前的状态。所以可以把$c_t$中的每个元素看作是小的标量计数器。 $c_t$经过tanh后被压缩到0~1，再用输出门逐元素相乘。输出门告诉我们对于单元状态中的每个元素，当我们在此时间步计算外部的隐藏状态时，是否希望单元状态中的此元素暴露出去。 LSTM示意图：我们通过传输进来的单元获得了上游梯度$\frac{dLoss}{dc_t}$，然后通过加法运算向后进行方向传播，这个加法运算仅仅是将上游的梯度复制到这两个分支中，这样上游梯度直接被复制并且通过与遗忘门元素相乘的方式贯穿了反向传播过程。此方法的优点在于这里与遗忘门f相乘是矩阵元素相乘而不是矩阵相乘；另一点是矩阵元素相乘可能会在不同的时间点乘以一个不同的遗忘门（更容易避免梯度消失和梯度爆炸的问题），而在vanilla RNN中我们是不断地乘以相同的权重矩阵；最后一个优点是梯度从最后一个隐藏状态$h_T$传递到$c_0$只会经过一个tanh。 通过单元状态进行反向传播的路径是一种梯度高速公路，使梯度相对畅通无阻地从模型最末端的损失函数返回到模型最开始的初始单元状态（与ResNet类似）。 Q：遗忘门f是一个0~1的数，是否也会导致出现梯度消失的问题？A：人们常会初始化遗忘门的偏置参数，进而使遗忘门总是非常接近于1。 这里推荐一篇论文：[LSTM: A Search Space Odyssey Greff et al., 2015]，这篇论文详细研究了LSTM更新方程的每一个部分。]]></content>
      <categories>
        <category>CS231n</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Deep Leanring</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n Lecture 9 CNN Architectures Summary]]></title>
    <url>%2F2018%2F09%2F28%2FCS231n%20Lecture%209%20CNN%20Architectures%20Summary%2F</url>
    <content type="text"><![CDATA[Lecture 9主要讲了一些经典的、比较流行的网络结构，详细讲解了AlexNet、ZFNet、VGGNet、GoogleNet和ResNet。 AlexNetAlexNet网络结构图如下图所示： 网络详细信息如下[227x227x3] INPUT[55x55x96] CONV1: 96 11x11 filters at stride 4, pad 0[27x27x96] MAX POOL1: 3x3 filters at stride 2[27x27x96] NORM1: Normalization layer[27x27x256] CONV2: 256 5x5 filters at stride 1, pad 2[13x13x256] MAX POOL2: 3x3 filters at stride 2[13x13x256] NORM2: Normalization layer[13x13x384] CONV3: 384 3x3 filters at stride 1, pad 1[13x13x384] CONV4: 384 3x3 filters at stride 1, pad 1[13x13x256] CONV5: 256 3x3 filters at stride 1, pad 1[6x6x256] MAX POOL3: 3x3 filters at stride 2[4096] FC6: 4096 neurons[4096] FC7: 4096 neurons[1000] FC8: 1000 neurons (class scores) AlexNet网络特点 First use of ReLU； Used Norm layers (not common anymore)； Heavy data augmentation； Dropout 0.5； Batch size 128； SGD Momentum 0.9； Learning rate 1e-2, reduced by 10 manually when val accuracy plateaus； L2 weight decay 5e-4（正则化的权重衰减）； 7 CNN ensemble: 18.2% -&gt; 15.4%（模型集成，取平均）。 由于当时所使用GPU的显存不够存放这么多参数，因此从第一个卷积层开始将参数分为两组，在两个GPU中训练，如第一个卷积层中参数被分为两组，每组有$11\times11\times3\times48$个参数。 ZFNetZFNet网络结构图如下图所示： ZFNet网络特点 ZFNet框架大体与AlexNet一致； CONV1：Change from ($11\times11$ stride 4) to ($7\times7$ stride 2)； CONV3,4,5：Use 384,384,256 filters, instead of 512,1024,512 filters. VGGNetVGGNet网络结构图如下图所示： VGG16网络详细信息如下INPUT: [224x224x3] memory: 224x224x3=150K params: 0 CONV3-64: [224x224x64] memory: 224x224x64=3.2M params: (3x3x3)x64 = 1,728 CONV3-64: [224x224x64] memory: 224x224x64=3.2M params: (3x3x64)x64 = 36,864 POOL2: [112x112x64] memory: 112x112x64=800K params: 0 CONV3-128: [112x112x128] memory: 112x112x128=1.6M params: (3x3x64)x128 = 73,728 CONV3-128: [112x112x128] memory: 112x112x128=1.6M params: (3x3x128)x128 = 147,456 POOL2: [56x56x128] memory: 56x56x128=400K params: 0 CONV3-256: [56x56x256] memory: 56x56x256=800K params: (3x3x128)x256 = 294,912 CONV3-256: [56x56x256] memory: 56x56x256=800K params: (3x3x256)x256 = 589,824 CONV3-256: [56x56x256] memory: 56x56x256=800K params: (3x3x256)x256 = 589,824 POOL2: [28x28x256] memory: 28x28x256=200K params: 0 CONV3-512: [28x28x512] memory: 28x28x512=400K params: (3x3x256)x512 = 1,179,648 CONV3-512: [28x28x512] memory: 28x28x512=400K params: (3x3x512)x512 = 2,359,296 CONV3-512: [28x28x512] memory: 28x28x512=400K params: (3x3x512)x512 = 2,359,296 POOL2: [14x14x512] memory: 14x14x512=100K params: 0 CONV3-512: [14x14x512] memory: 14x14x512=100K params: (3x3x512)x512 = 2,359,296 CONV3-512: [14x14x512] memory: 14x14x512=100K params: (3x3x512)x512 = 2,359,296 CONV3-512: [14x14x512] memory: 14x14x512=100K params: (3x3x512)x512 = 2,359,296 POOL2: [7x7x512] memory: 7x7x512=25K params: 0 FC: [1x1x4096] memory: 4096 params: 7x7x512x4096 = 102,760,448 FC: [1x1x4096] memory: 4096 params: 4096x4096 = 16,777,216 FC: [1x1x1000] memory: 1000 params: 4096x1000 = 4,096,000 VGGNet网络特点 更小的filters，更深的网络； Only 3x3 CONV stride 1, pad 1 and 2x2 MAX POOL stride 2； 倒数第二层FC（FC7，1000个，即类别层之前）的hidden number=4096被验证已经能够很好地进行特征表达，可以用于在其他数据中提取特征，并有比较好的泛化性能； 为何使用小的filters（3x3 CONV）：3个3x3的卷积层和1个7x7的卷积层拥有同样有效的感受野（解析可见https://blog.csdn.net/program_developer/article/details/80958716），但是其更深，更非线性化，且其参数数量更少，单层参数数量分别为3x（3x3xCxC）&lt;7x7xCxC（C为channel数量）。 GoogleNetGoogleNet网络结构图如下图所示： GoogleNet网络特点 22层网络； 没有FC层； 为提高计算效率引入了“inception” module和“bottleneck”的概念； 网络有两个辅助输出（Auxiliary classification outputs）； 仅有5million个参数，比AlexNet少12倍。 “inception” module可以将网络看成是由局部网络拓扑（“inception” module）堆叠而成。对进入相同层的相同输入并行应用不同类别的滤波操作。我们将来自前面层的输入进行不同的卷积操作、池化操作，从而得到不同的输出，最后需要将所有输出在深度层面上连接到一起。计算与串联方式如下图所示： “blottleneck”使用 “inception” module 后随之而来的问题就是： 单层的参数就达到854M个，计算量极大； 且数据经过每一个 “inception” module 后深度都会增加（光池化层得到输出数据的尺寸就已经与原数据相同）。 为解决上述问题，构建了称之为“bottleneck”的1x1卷积层以减少特征深度（如下图所示）： 改进后的“inception” module如下图所示： Auxiliary classification outputsGoogleNet同时拥有两个辅助输出，可以对前面几个层进行更多的梯度训练。当网络深度很深的时候，一些梯度信号会最小化并且丢失了前面几层的梯度信号，该方法能在一定层度上解决梯度消失的问题。 ResNetResNet网络结构图如下图所示： ResNet网络特点： 152层； 利用残差层实现优化； 网络由残差盒堆叠而成（每一个残差盒包含两个3x3 CONV）； 如果将残差盒中的所有权重置零，那么残差盒的输出与输入就是相等的，因此，在某种层度上，这个模型是相对容易去训练的，并不需要添加额外的层； 神经网络中添加L2正则化的作用是迫使网络中的所有参数趋近于0，其他网络结构（如CONV）参数趋于0不太说得通。但在残差网络中，如果所有参数趋于0，那就是促使模型不再使用他不需要的层，因为它只趋使残差盒趋向同一性，也就不需要进行分类； 残差连接在反向传播时，为梯度提供了一个超级“高速通道”（梯度经过残差盒的加法门分流然后汇总），这使得网络训练更加容易（DenseNet和FractalNet也有类似的梯度直传式的连接）； 周期性的，会使用两倍数量的filters，用stride 2 CONV进行下采样（所有维度/2）； 网络起始处有一个额外的CONV； 没有额外的FC； 只有一个全局的AVE POOL； 每一个CONV后都带有一个BN； 用一个额外的带尺度因子的Xavier/2去初始化； 初始学习率为0.1，当Validation error停滞时，将其缩小十倍； Mini-batch size = 256； Weight decay = 1e-5. 残差盒（Residual block）当我们在普通神经网络上堆叠越来越多的层时到底会发生什么？由上图实验结果可知，56层网络的训练误差和测试误差都高于20层的网络。但是为何56层本应发生过拟合的NN的训练误差不如20层的NN呢，它最差也应该和20层的NN性能一样才对？ 本文作者假设：这是一个优化问题，层深的模型更难优化。于是，作者提出了残差网络的概念，其与常规网络的区别如下图所示：Residual block使这些网络层拟合的是残差映射H(x)-x而不是直接映射H(x)。某种意义上可以看成是一种对输入的修正。学习残差映射你只需知道什么是∆x=H(x)-x，通常来说，很多网络层之间实际上都是相差无几的，通过学习一个恒等映射加上很小的∆x（若恒等映射是最好的，只需将∆x置零），这样更容易学习。 与GoogleNet类似，如果网络层数较多的话，ResNet的残差盒会使用“bottleneck”来加速计算（如下图所示）。 扩展Network in Network（NiN）每个卷积层中都有一个完全连接的MLP(micronetwork)，以计算局部图像块的更多抽象特征。这个模型是GoogleNet和ResNet模型“bottleneck”的灵感来源。 Identity Mappings in Deep Residual Networks在ResNet的基础上进行修改，新的结构能够实现一种更直接的路径用于在整个网络中传播信息（将激活层移动到残差映射路径中）。 Wide Residual Networks作者认为残差量是一个十分重要的因素而不是深度。使用了更宽的残差模块（FxK filters而不是F filters in each layer），宽网络的另一个优点是便于使用并行计算。本文旨在比较网络的宽度、深度和残差连接所做出的的贡献。 ResNeXt通过多条平行路径增加残差盒宽度，这些分支总和被称为“cardinality”，思想类似于“inception”module。 Deep Networks with Stochastic Depth动机是在训练过程中通过短网络减少消失梯度和训练时间。该思想类似于Dropout，只不过这里是沿网络深度方向的dropout。方法是在每次训练中随机drop某层子集（即ResNet中∆x=0，该层为恒等映射），在测试时使用完整的训练好的网络。 FractalNet作者认为引入残差可能不是必须的，关键在于有效地从浅层网络转型为深层网络。因此他使用了上图所示这种分型结构，各层都以分形的形式存在，因此同时存在浅层和深层路径到大输出值。他们通过抛弃子路径的方式训练，类似于dropout，测试时使用整个分形网络。 DenseNet密集连接卷积神经网络。每个Dense block中每层都与其后的所有层以前馈的形式连接，因此在这个Dense block内，你对其的输入也是对所有其他各层的输入，你会计算每一个卷积输出，这些输出与其后的所有层连接，所有这些值集中起来，共同作为卷积层的输入。这一方法能缓解梯度消失的问题，加强特征图的传递，鼓励特征重用。 SqueezeNet由一个个fire模块组成，每个fire模块都含有一个squeeze层，其由许多1x1的filters组成。接着，它再传递给一个扩大层含有一些1x1和3x3的filters。参数只有AlexNet的1/50，性能相似。]]></content>
      <categories>
        <category>CS231n</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Deep Leanring</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n Convolutional Neural Networks for Visual Recognition Summary and Assignments]]></title>
    <url>%2F2018%2F09%2F24%2FCS231n%20Convolutional%20Neural%20Networks%20for%20Visual%20Recognition%20Summary%20and%20Assignments%2F</url>
    <content type="text"><![CDATA[CS231n课程大家都很熟悉了，深度学习入门必备课程。这里就不多介绍了，只对课程资源进行归纳汇总，分享一下自己学习该课程后完成的作业，以供一起学习的同学们参考、交流。由于该课程的课件较为精炼，没有长篇大论，且知乎有全套的课件翻译，因此这里暂不对该课程知识点进行归纳总结，后续学习中如果有需要提炼的地方会对本文进行更新。 课程资源课程地址：http://cs231n.stanford.edu/课程视频(EN, Spring 2017)：https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv课程地址(CN, Spring 2017)：http://www.mooc.ai/course/268知乎课件翻译地址(Winter 2016)：https://zhuanlan.zhihu.com/p/22339097 课程作业我在学习这门课程的时候，作业是从课程官网下载的，因此版本是Spring 2018。相较于之前的作业版本，Spring 2018 Assignment 2 中新加入了Layer Normalization和Group Normalization的内容以及PyTorch和Tensorflow的相关练习。 Assignments还在更新中，目前Assignments 2已经完成，后续会继续更新Assignments 3。课程Assignments详见：https://github.com/KunBB/cs231n_assignment 课程小结Lecture 1 ~ Lecture 7见知乎的翻译课件，或是CS231n官网的英文课件； Lecture 8主要讲解了一些主流的深度学习框架、PyTorch和Tensorflow的基本使用流程等，这部分内容通过完成最新的Assignment 2中的PyTorch或Tensorflow就差不多可以掌握了。 Lecture 9Lecture 9主要讲了一些经典的、比较流行的网络结构，详细讲解了AlexNet、ZFNet、VGGNet、GoogleNet和ResNet。 Lecture 10Lecture 10主要讲解了循环神经网络模型的结构、前向传播与反向传播，并由此引出了Attention机制、LSTM（长短期记忆网络）等，除此之外还介绍了图像标注、视觉问答等前沿问题。 Lecture 11Lecture 11主要讲解的是分割、定位与检测。具体包括语义分割、分类定位、目标检测和实例分割四部分。 Lecture 12Lecture 12主要讲解的是对卷积神经网络的可视化和解释性的研究，从网络层的特征可视化开始到基于梯度提升方法的特征匹配、特征反演，进而衍生出纹理合成、图像生成和风格转移等。 Lecture 13Lecture 13主要讲解了无监督模型和生成模型，其中详细介绍了生成模型中的pixelRNN、pixelCNN、VAE、GAN等图像生成方法。]]></content>
      <categories>
        <category>CS231n</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Deep Leanring</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reinforcement Learning：An Introduction Chapter 1 学习笔记]]></title>
    <url>%2F2018%2F09%2F19%2FReinforcement%20Learning%EF%BC%9AAn%20Introduction%20Chapter1%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Chapter 1: Introduction人类与环境进行互动，学习环境如何响应我们的行为，并试图通过自身行为影响将来发生的事，这就是一种交互式的学习方式，是人类获取知识的主要来源，同时也是几乎所有学习和智能化理论的基本思想。强化学习正是一种从交互中学习的计算方法，它更侧重于从交互中进行目标导向的学习方式，而不是其他的机器学习方式。 1.1 Reinforcement Learning强化学习特征强化学习就是学习该做什么，如何将情境映射到动作从而最大化奖励信号。试错搜索（trial-and-error search）和延迟奖励（delayed reward）是强化学习两个最重要的显著特征，另一个重要特征是强化学习并不局限于孤立的子问题，即：· 学习者不会被告知需要采取哪些行动，而是必须通过尝试来发现哪些行动可以产生最大的回报；· 当前行动不仅影响即时奖励，还会影响下一个state，以及后续奖励；· 明确考虑了目标导向的agent与不确定环境交互的整个问题。 强化学习与其他人工智能技术的区别监督学习：是从一组有标记的训练集中进行学习，目的是让系统归纳与推断其响应，使其在训练集中不存在的样例下也能正确做出相应action。监督学习是一种重要的学习方式，但其不足以从交互中学习。在交互问题中获取正确而又代表所有情况的所期望行为的样例是不切实际的。在未知领域，agent必须能够从自身经验中学习才能习得最有益的action。 非监督学习：通常是寻找隐藏在未标记数据集合中的某种结构。虽然强化学习也不需要带有正确标记的例子，但它的目标是最大化奖励信号，而不是试图找到隐藏的结构。当然，找到agent学习经验中的隐藏结构也是有用的，但这并不是最终目标。 强化学习的挑战探索与开发的权衡（trade-off between exploration and exploitation）。为了获得大量奖励，agent必须更倾向于过去尝试过的行为，并且发现他们能够有效地产生奖励。但是要发现这样的行为，agent必须尝试以前没有尝试过的行为，它必须利用它已经经历的经验来获得奖励，但也必须进行探索，以便在将来做出更好的选择。困难在于，任何探索和开发都有可能会失败，agent必须尝试各种操作，并逐渐倾向于那些看起来最好的操作。在随机任务中，必须多次尝试每一个action以获得对其期望奖励的可靠估计。 1.3 Elements of Reinforcement Learning策略（policy）：策略定义了agent在给定时间内的行为方式。策略是从感知的环境状态到该state下action的映射。通常策略可以是随机的，指定每个action的概率。 奖励信号（reward signal）：奖励信号定义了强化学习问题的目标，是agent一次action后的反馈，说明了agent某个action对于目标而言是有利的还是有害的。奖励信号是更改策略的基础，如果回报低，下次遇到相同的情况，agent就会采取不同的action。agent唯一的目标是最大化累计获得的奖励。 值函数（value function）：state的值表示agent以该state为起点，未来可期望的各个state回报的总和。奖励信号表示该state直接意义上的好坏，但值函数表示了以该state为起点，长期运行中的好坏。我们寻求的action应该是带来最高value而非最高reward。 环境模型（model of the environment）：利用models来解决强化学习的方法为model-based method，反之叫做model-free method。对环境进行建模，不必在真实环境中试验每一action，给定state和action，model会给出下一个state和返回的reward，极大减小了试错搜索的成本，是未来新的发展方向。 1.4 Limitations and Scope进化方法（evolutionary methods）：如果策略空间较小，或可以被结构化（好的策略容易被检索到），或者有大量时间可以用于搜索，则进化方法是可行的。此外，进化方法在agent无法感知其环境的完整状态的问题上具有优势。 强化学习方法：进化方法（EM）只看policy的最后结果而不考虑中间的演变的过程。而强化学习方法在与环境的交互中学习，许多情况下，可以利用个体行为相互作用的细节。进化方法忽略了强化学习问题的许多有用结构：EM没有利用所搜索的policy是states到actions的映射这个事实；EM没有注意到agent一生经过了哪些states，选择了哪些actions。虽然某些情况下，该信息可能具有误导性（例如state被误观察），但更一般的，这些信息会带来更高效的搜索。 1.5 An Extended Example: Tic-Tac-Toe优化方法对比以“井”字游戏为例说明了传统的AI方法如minimax、dynamic programming、evolutionary method都不太适合即使是这么简单的RL问题。 经典的博弈论的minimax解决方案在这里是不正确的，因为它假定了对手的特定玩法。 用于顺序决策问题的经典优化方法，例如dynamic programming，可以为任何对手计算最优解，但需要输入该对手的完整规范，包括对手在每个棋盘状态下进行每次移动的概率。 为了评估策略,进化方法使得策略固定并且针对对手玩许多次游戏，或者使用对手的模型模拟许多次游戏。胜利的频率给出了对该策略获胜概率的无偏估计，并且可用于指导下一个策略的选择。但是每次策略更改都是在许多游戏之后进行的，并且只使用每个游戏的最终结果：在游戏期间发生的事情会被忽略。例如，如果玩家获胜，那么游戏中的所有行为都会被信任，而不管具体哪些actions对获胜至关重要，甚至可以归功于从未发生过的actions。 相反值函数方法允许评估各个states。最后，进化和值函数方法都在搜索策略空间，但学习值函数会利用游戏过程中可用的信息。 value function方法步骤 建立数据表，每个数据都代表游戏中的一个可能state，每个数字都是我们从该state获胜概率的最新估计； 假设我们总是玩X，那么连续三个X的value是1，连续三个O的value为0，其他状态的初始值设置为0.5，表示我们有50％的获胜机会； 进行多场游戏，大多数时候我们采用贪婪式方法，选择导致具有最大value的state移动，即具有最高的估计获胜概率。但偶尔也会采取随机下法即探索性动作； 在贪婪选择时，使用时间差分法（temporal-di↵erence）更新之前state的value：α为步长，S为state，V()为value。 可以通过改变α慢慢趋向于0使得这个方法收敛到一个最优策略；也可以不改变α使得策略不断改变以对抗对手。 扩展本小节引出如下几点思考： 将先验知识应用到强化学习中可以改善学习效果； 强化学习的动作，除了像这个游戏这种离散的，也可能是连续的，value函数也可能是连续函数； 监督学习为程序提供了从其经验中概括（泛化）的能力。因此当状态集非常大甚至无限时，将监督学习方法与强化学习方法相结合是一个很好的解决途径。ANN和DL并不是唯一的或最好的方法； 如果能够获得或构建一个环境模型，则强化学习效果与效率会更好更高。 1.7 Early History of Reinforcement Learning本小节讲述了RL的三条研究主线： learning with trial and error； optimal control and its solution using value functions and dynamic programming(planning)； TD-methods。 ExerciseExercise 1.1：Self-Play Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?译：假设上述强化学习算法不是与随机对手比赛，而是双方都在学习。 在这种情况下你认为会发生什么？ 是否会学习选择行动的不同策略？ 答：对于固定的对手来说，算法可能是次优的，对于随机对手而言，算法可能是最优的。对抗式学习和足够的探索相较于一个固定的对手可以产生更为强大的智能体。最后两个智能体应该会达到某种动态平衡，或是某方一直输，另一方一直赢（初始动作或顺序可能影响了学习策略）。 Exercise 1.2：Symmetries Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the learning process described above to take advantage of this? In what ways would this change improve the learning process? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?译：许多井字位置看起来不同，但由于对称性，它们实际上是相同的。我们如何修改上述学习过程以利用这一点？这种变化会以何种方式改善学习过程？ 现在再想一想。假设对手没有利用对称性。在那种情况下，我们应该吗？那么，对称等价的位置是否必须具有相同的值？ 答：可以依据4个轴的对称性对状态空间进行约减，即对称移动视为属于相同的状态空间，进而将减少实际的状态数量，加速学习。如果对手没有利用对称性，则其策略会区分“对称”状态，这可能导致我们算法整体性能变差。例如，如果对手在一个状态空间中存在弱点但在另一个状态空间中没有（即使它们是对称的），则对称相似的状态应该具有相同的值是不正确的，因此，这种情况下我们也不应该使用对称性。 Exercise 1.3：Greedy Play Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a nongreedy player? What problems might occur?译：假设强化学习者是贪婪的，也就是说，它总是做出能够把它带到它认为最好的位置的动作。它可能会比一个不贪婪的学习者学的更好或更差吗？可能会出现什么问题？ 答：一般而言，贪婪玩家的学的可能会更差。贪婪玩家会追求最大的即时reward，而好的学习策略应该是追求最大的value，即累积回报。如果每一步都追求最好的动作，我们可能永远找不到最优解。贪婪玩家可能会陷入局部最优点。 Exercise 1.4：Learning from Exploration Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a different set of probabilities. What (conceptually) are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?译：假设在所有动作之后发生了学习更新，包括探索性动作。 如果步长参数随时间适当减小（但不是探索的趋势），则状态值将收敛到不同的概率集。 什么（概念上）是我们这样做时计算的两组概率，当我们不这样做时，从探索性动作中学习？ 假设我们继续做出探索性的动作，哪一组概率可能更好学习？ 哪会赢得更多？ 答：一个状态的值是从一个状态开始直到获胜的可能性。随着步长的适当减少，且假设探索的概率是固定的，没有从探索中学习的概率集是给定从此采取的最佳行动的每个状态的值，而从探索中学习的概率集是包括主动探索策略在内的每个状态的期望值。使用前者能够更好地学习，因为它避免了算法一味地进行贪婪式的行动，可能到达一个一般来说我们永远不会到达的状态，进而减少了次优的未来状态的偏差（例如，如果你可以在一次移动中赢得一盘棋，但如果你执行另一次移动你的对手获胜，这不会使该状态变坏）。前者会在所有其他条件相同的情况下获得更多胜利。 Exercise 1.5：1.5 Other Improvements Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?译：你能想到其他改善强化学习者的方法吗？你能想出更好的方法来解决所提出的井字游戏问题吗？ 答：根据对手行为的变化改变探索率。加大损失的惩罚力度。 Reference:[1] 《reinforcement learning：an introduction》第一章《The Reinforcement Learning Problem》总结：https://blog.csdn.net/mmc2015/article/details/74931291[2] 强化学习经典入门书的读书笔记系列—第一篇：https://zhuanlan.zhihu.com/p/27133367[3] Reinforcement Learning：An Introduction 读书笔记- Chapter 1：https://blog.csdn.net/PeytonPu/article/details/78450681[4] rl-book-exercises：https://github.com/jlezama/rl-book-exercises]]></content>
      <categories>
        <category>Reinforcement Learning</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Artificial Intelligence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Qt+webservice的多线程实现]]></title>
    <url>%2F2018%2F08%2F13%2FQt%2Bwebservice%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[项目使用Qt搭建了一个数据库软件，需要远程访问公司的MES系统，使用webservice技术进行通信并以XML格式传输数据，为了使网络监听过程中不影响主线程程序的正常运行，我们需要将webservice相关功能放在新开的独立线程中。 本项目使用的是QtCreator(Qt5.5.0)+VisualStudio2013+gSOAP2.8搭建。其他版本只要版本是正确对应的，都大同小异。 WebService相关概念参见参考文献[3]。 WebService是一种跨编程语言和跨操作系统平台的远程调用技术。所谓跨编程语言和跨操作平台，就是说服务端和和客户端的搭建平台与编程语言可以都不相同。所谓远程调用，就是一台计算机a上 的一个程序可以调用到另外一台计算机b上的一个对象的方法。 其实可以从多个角度来理解WebService，从表面上看，WebService就是一个应用程序向外界暴露出一个能通过Web进行调用的API，也就是说能用编程的方法通过 Web来调用这个应用程序。我们把调用这个WebService的应用程序叫做客户端，而把提供这个WebService的应用程序叫做服务端。从深层次看，WebService是建立可互操作的分布式应用程序的新平台，是一个平台，是一套标准。它定义了应用程序如何在Web上实现互操作性，你可以用任何 你喜欢的语言，在任何你喜欢的平台上写Webservice ，只要我们可以通过Webservice标准对这些服务进行查询和访问。 WebService平台需要一套协议来实现分布式应用程序的创建。任何平台都有它的数据表示方法和类型系统。要实现互操作性，WebService平台必须提供一套标准的类型系统，用于沟通不同平台、编程语言和组件模型中的不同类型系统。Webservice平台必须提供一种标准来描述Webservice，让客户可以得到足够的信息来调用这个Webservice。最后，我们还必须有一种方法来对这个Webservice进行远程调用,这种方法实际是一种远程过程调用协议(RPC)。为了达到互操作性，这种RPC协议还必须与平台和编程语言无关。 XML+XSD,SOAP和WSDL就是构成WebService平台的三大技术。 XML+XSDWebService采用HTTP协议传输数据，采用XML格式封装数据(即XML中说明调用远程服务对象的哪个方法，传递的参数是什么，以及服务对象的 返回结果是什么)，XML是WebService平台中表示数据的格式，其优点在于它既是平台无关又是厂商无关的。XML解决了数据表示的问题，但它没有定义一套标准的数据类型，更没有说怎么去扩展这套数据类型。例如整形数到底代表什么？16位，32位，64位？这些细节对实现互操作性很重要。XML Schema(XSD)就是专门解决这个问题的一套标准。它定义了一套标准的数据类型，并给出了一种语言来扩展这套数据类型。WebService平台就是用XSD来作为其数据类型系统的。当你用某种语言(如VB.NET或C#)来构造一个Webservice时，为了符合WebService标准，所有你使用的数据类型都必须被转换为XSD类型。你用的工具可能已经自动帮你完成了这个转换，但你很可能会根据你的需要修改一下转换过程。 SOAPSOAP是”简单对象访问协议”，SOAP协议 = HTTP协议 + XML数据格式。WebService通过HTTP协议发送请求和接收结果时，发送的请求内容和结果内容都采用XML格式封装，并增加了一些特定的HTTP消息头，以说明HTTP消息的内容格式，这些特定的HTTP消息头和XML内容格式就是SOAP协议。SOAP提供了标准的RPC方法来调用WebService。SOAP协议定义了SOAP消息的格式，SOAP协议是基于HTTP协议的，SOAP也是基于XML和XSD的，XML是SOAP的数据编码方式。打个比喻：HTTP就是普通公路，XML就是中间的绿色隔离带和两边的防护栏，SOAP就是普通公路经过加隔离带和防护栏改造过的高速公路。 WSDL好比我们去商店买东西，首先要知道商店里有什么东西可买，然后再来购买，商家的做法就是张贴广告海报。 WebService也一样，WebService客户端要调用一个WebService服务，首先要有知道这个服务的地址在哪，以及这个服务里有什么方法可以调用，所以WebService务器端首先要通过一个WSDL文件来说明自己家里有啥服务可以对外调用，服务是什么（服务中有哪些方法，方法接受的参数是什么，返回值是什么），服务的网络地址用哪个url地址表示，服务通过什么方式来调用。WSDL(Web Services Description Language)就是这样一个基于XML的语言，用于描述WebService及其函数、参数和返回值。它是WebService客户端和服务器端都能理解的标准格式。因为是基于XML的，所以WSDL既是机器可阅读的，又是人可阅读的，这将是一个很大的好处。一些最新的开发工具既能根据你的Webservice生成WSDL文档，又能导入WSDL文档，生成调用相应WebService的代理类代码。WSDL文件保存在Web服务器上，通过一个url地址就可以访问到它。客户端要调用一个WebService服务之前，要知道该服务的WSDL文件的地址。 WebService服务提供商可以通过两种方式来暴露它的WSDL文件地址：1.注册到UDDI服务器，以便被人查找；2.直接告诉给客户端调用者。 gSOAP总结gsoap概念：是一种能够把C/C++语言的接口转换成基于soap协议的webservice服务的工具。从官网的说明文档可知gSOAP可以为我们完成以下工作： 1、自动生成C和C++源代码，以使用和部署XML、JSON REST API以及SOAP/XML API；2、使用gSOAP的快速XML流处理模型进行XML解析和验证，实现实现可移植的快速的和精简的API，每秒处理10K+消息仅需几KB代码和数据；3、将WSDL转换为有效的C或C++源代码以使用或部署XML Web服务；4、将XML schemas(XSD)转换为高效的C或C++源代码，以使用gSOAP全面的XML schemas功能覆盖来使用或部署XML REST API；5、为WSDL和/或XSD定义的大型复杂XML规范生成高效的C和C ++代码，例如eBay，ONVIF，HL7，FHIR，HIPAA 5010，CDISC，XMPP XEP，TR-069，AWS，EWS，ACORD，ISO 20022和SWIFT，FixML，XBRL，OTA，IATA NDC，FedEx等（您甚至可以将多个组合在一起）；6、安全的XML处理过程：gSOAP不容易受到大部分XML攻击手段的攻击；7、使用强大的XML数据绑定准确地序列化XML中的C和C++数据，这有助于通过静态类型快速开发类型安全的API，以避免运行时错误；8、在WSDL和XSD文件上使用wsdl2h选项-O2或-O4进行“schema slicing”，通过自动删除未使用的schema类型（WSDL和XSD根目录中无法访问的类型）来优化XML代码库的大小；9、使用和部署JSON REST API；10、使用SAML令牌安全地使用HTTP/S，WS-Security和WS-Trust来使用和部署API;11、使用测试信使CLI测试您的SOAP/XML API，它会自动生成基于WSDL的完整和随机化的SOAP/XML消息（使用带有选项-g的wsdl2h和soapcpp2）来测试Web服务API和客户端；12、使用gSOAP Apache模块在Apache Web服务器中部署API，在IIS中使用gSOAP ISAPI扩展部署API；13、使用gSOAP cURL插件和gSOAP WinInet插件来使用API；14、符合XML的行业标准，包括XML REST和SOAP，WSDL，MIME，MTOM，DIME，WS-Trust，WS-Security（集成了WS-Policy和WS-SecurityPolicy），WS-ReliableMessaging，WS-Discovery，WS-Addressing等； gSOAP简介gSOAP一种跨平台的开源的C/C++软件开发工具包。生成C/C++的RPC代码，XML数据绑定，对SOAP Web服务和其他应用形成高效的具体架构解析器，它们都受益于一个XML接口。 这个工具包提供了一个全面和透明的XML数据绑定解决方案，Autocoding节省大量开发时间来执行SOAP/XML Web服务中的C/C++。此外，使用XML数据绑定大大简化了XML自动映射。应用开发人员不再需要调整应用程序逻辑的具体库和XML为中心的数据。 gSOAP结构使用gSOAP首先需要用到了两个工具就是../gsoap-2.8/gsoap/bin/win32/wsdl2h.exe和../gsoap-2.8/gsoap/bin/win32/soapcpp2.exe，用于自动生成包含接口的c/c++源文件。 wsdl2h.exe该工具可以根据输入的wsdl或XSD或URL产生相应的C/C++形式的.h供soapcpp2.exe使用。示例如下： 新建一个文件夹，将wsdl2h.exe(和.wsdl文件)放入，命令行进入当前路径后输入以下命令：1234wsdl2h [options] XSD and WSDL files ...wsdl2h -o file.h file1.wsdlwsdl2h -o file.h http://www.genivia.com/calc.wsdl 根据WSDL自动生成file.h头文件，以供soapcpp2.exe使用。 wsdl2h主要的运行选项如下： 选项 描述 -a 对匿名类型产生基于顺序号的结构体名称 -b 提供单向响应消息的双向操作（双工） -c 生成c代码 -c++ 生成c++代码 -c++11 生成c++11代码 -d 使用DOM填充xs：any和xsd：anyType元素 -D 使用指针使具有默认值的属性成员可选 -e 不要限定枚举名称，此选项用于向后兼容gSOAP 2.4.1及更早版本，该选项不会生成符合WS-I Basic Profile 1.0a的代码。 -f 为schema扩展生成平面C++类层次结构 -g 生成全局顶级元素声明 -h 显示帮助信息 -I path 包含文件时指明路径，相当于#import -i 不导入（高级选项） -j 不生成SOAP_ENVHeader和SOAP_ENVDetail定义 -k 不生成SOAP_ENV__Header mustUnderstand限定符 -l 在输出中包含许可证信息 -m 使用xsd.h模块导入基元类型 -N name 用name 来指定服务命名空间的前缀 -n name 用name 作为命名空间的前缀取代缺省的ns -O1 通过省略重复的选择/序列成员来优化 -O2 优化-O1并省略未使用的模式类型（从根目录无法访问） -o file 输出文件名 -P 不要创建从xsd__anyType继承的多态类型 -p 创建从base xsd__anyType继承的多态类型，当WSDL包含多态定义时，会自动执行此操作 -q name 使用name作为所有声明的C++命名空间 -R 在WSDL中为REST绑定生成REST操作 -r host[:port[:uid:pwd]] 通过代理主机，端口和代理凭据连接 -r:uid:pwd 连接身份验证凭据（验证身份验证需要SSL） -s 不生成STL代码（没有std :: string和没有std :: vector） -t file 使用类型映射文件而不是默认文件typemap.dat -U 将Unicode XML名称映射到UTF8编码的Unicode C/C++标识符 -u 不产生工会unions -V 显示当前版本并退出 -v 详细输出 -W 抑制编译器警告 -w 始终在响应结构中包装响应参数 -x 不生成_XML any/anyAttribute可扩展性元素 -y 为结构和枚举生成typedef同义词 soapcpp2.exe该工具是一个根据.h文件生成若干支持webservice代码文件生成工具，生成的代码文件包括webservice客户端和服务端的实现框架，XML数据绑定等，具体说明如下： 新建一个文件夹，将soapcpp2.exe和.h文件放入，命令行进入当前路径后输入以下命令：1234soapcpp2 [options] header_file.hsoapcpp2 mySoap.hsoapcpp2 -c -r -CL calc.h 会生成如下c类型文件： 使用命令-i会生成如下c++类型文件： 文件 描述 soapStub.h 根据输入的.h文件生成的数据定义文件，一般我们不直接引用它 soapH.h 所有客户端服务端都应包含的主头文件 soapC.cpp 指定数据结构的序列化和反序列化方法 soapClient.cpp 用于远程操作的客户端存根例程 soapServer.cpp 服务框架例程 soapClientLib.cpp 客户端存根与本地静态(反)序列化器结合使用 soapServerLib.cpp 服务框架与本地静态(反)序列化器结合使用 soapXYZProxy.h 使用选项-i：c++服务端对象(与soapC.cpp和soapXYZProxy.cpp链接) soapXYZProxy.cpp 使用选项-i：客户端代码 soapXYZService.h 使用选项-i：server对象(与soapC.cpp和soapXYZService.cpp链接) soapXYZService.cpp 使用选项-i：服务端代码 .xsd ns.XSD由XML Schema生成，ns为命名空间前缀名，我们可以看看是否满足我们的协议格式（如果有此要求） .wsdl ns.wsdl由WSDL描述生成 .xml 生成了几个SOAP/XML请求和响应文件。即满足webservice定义的例子message(实际的传输消息)，我们可以看看是否满足我们的协议格式(如果有此要求) .nsmap 根据输入soapcpp2.exe的头文件中定义的命名空间前缀ns生成ns.nsmap，该文件包含可在客户端和服务端使用的命名空间映射表 soapcpp的主要运行选项如下： 选项 描述 -1 生成SOAP 1.1绑定 -2 生成SOAP 1.2绑定 -0 没有SOAP绑定，使用REST -C 仅生成客户端代码 -S 仅生成服务端代码 -T 生成服务端自动测试代码 -Ec 为深度数据复制生成额外的例程 -Ed 为深度数据删除生成额外的例程 -Et 使用walker函数为数据遍历生成额外的例程 -L 不生成soapClientLib / soapServerLib -a 使用SOAPAction和WS-Addressing来调用服务端操作 -A 要求SOAPAction调用服务端操作 -b 序列化字节数组char [N]为字符串 -c 生成纯C代码 -d 保存到指定目录中 -e 生成SOAP RPC编码样式绑定 -f N 多个soapC文件，每个文件有N个序列化程序定义（N≥10） -h 打印一条简短的用法消息 -i 生成从soap struct继承的服务代理类和对象 -j 生成可以共享soap结构的C++服务代理类和对象 -I 包含其他文件时使用，指明 &lt; path &gt; (多个的话，用`:’分割)，相当于#import ，该路径一般是gSOAP目录下的import目录，该目录下有一堆文件供soapcpp2生成代码时使用 -l 生成可链接模块（实验） -m 为MEX编译器生成Matlab代码 -n 用于生成支持多个客户端和服务器端 -p 生成的文件前缀采用&lt; name &gt; ，而不是缺省的 “soap” -q 使用name作为c++所有声明的命名空间 -r 生成soapReadme.md报告 -s 生成的代码在反序列化时，严格检查XML的有效性 -t 生成的代码在发送消息时，采用xsi:type方式 -u 通过抑制XML注释来取消注释WSDL / schema输出中的注释 -V 显示当前版本并退出 -v 详细输出 -w 不生成WSDL和schema文件 -x 不生成示例XML消息文件 -y 在示例XML消息中包含C / C ++类型访问信息 实例介绍功能介绍1、客户端能够向服务端发送字符串数据；2、服务端能够接收到客户端发送的字符串数据；3、服务端对字符串数据解析、查重并录入数据库；4、软件窗口显示连接状态。 最终效果如下所示：点击“开始连接”按钮： 从客户端接收到一组数据后： 实例步骤1. 生成源代码由于没有.wsdl文件，因此我们跳过wsdl2h.exe这一步骤，手动编写供soapcpp2.exe使用的头文件mySoap.h：123456//gsoap ns service name: sendMsg//gsoap ns service style: rpc//gsoap ns service encoding: encoded//gsoap ns service namespace: urn:sendMsgint ns__sendMsg(char* szMsgXml,struct nullResponse&#123;&#125; *out); 新建文件夹，将soapcpp2.exe和mySoap.h放入，打开命令后进入该目录下，运行命令soapcpp2 mySoap.h，生成如下文件： 2. 客户端程序编写在QtCreator中向客户端工程目录添加文件sendMsg.nsmap、soapH.h、soapStub.h、stdsoap2.h、soapC.cpp、soapClient.cpp、stdsoap2.cpp(stdsoap2.h和stdsoap2.cpp位于文件夹../gsoap-2.8/gsoap)。 客户端项目构建的是一个控制台程序，因此直接在main函数里进行编写代码：12345678910111213141516171819202122232425262728293031#include &lt;QCoreApplication&gt;#include "stdio.h"#include "gservice.nsmap"#include "soapStub.h"int main(int argc, char *argv[])&#123; QCoreApplication a(argc, argv); printf("The Client is runing...\n"); struct soap *CalculateSoap = soap_new(); char server_addr[] = "127.0.0.1:8080"; // url地址，IP号+设备端口号 char* data = "flange,6,7,8,9,10,11,12,13"; // 所需传递的数据 nullResponse result; // 用于sendMsg的空返回值 int iRet = soap_call_ns__sendMsg(CalculateSoap,server_addr,NULL,data,&amp;result); // 调用之前定义的ns__sendMsg方法(即服务端提供的方法) if ( iRet == SOAP_ERR)&#123; printf("Error while calling the soap_call_ns__sendmsg"); &#125; else &#123; printf("Calling the soap_call_ns__add success。\n"); &#125; soap_end(CalculateSoap); soap_done(CalculateSoap); return a.exec();&#125; 3. 服务端程序编写在QtCreator中向服务端工程目录添加文件sendMsg.nsmap、soapH.h、soapStub.h、stdsoap2.h、soapC.cpp、soapServer.cpp、stdsoap2.cpp。 由于服务端网络通信功能需要不断对端口进行监听，因此为避免影响软件其他功能的运行，在此需要新开一条线程。项目头文件源码如下：socketconnect.h:12345678910111213141516171819202122232425262728293031323334353637#ifndef SOCKETCONNECT_H#define SOCKETCONNECT_H#include &lt;QWidget&gt;#include &lt;QDebug&gt;#include &lt;QVector&gt;#include "webservice_thread.h" // 多线程头文件#include &lt;QMessageBox&gt;namespace Ui &#123;class SocketConnect;&#125;class SocketConnect : public QWidget&#123; Q_OBJECTpublic: void setDbconn(QSqlDatabase *dbconn); // 主线程数据库 explicit SocketConnect(QWidget *parent = 0); ~SocketConnect();private slots: void pB_lj_clicked(); // 连接按钮 void pB_dk_clicked(); // 断开连接按钮 void linkState_accept(QVector&lt;QString&gt;); // 用于在窗口显示连接状态 void data_accept(QVector&lt;QString&gt;); // 接收数据，检查后录入数据库private: QSqlDatabase *dbconn; WebserviceThread *WebT; QVector&lt;QString&gt; v_linkstate; Ui::SocketConnect *ui;&#125;;#endif // SOCKETCONNECT_H socketconnect.cpp:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126#include "socketconnect.h"#include "ui_socketconnect.h"#include "scrollwidget.h"SocketConnect::SocketConnect(QWidget *parent) : QWidget(parent), ui(new Ui::SocketConnect)&#123; ui-&gt;setupUi(this); ui-&gt;tabWidget-&gt;setTabText(1, QStringLiteral("法兰盘部件")); ui-&gt;tabWidget-&gt;setCurrentIndex(0); ui-&gt;lineEdit-&gt;setText("127.0.0.1"); // 暂时无用 ui-&gt;lineEdit_2-&gt;setText("8080"); ui-&gt;lineEdit_3-&gt;setText("helloworld"); ui-&gt;lineEdit_4-&gt;setText("helloworld"); ui-&gt;lineEdit_4-&gt;setEchoMode(QLineEdit::Password); ui-&gt;tableWidget_fla-&gt;setEditTriggers(QAbstractItemView::NoEditTriggers); // 表格不可编辑 ui-&gt;tableWidget_fla-&gt;setSelectionBehavior(QAbstractItemView::SelectRows); ui-&gt;tableWidget_fla-&gt;setAlternatingRowColors(true); ui-&gt;tableWidget_fla-&gt;setStyleSheet("QTableView&#123;alternate-background-color: rgb(183, 242, 238);&#125;"); // 设置隔行换色 connect(ui-&gt;pushButton_lj,SIGNAL(clicked(bool)),this,SLOT(pB_lj_clicked())); connect(ui-&gt;pushButton_dk,SIGNAL(clicked(bool)),this,SLOT(pB_dk_clicked()));&#125;SocketConnect::~SocketConnect()&#123; delete ui;&#125;void SocketConnect::setDbconn(QSqlDatabase *db)&#123; this-&gt;dbconn=db;&#125;void SocketConnect::pB_lj_clicked()&#123; if(ui-&gt;lineEdit_3-&gt;text()=="helloworld"&amp;&amp;ui-&gt;lineEdit_4-&gt;text()=="helloworld")&#123; WebT = new WebserviceThread(this); int port = ui-&gt;lineEdit_2-&gt;text().toInt(); WebT-&gt;setParameters(port); connect(WebT,SIGNAL(linkState_send(QVector&lt;QString&gt;)),this,SLOT(linkState_accept(QVector&lt;QString&gt;))); connect(ui-&gt;pushButton_dk,SIGNAL(clicked(bool)),WebT,SLOT(stopclick_accept())); connect(WebT,SIGNAL(data_send(QVector&lt;QString&gt;)),this,SLOT(data_accept(QVector&lt;QString&gt;))); WebT-&gt;start(); // 进入线程 &#125; else&#123; QMessageBox::critical(NULL, QString::fromLocal8Bit("警告"), QStringLiteral("账号或密码错误，无法连接！"), QMessageBox::Yes); &#125;&#125;void SocketConnect::linkState_accept(QVector&lt;QString&gt; link_state)&#123; ScrollWidget *s_linkstate = new ScrollWidget; // 自己定义的QWidget的子类，用于增加、更新、清空QList&lt;QWidget*&gt; QString linkstate; for(int i=0;i&lt;link_state.size();i++)&#123; linkstate+=link_state[i]; &#125; v_linkstate.append(linkstate); // 存储线程文件传过来的连接状态 for(int i=0;i&lt;v_linkstate.size();i++)&#123; QLabel *label_linkstate = new QLabel(v_linkstate[i]); s_linkstate-&gt;addWidget(label_linkstate); &#125; s_linkstate-&gt;updateWidget(); ui-&gt;scrollArea-&gt;setWidget(s_linkstate); // 更新scrollarea&#125;void SocketConnect::data_accept(QVector&lt;QString&gt; content)&#123; dbconn-&gt;open(); QString dataClass = content[0]; // 第一个数据存储了数据类型信息 content.erase(content.begin()); //检查编号数据是否重复或为空 dbconn-&gt;open(); if(content[0].isEmpty())&#123; QMessageBox::critical(NULL, QString::fromLocal8Bit("警告"), QStringLiteral("远程传输数据的编号为空，请检查并重新录入！"), QMessageBox::Yes); return; &#125; QSqlQueryModel *model_sql = new QSqlQueryModel; model_sql-&gt;setQuery(QString("SELECT * FROM wbq.%1").arg(dataClass)); for(int k=0 ;k&lt;model_sql-&gt;rowCount(); k++)&#123; QModelIndex index = model_sql-&gt;index(k,0); if(content[0] == model_sql-&gt;data(index).toString())&#123; QMessageBox::critical(NULL, QString::fromLocal8Bit("警告"), QStringLiteral("远程传输数据的编号已存在于数据库中，请检查并重新录入！"), QMessageBox::Yes); return; &#125; &#125; //结束检查 if(dataClass=="flange")&#123; ui-&gt;tabWidget-&gt;setCurrentIndex(1); //数据写入MySQL数据库 QSqlQueryModel *model = new QSqlQueryModel; model-&gt;setQuery(QStringLiteral("INSERT INTO wbq.coordinator SET 编号=%1, ......").arg(content[0])......; // 输入数据 dbconn-&gt;close(); delete model; //数据显示到表格中 int row = ui-&gt;tableWidget_fla-&gt;rowCount()+1; ui-&gt;tableWidget_fla-&gt;setRowCount(row); for(int i=0;i&lt;content.size();i++)&#123; ui-&gt;tableWidget_fla-&gt;setItem(row-1,i,new QTableWidgetItem); ui-&gt;tableWidget_fla-&gt;item(row-1,i)-&gt;setText(content[i]); &#125; &#125; else&#123; QMessageBox::critical(NULL, QString::fromLocal8Bit("警告"), QStringLiteral("信息格式有误或信息错误！"), QMessageBox::Yes); &#125;&#125;void SocketConnect::pB_dk_clicked()&#123; ui-&gt;scrollArea-&gt;takeWidget(); QVector&lt;QString&gt; tmp; v_linkstate.swap(tmp); ui-&gt;tableWidget_fla-&gt;setRowCount(0);&#125; 多线程头文件webservice_thread.h:123456789101112131415161718192021222324252627282930313233343536373839#ifndef WEBSERVICE_THREAD#define WEBSERVICE_THREAD#include&lt;QtSql/QSql&gt;#include&lt;QtSql/qsqlquerymodel.h&gt;#include&lt;QtSql/QSqlQuery&gt;#include&lt;QtSql/qsqldatabase.h&gt;#include&lt;QSqlError&gt;#include&lt;QStandardItem&gt;#include &lt;QThread&gt;#include &lt;QDebug&gt;#include &lt;QVector&gt;#include &lt;QMetaType&gt;#include &lt;QWaitCondition&gt;class WebserviceThread: public QThread&#123; Q_OBJECTpublic: WebserviceThread(QObject *parent=0); ~WebserviceThread(); void run(); void setParameters(int);signals: void linkState_send(QVector&lt;QString&gt;); // 向主线程发送连接状态 void data_send(QVector&lt;QString&gt;); // 向主线程发送从客户端接收的数据private slots: void stopclick_accept(); // 改变runstateprivate: QVector&lt;QString&gt; readXML(QString); int runstate; // 用于终止循环 int nPort;&#125;;#endif // WEBSERVICE_THREAD 多线程文件webservice_thread.cpp：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110#include "webservice_thread.h"#include &lt;sstream&gt;//gsoap文件#include"gservice.nsmap"#include"soapH.h"#include"soapStub.h"#include"stdsoap2.h"#include"stdsoap2.cpp"#include"soapC.cpp"#include"soapServer.cpp"QString Msg; // 存储客户端发送过来的数据WebserviceThread::WebserviceThread(QObject *parent):QThread(parent)&#123; qRegisterMetaType&lt;QVector&lt;QString&gt;&gt;("QVector&lt;QString&gt;"); runstate=0; //置1时停止网络循环&#125;WebserviceThread::~WebserviceThread()&#123;&#125;void WebserviceThread::setParameters(int port)&#123; nPort=port;&#125;int http_get_wbq(soap *);void WebserviceThread::run()&#123; struct soap wbq_soap; soap_init(&amp;wbq_soap); wbq_soap.fget = http_get_wbq; // 网上有人说如果要传输的数据量大的话应该用http post int nMaster = (int)soap_bind(&amp;wbq_soap,NULL,nPort,100); // 端口绑定 if(nMaster&lt;0) soap_print_fault(&amp;wbq_soap,stderr); else&#123; QVector&lt;QString&gt; link_state_1; link_state_1.append(QString("Socket connection successful: master socket = ")); link_state_1.append(QString::number(nMaster, 10)); emit linkState_send(link_state_1); // 发射初始连接后的状态信息 for(int i=0;;i++)&#123; int nSlave = (int)soap_accept(&amp;wbq_soap); // 端口监听，获取客户端连接信息 if(nSlave&lt;0)&#123; soap_print_fault(&amp;wbq_soap,stderr); break; &#125; QVector&lt;QString&gt; link_state_2; link_state_2.append(QString("Times ")); link_state_2.append(QString::number(i, 10)); link_state_2.append(QString(". Accepted connection from ")); link_state_2.append(QString::number(int((wbq_soap.ip&gt;&gt;24)&amp;0xFF), 10)); link_state_2.append(QString(".")); link_state_2.append(QString::number(int((wbq_soap.ip&gt;&gt;16)&amp;0xFF), 10)); link_state_2.append(QString(".")); link_state_2.append(QString::number(int((wbq_soap.ip&gt;&gt;8)&amp;0xFF), 10)); link_state_2.append(QString(".")); link_state_2.append(QString::number(int((wbq_soap.ip)&amp;0xFF), 10)); link_state_2.append(QString(": slave socket = ")); link_state_2.append(QString::number(nSlave, 10)); if(!runstate) emit linkState_send(link_state_2); // 发射客户端连接信息 if(runstate)//点击断开连接后的下一次循环可以ping通，但无法调用服务端的sendMsg方法，即数据无法传送，不会造成数据丢失。 break; if(soap_serve(&amp;wbq_soap)!=SOAP_OK) soap_print_fault(&amp;wbq_soap,stderr); soap_destroy(&amp;wbq_soap); soap_end(&amp;wbq_soap); QVector&lt;QString&gt; data = readXML(Msg); // 解析数据 emit data_send(data); // 向主线程传递解析后的数据 Msg=""; &#125; &#125; soap_done(&amp;wbq_soap);&#125;void WebserviceThread::stopclick_accept()&#123; runstate = 1;&#125;/*此功能是按自己格式解析的*/QVector&lt;QString&gt; WebserviceThread::readXML(QString Msg)&#123; QVector&lt;QString&gt; data; QStringList msglist = Msg.split(","); for(int i=0;i&lt;msglist.size();i++)&#123; data.append(msglist[i]); &#125; return data;&#125;int ns__sendMsg(struct soap *soap, char* smsg, nullResponse* result)&#123; Msg = QString(smsg); return SOAP_OK;&#125;/*用于在网页页面正常显示信息（百度得到的解决方案）*/int http_get_wbq(struct soap *soap)&#123; soap_response(soap, SOAP_HTML); // HTTP response header with text/html soap_send(soap, "&lt;HTML&gt;My Web server is operational.&lt;/HTML&gt;"); soap_end_send(soap); return SOAP_OK;&#125; 存在问题1、gSOAP中有对数据序列化与反序列化的功能，不应按自己的格式来传送数据；2、断开连接按功能在问题，不能及时断开连接。点击“断开连接”按钮后，客户端需要再试图与服务端通信一次，服务端才能真正跳出循环，目前只能做到最后一次通信客户端报错，数据无法传送，不会造成数据丢失。 Reference:[1] Qt中多线程的使用：https://blog.csdn.net/mao19931004/article/details/53131872[2] Qt使用多线程的一些心得——1.继承QThread的多线程使用方法：https://blog.csdn.net/czyt1988/article/details/64441443[3] WebService学习总结(一)——WebService的相关概念：https://www.cnblogs.com/xdp-gacl/p/4048937.html[4] gsoap使用总结：https://blog.csdn.net/byxdaz/article/details/51679117]]></content>
      <categories>
        <category>Qt</category>
      </categories>
      <tags>
        <tag>Qt</tag>
        <tag>Multithreading</tag>
        <tag>webservice</tag>
        <tag>xml</tag>
        <tag>gsoap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Qt+Python混合编程]]></title>
    <url>%2F2018%2F08%2F13%2FQt%2BPython%E6%B7%B7%E5%90%88%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[项目使用Qt搭建了一个数据库软件，这个软件还需要有一些数据分析、特征重要度计算、性能预测等功能，而python的机器学习第三方库比较成熟，使用起来也比较便捷，因此这里需要用到Qt(c++)+python混合编程，在此记录一下相关方法与问题，以方便自己与他人。 本项目使用的是QtCreator(Qt5.5.0)+VisualStudio2013+python3.6.5搭建。其他版本只要版本是正确对应的，都大同小异。 准备工作假设你已经正确安装了Qt和python，由于Qt中的slots关键字与python重复，这里我们需要修改一下文件../Anaconda/include/object.h(注意先将原文件备份)：原文件(448行)：1234567typedef struct&#123; const char* name; int basicsize; int itemsize; unsigned int flags; PyType_Slot *slots; /* terminated by slot==0. */&#125; PyType_Spec; 修改为：123456789typedef struct&#123; const char* name; int basicsize; int itemsize; unsigned int flags; #undef slots // 这里取消slots宏定义 PyType_Slot *slots;/* terminated by slot==0. */ #define slots Q_SLOTS // 这里恢复slots宏定义与QT中QObjectDefs.h中一致&#125; PyType_Spec; 完成上述工作后我们需要在.pro文件中加入python的路径信息(我的python路径是Y:/Anaconda)：123INCLUDEPATH += -I Y:/Anaconda/includeLIBS += -LY:/Anaconda/libs -lpython36 将python3.dll，python36.dll，pythoncom36.dll，pywintypes36.dll放到.exe目录下。 Qt调用python脚本python文件创建一个python脚本放在release项目目录下，这里我们新建了一个kde.py,其中包含无返回值函数plotKDE(x, column, kernel, algorithm, breadth_first, bw, leaf_size, atol, rtol, title)用于绘制核KDE曲线与直方图和有返回值函数loadData()用于读取本地.csv文件，图像绘制效果如下所示： kde.py部分代码如下(为方便表达，后续步骤中我们将plotKDE函数简写为plotKDE(x, column， kernel))：1234567891011121314151617181920212223242526272829303132333435363738import csvimport osimport matplotlib as mplfrom matplotlib import pyplot as pltimport numpy as npfrom sklearn.neighbors import KernelDensitympl.use('TkAgg')mpl.rcParams['font.sans-serif'] = ['SimHei']mpl.rcParams['axes.unicode_minus'] = FalseBASE_DIR = os.path.dirname(__file__)file_path = os.path.join(BASE_DIR, 'train.csv')def plotKDE(x, column, kernel='gaussian', algorithm='auto', breadth_first=1, bw=30, leaf_size=40, atol=0, rtol=1E-8, title): # kde x_plot = np.linspace(min(x), max(x), 1000).reshape(-1, 1) x = np.mat(x).reshape(-1, 1) fig, ax = plt.subplots() kde = KernelDensity(bandwidth=bw, algorithm=algorithm, kernel=kernel, atol=atol, rtol=rtol, breadth_first=breadth_first, leaf_size=leaf_size).fit(x) log_dens = kde.score_samples(x_plot) ax.hist(x, density=True, color='lightblue') ax.plot(x_plot[:, 0], np.exp(log_dens)) plt.title(title[column]) plt.show()def loadData(): x = [] with open(file_path, 'rt') as csvfile: reader = csv.reader(csvfile) for line in reader: tmp = list(map(float, line[4:])) x.append(tmp) return x Qvector转pyObject类型此处新建了一个类用于将Qt中存储的数据的QVector&lt;double&gt;类型等转换为用于python脚本的QObject类型。QVector&lt;QVector&lt;double&gt;&gt;的转换方法可以此类推。12345678910PyObject *Utility::UtilityFunction::convertLabelData(QVector&lt;double&gt; *labels)&#123; int labelSize = labels-&gt;size(); PyObject *pArgs = PyList_New(labelSize); for (int i = 0; i &lt; labelSize; ++i) &#123; PyList_SetItem(pArgs, i, Py_BuildValue("d", (*labels)[i])); &#125; return pArgs;&#125; Qt调用python脚本在Qt项目中调用kde.py的plotKDE函数显示图像(有输入，无返回值)：12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 初始化Py_Initialize();if (!Py_IsInitialized()) &#123; printf("inititalize failed"); qDebug() &lt;&lt; "inititalize failed"; return ;&#125;else &#123; qDebug() &lt;&lt; "inititalize success";&#125;// 加载模块，即loadtraindata.pyPyObject *pModule = PyImport_ImportModule("kde");if (!pModule) &#123; PyErr_Print(); qDebug() &lt;&lt; "not loaded!"; return ;&#125;else &#123; qDebug() &lt;&lt; "load module success";&#125;// QVector&lt;double&gt; pKDE中存放了选中列的所有数据PyObject *pKDEdata = Utility::UtilityFunction::convertLabelData(&amp;pKDE); // 类型转换PyObject *pArg = PyTuple_New(3);PyTuple_SetItem(pArg, 0, pKDEdata);// int column表示选中的列的索引PyTuple_SetItem(pArg, 1, Py_BuildValue("i", column));// Qstring kernel表示核类型PyTuple_SetItem(pArg, 2, Py_BuildValue("s", kernel.toStdString().c_str()));// 加载函数loadData()PyObject *pFunc = PyObject_GetAttrString(pModule, "plotKDE");if (!pFunc) &#123; printf("get func failed!");&#125;else &#123; qDebug() &lt;&lt; "get func success";&#125;PyObject_CallObject(pFunc, pArg);Py_DECREF(pModule);Py_DECREF(pFunc);Py_Finalize(); 在Qt项目中调用kde.py的loadData函数读取本地数据(无输入，有返回值)：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960Py_Initialize();QVector&lt;QVector&lt;double&gt; &gt; *trainData; // 存储python脚本读入的数据if (!Py_IsInitialized()) &#123; printf("inititalize failed"); qDebug() &lt;&lt; "inititalize failed"; return ;&#125;else &#123; qDebug() &lt;&lt; "inititalize success";&#125;// 添加当前路径(读文件的时候才需要)PyRun_SimpleString("import sys");PyRun_SimpleString("sys.path.append('./')");// 加载模块，即loadtraindata.pyPyObject *pModule = PyImport_ImportModule("kde");if (!pModule) &#123; PyErr_Print(); qDebug() &lt;&lt; "not loaded!"; return ;&#125;else &#123; qDebug() &lt;&lt; "load module success";&#125;// 加载函数loadData()PyObject *pLoadFunc = PyObject_GetAttrString(pModule, "loadData");if (!pLoadFunc) &#123; printf("get func failed!");&#125;else &#123; qDebug() &lt;&lt; "get func success";&#125;PyObject *retObjectX = PyObject_CallObject(pLoadFunc, NULL); // 获得python脚本返回数据if (retObjectX == NULL) &#123; qDebug() &lt;&lt; "no return value"; return ;&#125;/*将retObjectX导入trainData中(二维数据)*/int row = PyList_Size(retObjectX);for (int i = 0; i &lt; row; ++i) &#123; PyObject *lineObject = PyList_GetItem(retObjectX, i); int col = PyList_Size(lineObject); QVector&lt;double&gt; tmpVect; for (int j = 0; j &lt; col; ++j) &#123; PyObject *singleItem = PyList_GetItem(lineObject, j); double item = PyFloat_AsDouble(singleItem); tmpVect.push_back(item); &#125; trainData-&gt;push_back(tmpVect);&#125;Py_Finalize(); 注意事项这里列写一下软件搭建过程中遇到的问题，以供参考。 重装python的话别忘了修改.pro文件中的python路径； importError:dll not load：常见的matplotlib,numpy等DLL加载错误，通常是由python与对应的第三方包的版本不一致导致的。将anaconda文件下的python.dll和python3.dll文件拷贝到qt可执行文件exe同级目录下并覆盖。 多次调用Py_Initialize()和Py_Finalize()可能会出现异常：最好在main.cpp里就输入Py_Initialize()，程序最后再Py_Finalize()。 Reference:[1] QT与Python混合编程经验记录：http://www.cnblogs.com/jiaping/p/6321859.html[2] C++调用python浅析：https://blog.csdn.net/magictong/article/details/8947892]]></content>
      <categories>
        <category>Qt</category>
      </categories>
      <tags>
        <tag>Qt</tag>
        <tag>python</tag>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Qt+MySQL编程]]></title>
    <url>%2F2018%2F08%2F12%2FQt%2BMySQL%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[项目需要开发一个数据库软件，并且整个软件都是使用Qt搭建的，数据库选用的是MySQL，因此需要使用Qt调用MySQL，在此记录一下相关方法与问题，以方便自己与他人。 本项目使用的是QtCreator(Qt5.5.0)+VisualStudio2013+MySQL5.7.17.0搭建。其他版本只要版本是正确对应的，都大同小异。 准备工作假设你已经正确安装了Qt和MySQL，并且已经将文件../MySQL/MySQL Server 5.7/lib/libmysql.dll复制到文件夹../Qt5.5.0/5.5/msvc2013_64/bin中，将文件../MySQL/MySQL Server 5.7/lib/libmysql.lib复制到文件夹../Qt5.5.0/5.5/msvc2013_64/lib中。之前遇到过MySQL安装过程中卡在MySQL Serve加载处，网络上的各种方法都没用，最后发现可能是使用了XX-Net等网络代理工具的问题，解决方法是关闭网络防火墙。 Qt链接MySQL数据库首先在xxx.pro工程文件中添加：1QT += sql 在相应文件中引入相关头文件：1234#include &lt;QSql&gt;#include &lt;QSqlQueryModel&gt;#include &lt;QSqlDatabase&gt;#include &lt;QSqlQuery&gt; 在mainwindow.h文件的构造函数中添加：12345QString hostName;QString dbName;QString userName;QString password;QSqlDatabase dbconn; 在mainwindow.cpp文件的构造函数中添加：1234567891011121314151617hostName = "localhost"; // 主机名dbName = "wbq"; // 数据库名称userName = "root"; // 用户名password = "helloworld"; // 密码dbconn = QSqlDatabase::addDatabase("QMYSQL");dbconn.setHostName(hostName);dbconn.setDatabaseName(dbName);dbconn.setUserName(userName);dbconn.setPassword(password);qDebug("database open status: %d\n", dbconn.open());QSqlError error = dbconn.lastError();qDebug() &lt;&lt; error.text();dbconn.close(); 如果数据库能够成功打开则调试窗口会出现以下信息：123database open status: 1&quot; &quot; 主界面创建了数据库dbconn后，其他界面若需调用数据库只用定义一个子类成员函数：1234void subwindow::setDbconn(QSqlDatabase *dbconn)&#123; this-&gt;dbconn = dbconn;&#125; 然后在mainwindow.cpp文件对应处调用该函数。 Qt的MySQL数据库操作读取MySQL数据库中某表格的字段名通过以下代码可以将MySQL中某表格中的字段名读取到一个QSqlQueryModel中，读取model中的数据可以获取到字段名。1234QSqlQueryModel *model_name = new QSqlQueryModel;model_name-&gt;setQuery(QString("select COLUMN_NAME from information_schema.COLUMNS where table_name = 'your_table_name' and table_schema = 'your_db_name'"));QModelIndex index_name = model_name-&gt;index(1,0); // model为n行1列qDebug()&lt;&lt;model_name-&gt;data(index_name).toString(); 读取MySQL数据并输入到TableView中MySQL数据库中数据可以看成一个model。假设要读取wbq数据库中的cursheet表，读取方式如下：12345dbconn-&gt;open();QSqlQueryModel *model = new QSqlQueryModel;model-&gt;setQuery(QString("SELECT * FROM %1").arg("wbq.cursheet"));ui-&gt;tableView-&gt;setModel(model);dbconn-&gt;close(); 读取MySQL多个表并根据主键值匹配融合成一个表通常在读入数据后可能需要对多附表进行匹配操作，此时需要读取表中具体元素的数据，方法如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/*定义一个用于存储匹配结果的model*/QStandardItemModel model_merge = new QStandardItemModel(ui-&gt;tableView)/*从wbq数据库中读取两个表进行匹配*/dbconn-&gt;open();QSqlQueryModel *model_1 = new QSqlQueryModel;model_1-&gt;setQuery(QString("SELECT * FROM wbq.sheet1"));QSqlQueryModel *model_2 = new QSqlQueryModel;model_2-&gt;setQuery(QString("SELECT * FROM wbq.sheet2"));/*将model_1中数据输入到model_merge中*/for(int i=0;i&lt;model_1-&gt;rowCount();i++)&#123; for(int j=0;j&lt;model_1-&gt;columnCount();j++)&#123; QModelIndex index = model_1-&gt;index(i,j); model_merge-&gt;setItem(i,j,new QStandardItem(model_1-&gt;data(index).toString())); &#125;&#125;/*将model_2中的数据与model_merge进行匹配(算法未优化，还望各位多包涵)*/QVector&lt;int&gt; nullRow_2; // 用于记录空值for(int i=0;i&lt;model_merge-&gt;rowCount();i++)&#123; QModelIndex index_m = model_merge-&gt;index(i,1); for(int j=0;j&lt;model_2-&gt;rowCount();j++)&#123; QModelIndex index_2 = model_2-&gt;index(j,0); // 若两组数据键值相等，则将model_2数据扩展到model_merge已有列之后 if(model_merge-&gt;data(index_m).toString()==model_2-&gt;data(index_2).toString())&#123; for(int k=1;k&lt;model_2-&gt;columnCount();k++)&#123; QModelIndex index_m2 = model_2-&gt;index(j,k); model_merge-&gt;setItem(i,k+22,new QStandardItem(model_2-&gt;data(index_m2).toString())); &#125; break; &#125; // 如果遍历完依然没有匹配上则存入nullRow_2 if(j==(model_2-&gt;rowCount()-1))&#123; nullRow_2.append(i); &#125; &#125;&#125;// 删除nullRow_2中存放的无法匹配的数据for(int i=nullRow_2.size();i&gt;0;i--)&#123; model_merge-&gt;removeRow(nullRow_2[i-1]);&#125; 增加操作假设要向wbq数据库中的cursheet表增加数据,QVector&lt;QString&gt; validStringVect为数据名称，QVector&lt;QLineEdit*&gt; validLineEditVect为数据值：123456789101112131415161718192021222324252627/*创建语句头*/QString sqlStr = "";sqlStr += QString("INSERT INTO wbq.%1 SET").arg(curSheet);dbconn-&gt;open();/*为每一组数据在sqlstr中扩充相应的语句*/for (int i = 0; i &lt; validStringVect.size(); ++i) &#123; sqlStr += QString(" `%1`='%2'").arg(validStringVect.at(i)).arg(validLineEditVect.at(i)-&gt;text()); if (i &lt; validStringVect.size()-1) &#123; sqlStr += ","; &#125;&#125;// 中文的话需要注意sqlstr的编码问题/*将语句sqlstr发送到MySQL，进行相应的操作*/QSqlQueryModel *model = new QSqlQueryModel;model-&gt;setQuery(sqlStr);model-&gt;setQuery(QString("SELECT * FROM %1").arg("wbq.cursheet")); // 刷新表格dbconn-&gt;close(); 删除操作删除操作通过寻找主键完成对某一组数据的删除工作，具体实现如下所示：12345678910111213141516171819202122232425262728293031/*获取表格中的选中行索引（可为多行）*/std::set&lt;int&gt; rowSet;QModelIndexList indexList = ui-&gt;tableView-&gt;getSelectedIndexs();int indexNum = indexList.size();for (int i = 0; i &lt; indexNum; ++i) &#123; rowSet.insert(indexList.at(i).row());&#125;/*根据主键值遍历删除选中行*/for (iter = rowSet.begin(); iter != rowSet.end(); ++iter) &#123; QModelIndex indexToDel = model-&gt;index((*iter), 0); QString tmpSql; tmpSql = QString(QStringLiteral("DELETE FROM wbq.%1 WHERE index='%2'")).arg(curSheet).arg(indexToDel.data().toString()); dbconn-&gt;open(); QSqlQueryModel *model = new QSqlQueryModel; model-&gt;setQuery(tmpSql); dbconn-&gt;close();&#125;/*刷新表格*/dbconn-&gt;open();QSqlQueryModel *model = new QSqlQueryModel;model-&gt;setQuery(QString("SELECT * FROM %1").arg("wbq.cursheet"));ui-&gt;tableView-&gt;setModel(model);dbconn-&gt;close(); 编辑操作通过对TableView中某item数据修改完成对MySQL对应元素数据的修改，具体方法如下：12345678910111213141516171819QString column_en[8] = &#123;......&#125; // 存储表格中的所有可编辑列列名QModelIndex curIndex = ui-&gt;tableView-&gt;currentIndex(); // 获取当前选中的item的indexQAbstractItemModel *model = ui-&gt;tableView-&gt;model();QModelIndex primaryIndex = model-&gt;index(curIndex.row(), 0); // 获取选中行第一列的index(主键值)QString primaryKey = primaryIndex.data().toString(); // 获取选中行的主键值QString editColumn = pColumn[curIndex.column()]; // 获取选中列列名QString editStr = tmpLineEdit-&gt;text(); // item修改后的值QString updateStr;updateStr = QString(QStringLiteral("UPDATE wbq.%1 SET %2='%3' WHERE 主键值='%4'")).arg(this-&gt;curSheet).arg(editColumn).arg(editStr).arg(primaryKey);dbconn-&gt;open();QSqlQueryModel *model = new QSqlQueryModel;model-&gt;setQuery(updateStr);model-&gt;setQuery(QString("SELECT * FROM %1").arg("wbq.cursheet")); // 刷新表格ui-&gt;tableView-&gt;setModel(model);dbconn-&gt;close(); 查询操作软件中输入某列或某几列数据的上下限，完成查询功能。具体方法如下：123456789101112131415161718192021222324252627QString sqlStr = "";sqlStr += QString("SELECT * FROM wbq.%1 WHERE ").arg(curSheet);// 遍历各列数据的上下限bool first = true;for (int i = 0; i &lt; searchField.size(); ++i) &#123; if (!this-&gt;startLineEdit.at(i)-&gt;text().isEmpty()) &#123; if (!first) sqlStr += QString(" AND "); sqlStr += QString(" `%1` &gt; %2 ").arg(searchField.at(i)).arg(startLineEdit.at(i)-&gt;text()); first = false; &#125; if (!this-&gt;endLineEdit.at(i)-&gt;text().isEmpty()) &#123; if (!first) sqlStr += QString(" AND "); sqlStr += QString(" `%1` &lt; %2 ").arg(searchField.at(i)).arg(endLineEdit.at(i)-&gt;text()); first = false; &#125;&#125;if (first) // 防止出现所有列上下限都没有设置的情况 sqlStr += QString("1=1"); dbconn-&gt;open();QSqlQueryModel *model = new QSqlQueryModel;model-&gt;setQuery(sqlStr);ui-&gt;tableView-&gt;setModel(model);dbconn-&gt;close(); 统计计算项目要求软件实现对数据统计特征的计算功能，此处也是直接调用MySQL语句进行操作。具体方法如下：1234567891011121314151617181920212223242526272829303132333435363738394041dbconn-&gt;open();QVector&lt;QString&gt; calStr = &#123;"AVG", "STD", "MAX", "MIN"&#125;; // 四种统计属性QSqlQueryModel *queryModel = new QSqlQueryModel; // 获取各列统计信息的modelQVector&lt;QVector&lt;QString&gt;&gt; gVect; // 存储各列统计信息QVector&lt;QString&gt; fieldStr; // 需要统计的列名(假设已赋值)/*获取queryModel*/for (int i = 0; i &lt; calStr.size(); ++i) &#123; QString queryStr = QString("SELECT "); for (int j = 0; j &lt; fieldStr.size(); ++j) &#123; queryStr.append(QString("%1(CAST(`%2` AS DECIMAL(8, 2)))").arg(calStr[i]).arg(fieldStr[j])); // 8代表数值长度，2代表小数位数 if (j &lt; fieldStr.size()-1) queryStr.append(QString(",")); &#125; queryStr.append(QString(" FROM wbq.%1").arg(this-&gt;curSheet)); queryModel-&gt;setQuery(queryStr); /* queryModel读取到gVect中 */ int colCount = queryModel-&gt;columnCount(); QVector&lt;QString&gt; rowVect; for (int j = 0; j &lt; colCount; ++j) &#123; rowVect.push_back(QString("%1").arg(queryModel-&gt;index(0, j).data().toFloat())); &#125; gVect.push_back(rowVect);&#125;dbconn-&gt;close();/*显示各列统计信息*/QAbstractTableModel *model = new QAbstractTableModel；model-&gt;setDataVect(gVect);QVector&lt;QString&gt; calStr_cn = &#123;QStringLiteral("平均数"), QStringLiteral("方差"), QStringLiteral("最大值"), QStringLiteral("最小值")&#125;;model-&gt;setHeaderVect(fieldStr, calStr_cn);ui-&gt;tableView-&gt;setModel(model)； // 刷新表格 Qt+MySQL发布此步同样适用于一般的Qt软件发布。1). 单独将.exe执行程序放到一个空文件夹；2). 在开始菜单打开Qt5.5 64-bit for Desktop (msvc 2013)并cd命令到此文件夹；3). 运行命令windeployqt 文件名.exe，将生成的所有文件复制到release文件夹中；4). 将libmysql.dll复制到release文件夹中。 使用该软件的主机需要安装对应版本的MySQL，并将已存在的数据库导入。 注意事项这里列写一下软件搭建过程中遇到的问题，以供参考。 重装Qt后数据库无法连上：将release文件夹中Qt的.dll用新装的Qt替代，哪怕Qt版本一致。 出现错误cant connect mysql server on 127.0.0.1(10060)，mysql workbench无法连接，但MySQL Serve显示正在运行：关闭网络防火墙。 QT移植无法启动This application failed to start because it could not find or load the Qt platform plugin：1). 将../Qt5.5.0/5.5/msvc2013_64/bin中的所有.dll复制到.exe目录下，尽管右下可能用不到；2). 将文件夹../Qt5.5.0/5.5/msvc2013_64/plugins/platforms直接复制到.exe目录下。 在Qt5.5 64-bit for Desktop (msvc 2013)中输入windeployqt 文件名.exe时报错Warning: Cannot find Visual Studio installation directory, VCINSTALLDIR is not set.：直接用 “VS2013 开发人员命令提示” 命令行去执行刚才的windeployqt 文件名.exe，会将 “vcredist_x64.exe”（vc x64 运行最少环境）程序放入当前目录。 Reference:[1] 在QT中使用MySQL数据库：https://blog.csdn.net/yunzhifeiti/article/details/72709140[2] QT移植无法启动 This application failed to start because it could not find or load the Qt platform plugin：https://blog.csdn.net/jzwong/article/details/71479691]]></content>
      <categories>
        <category>Qt</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>C++</tag>
        <tag>Qt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+GitHub搭建私人博客]]></title>
    <url>%2F2018%2F08%2F11%2FHexo%2BGitHub%E6%90%AD%E5%BB%BA%E7%A7%81%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[使用GitHub作为服务器搭建自己的私人博客相对来说成本更低且更容易实现，缺点是国内的搜索引擎无法检索到你的网页信息。GithUb提供了一个Github Pages的服务，可以为托管在Github上的项目提供静态页面。从零开始，博客具体搭建步骤如下： 安装Git这一步可以参考廖雪峰的Git教程：https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00137396287703354d8c6c01c904c7d9ff056ae23da865a000一路默认安装即可。安装完成后，需要进一步进行设置，在命令行输入：12$ git config --global user.name &quot;Your Name&quot;$ git config --global user.email &quot;email@example.com&quot; 其中第一句的“名称”和第二句的“邮箱”替换成你自己的名字与E-mail地址，此步相当于注册你这台机器的信息。为了实现在Github中对Git仓库进行托管服务，我们需要先注册一个Github账号，然后创建SSH Key。打开Shell(Windows下打开Git Bash)，输入：1$ ssh-keygen -t rsa -C &quot;youremail@example.com&quot; 同样将邮箱地址换成你自己的邮箱地址。一路确认，会在用户主目录下创建一个.ssh的文件夹，里面有id_rsa和id_rsa.pub这两个文件，这两个就是SSH Key的秘钥对，id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。登录Github，打开“Account settings”，“SSH Keys”页面，然后，点“Add SSH Key”，填上任意Title，在Key文本框里粘贴id_rsa.pub文件的内容：点“Add Key”，你就应该看到已经添加的Key： 安装Node.js与Hexo点击链接下载Node.js安装包：https://nodejs.org/dist/v8.11.3/node-v8.11.3-x64.msi一路默认安装即可，完成后打开控制台，如果安装成功会显示如下信息： 安装Hexo。首先在本地创建一个用于写blog的文件夹，并使用cd命令进入该文件夹目录。在命令行输入$ npm install hexo -g安装Hexo。输入hexo -v检查Hexo是否安装成功：输入hexo init初始化该文件夹(漫长的等待…)，看到最后出现”Start blogging with Hexo!”即表示安装成功。接着输入npm install以安装所需的组件。 连接Hexo与Github(仅限于第一次搭建博客)在当前目录下找到并打开文件_config.yml，修改repository值(在末尾)：1234deploy: type: git repository: git@github.com:KunBB/KunBB.github.io.git branch: master repository值是你在Github项目里的ssh，如下图所示：注意blog中的所有配置文件名称与值之间都要有空格，否则不会生效。如type:git错误，type: git正确。 Hexo中使用Latex编写公式安装Kramed由于很多博客中会涉及到一些公式，而markdown本身的公式编写较为麻烦，作为一名科研工作者，Latex格式一定是相当熟悉的东西，因此我们需要通过安装第三方库来配置Hexo使用Latex格式书写公式。 Hexo默认的渲染引擎是marked，但是marked不支持mathjax，所以需要更换Hexo的markdown渲染引擎为hexo-renderer-kramed引擎，后者支持mathjax公式输出。12$ npm uninstall hexo-renderer-marked --save$ npm install hexo-renderer-kramed --save 更改文件配置打开文件”../node_modules/hexo-renderer-kramed/lib/renderer.js”进行修改(末尾)：123456789101112// Change inline math rulefunction formatText(text) &#123; // Fit kramed&apos;s rule: $$ + \1 + $$ return text.replace(/`\$(.*?)\$`/g, &apos;$$$$$1$$$$&apos;);&#125;修改为：// Change inline math rulefunction formatText(text) &#123; return text;&#125; 停止使用hexo-math并安装mathjax包卸载hexo-math：1$ npm uninstall hexo-math --save 安装hexo-renderer-mathjax包:1$ npm install hexo-renderer-mathjax --save 更新Mathjax配置文件打开文件”../node_modules/hexo-renderer-mathjax/mathjax.html”，将的src修改为： "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" 即： 更改默认转义规则因为LaTeX与markdown语法有语义冲突，所以Hexo默认的转义规则会将一些字符进行转义，所以我们需要对默认的规则进行修改。打开文件”../node_modules/kramed/lib/rules/inline.js”：1.将1escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, 更改为1escape: /^\\([`*\[\]()# +\-.!_&gt;])/, 2.将1em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 更改为1em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 开启mathjax打开文件”../Hexo/themes/next/_config.yml”，找到含有”mathjax”的字段进行如下修改：12345# MathJax Supportmathjax: enable: true per_page: true cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML 写博客的时候需要在开头开启mathjax选项，添加以下内容：123456title: LibSVM支持向量回归详解date: 2018-01-30 10:10:00categories: &quot;SVM&quot;tags: -Machine learningmathjax: true 上传博客在Shell(Windows下打开Git Bash)中输入1$ hexo s 可在本地查看博客效果，默认端口号为4000，地址：http://localhost:4000/注意复制时不要用ctrl+c(会终止进程)。若页面一直无法跳转，可能是端口被占用，此时可以输入hexo server -p 端口号来改变端口号。在Shell(Windows下打开Git Bash)中输入1$ hexo d -g 可将本地博客推送到Github服务器，完成私人博客更新。 其他博客图片管理博客中所插入的图片需要图片链接，由于博客托管于Github服务器，所以如果博客中图片链接为国内网页的链接则可能存在图片加载缓慢甚至无法加载的现象。为解决这个问题我们可以在Github上新建一个Repo用于存放博客中所需要的图片，将图片的Github链接写入博客。 博客文件管理当电脑重装系统或需要跟换电脑时，hexo+git往往需要重新配置，博客文件也需要复制到当前主机。因此比较方便的操作就是在Github上新建一个Repo用于存放blog目录的所有文件，当博客更新时，将最近版push到github。 Reference:[1] 使用Hexo+Github一步步搭建属于自己的博客（基础）：https://www.cnblogs.com/fengxiongZz/p/7707219.html[2] hexo+github搭建个人博客(超详细教程)：https://blog.csdn.net/ainuser/article/details/77609180[3] 如何在 hexo 中支持 Mathjax？：https://blog.csdn.net/u014630987/article/details/78670258[4] 使用LaTex添加公式到Hexo博客里：https://blog.csdn.net/Aoman_Hao/article/details/81381507]]></content>
      <categories>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>Hexo</tag>
        <tag>GitHub</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LibSVM支持向量回归详解]]></title>
    <url>%2F2018%2F01%2F30%2Flibsvm%20SVR%E9%83%A8%E5%88%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[LibSVM是是台湾林智仁(Chih-Jen Lin)教授2001年开发的一套支持向量机的库，可以很方便的对数据做分类或回归。由于LibSVM程序小，运用灵活，输入参数少，并且是开源的，易于扩展，因此成为目前国内应用最多的SVM的库，同时sklearn.svm也是使用的该库。 网络上对于LibSVM源码的讲解有很多，但个人感觉绝大多数讲解的不够清晰，很多都是贴个理论公式图片再粘段代码就一带而过。并且网络上基本都是对SVC的讲解，SVR部分几乎没有提及（虽然SVR只是SVC的扩展）。因此本篇博文将系统地讲解LibSVM中SVR训练与预测部分的源码（想学习SVC的同学同样适用）。 python复现LIBSVM中SVR部分功能可参见 https://github.com/KunBB/LibSVM_SVR_python LibSVM整体流程train：1234567891011121314151617181920//根据svm_type的不同进行初始化svm_train() //根据svm_type的不同调用不同的分类回归训练函数 svm_train_one() //针对epsilon-SVR这一类型进行模型参数初始化 solve_epsilon_svr() //使用SMO算法求解对偶问题（二次优化问题） Solver::Solve() //每隔若干次迭代进行一次shrinking，对样本集进行缩减降低计算成本 Solver::do_shrinking() //若满足停止条件则进行梯度重建并跳出循环 Solver::reconstruct_gradient() //选择出当前最大违反对i,j Solver::select_working_set() //计算参数优化后的rho Solver::caculate_rho() //得到优化后的alpha和SolutionInfo对象si //得到优化后的alpha和SolutionInfo对象si //得到decision_function对象f//得到svm_model对象model predict123456789//根据svm_type的不同开辟dec_value空间svm_predict() //根据svm_type的不同调用k_function函数 svm_predict_values() //根据kernel_type的不同计算k(i,j) Kernel::k_function() //得到k(x_train[i],x_test[j]) //得到预测值y_pre[j]//得到预测值y_pre[j] svm.h文件解析svm_node12345//存储一个样本（假设为样本i）的一个特征struct svm_node&#123; int index; //样本i的特征索引值，最后一个为-1 double value; //样本i第index个特征的值，最后一个为NULL&#125;; 如：x[i]={0.23,1.2,3.5,1.5}则需使用五个svm_node来表示x[i]，具体映射如下： index 0 1 2 3 -1 value 0.23 1.2 3.5 1.5 NULL svm_problem123456//存储参加运算的所有样本数据（训练集）struct svm_problem&#123; int l; //样本总数 double *y; //样本输出值（所属类别） struct svm_node **x; //样本输入值&#125;; 下图中最右边的长条格同上表，存储了三维数据。 svm_problem中的y与类Solver中的y并不完全一样!!!对于一般SVC而言可以看出一样的，其值为-1与+1，对于多分类而言svm_problem.y[i]可以是1、2、3等，而多类计算其实是二分类的组合，因此在二分类中y[i]依然等于+1与-1.更特殊的，在SVR中，svm_problem的y[i]等于其目标值，如：11.234、56.24、5.23等，在计算时svm_problem.y[i]整合到了Solver.p[i]与Solver.p[i+svm_problem.l]中（具体的问题后续章节再详细解释），而在Solver.y[i]依然为+1和-1. svm_parameter12345678910111213141516171819202122232425//svm_type和svm_type可能取值enum &#123; C\_SVC, NU\_SVC, ONE\_CLASS, EPSILON\_SVR, NU\_SVR &#125;;/* svm_type */enum &#123; LINEAR, POLY, RBF, SIGMOID &#125;; /* kernel_type *///svm模型训练参数struct svm_parameter &#123; int svm_type; int kernel_type; int degree; /* for poly */ double gamma; /* for poly/rbf/sigmoid */ double coef0; /* for poly/sigmoid */ /* these are for training only */ double cache_size; /* in MB */ double eps; /* stopping criteria */ double C; /* for C_SVC, EPSILON_SVR and NU_SVR */ int nr_weight; /* for C_SVC */ int *weight_label; /* for C_SVC */ double* weight; /* for C_SVC */ double nu; /* for NU_SVC, ONE_CLASS, and NU_SVR */ double p; /* for EPSILON_SVR */ int shrinking; /* use the shrinking heuristics */ int probability; /* do probability estimates */ &#125;; LibSVM中的核函数如下： 各参数解释如下： Parameter Interpretation degree 2式中的d gamma 2,3,4式中的gamma coef0 2,4式中的r cache_size 单位MB，训练所需内存，LibSVM2.5默认4M eps 停止条件需满足的最大误差值(文献[2]中式3.9) C 惩罚因子，越大模型过拟合越严重 nr_weight 权重的数目,目前在实例代码中只有两个值，一个是默认0，另外一个是svm_binary_svc_probability函数中使用数值2 *weight_label 权重，元素个数由nr_weight决定. nu NU_SVC,ONE_CLASS,NU_SVR中的nu p SVR中的间隔带epsilon shrinking 指明训练过程是否使用压缩 probability 指明是否做概率估计 svm_model123456789101112131415161718192021//保存训练后的模型参数struct svm_model&#123; struct svm_parameter param; /* parameter */ int nr_class; /* number of classes, = 2 in regression/one class svm */ int l; /* total #SV */ struct svm_node **SV; /* SVs (SV[l]) */ double **sv_coef; /* coefficients for SVs in decision functions (sv_coef[k-1][l]) */ double *rho; /* constants in decision functions (rho[k*(k-1)/2]) */ double *probA; /* pariwise probability information */ double *probB; int *sv_indices; /* sv_indices[0,...,nSV-1] are values in [1,...,num_traning_data] to indicate SVs in the training set */ /* for classification only */ int *label; /* label of each class (label[k]) */ int *nSV; /* number of SVs for each class (nSV[k]) */ /* nSV[0] + nSV[1] + ... + nSV[k-1] = l */ /* XXX */ int free_sv; /* 1 if svm_model is created by svm_load_model*/ /* 0 if svm_model is created by svm_train */&#125;; 各参数解释如下： Parameter Interpretation param 训练参数 nr_class 类别数 l 支持向量数 **SV 作为支持向量的样本集 **sv_coef 支持向量系数alpha *rho 判别函数中的b *proA 成对概率信息 *proB 成对概率信息 *sv_indices 记录支持向量在训练数据中的index *label 各类的标签 *nSV 各类的支持向量数 free_SV 若model由svm_load_model函数生成则为1，若为svm_train生成则为0 svm.cpp文件解析下图为svm.cpp中的类继承和组合情况（实现表示继承关系，虚线表示组合关系）：Cache类主要负责运算所涉及的内存的管理，包括申请、释放等。本篇博文主要讲解SVM求解过程，对于Cache类将不予解析。 Kernel类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Kernel : public QMatrix &#123;public: Kernel(int l, svm_node * const * x, const svm_parameter&amp; param); virtual ~Kernel(); static double k_function(const svm_node *x, const svm_node *y, const svm_parameter&amp; param); virtual Qfloat *get_Q(int column, int len) const = 0; virtual double *get_QD() const = 0; virtual void swap_index(int i, int j) const // no so const... &#123; swap(x[i], x[j]); if (x_square) swap(x_square[i], x_square[j]); &#125;protected: double (Kernel::*kernel_function)(int i, int j) const;private: const svm_node **x; double *x_square; // svm_parameter const int kernel_type; const int degree; const double gamma; const double coef0; static double dot(const svm_node *px, const svm_node *py); double kernel_linear(int i, int j) const &#123; return dot(x[i], x[j]); &#125; double kernel_poly(int i, int j) const &#123; return powi(gamma*dot(x[i], x[j]) + coef0, degree); &#125; double kernel_rbf(int i, int j) const &#123; return exp(-gamma * (x_square[i] + x_square[j] - 2 * dot(x[i], x[j]))); &#125; double kernel_sigmoid(int i, int j) const &#123; return tanh(gamma*dot(x[i], x[j]) + coef0); &#125; double kernel_precomputed(int i, int j) const &#123; return x[i][(int)(x[j][0].value)].value; &#125;&#125;; 成员变量 Parameter Interpretation svm_node **x 训练样本数据 *x_square x[i]^T*x[i]，使用RBF核会用到 kernel_type 核函数类型 degree svm_parameter gamma svm_parameter coef0 svm_parameter 成员函数Kernel(int l, svm_node * const * x, const svm_parameter&amp; param);构造函数。初始化类中的部分常量、指定核函数、克隆样本数据。 12345678910111213141516171819202122232425262728293031323334Kernel::Kernel(int l, svm_node * const * x_, const svm_parameter&amp; param) :kernel_type(param.kernel_type), degree(param.degree), gamma(param.gamma), coef0(param.coef0)&#123; switch (kernel_type) //根据kernel_type的不同定义相应的函数kernel_function() &#123; case LINEAR: kernel_function = &amp;Kernel::kernel_linear; break; case POLY: kernel_function = &amp;Kernel::kernel_poly; break; case RBF: kernel_function = &amp;Kernel::kernel_rbf; break; case SIGMOID: kernel_function = &amp;Kernel::kernel_sigmoid; break; case PRECOMPUTED: kernel_function = &amp;Kernel::kernel_precomputed; break; &#125; clone(x, x_, l); if (kernel_type == RBF) //如果使用RBF 核函数，则计算x_sqare[i]，即x[i]^T*x[i] &#123; x_square = new double[l]; for (int i = 0; i&lt;l; i++) x_square[i] = dot(x[i], x[i]); &#125; else x_square = 0;&#125; static double dot(const svm_node *px, const svm_node *py);点乘函数，点乘两个样本数据，按svm_node 中index (一般为特征)进行运算，一般来说，index为1，2，…直到-1。返回点乘总和。例如：x1={1,2,3} ,x2={4,5,6}总和为sum=1*4+2*5+3*6;在svm_node[3]中存储index=-1时，停止计算。 123456789101112131415161718192021double Kernel::dot(const svm_node *px, const svm_node *py)&#123; double sum = 0; while (px-&gt;index != -1 &amp;&amp; py-&gt;index != -1) &#123; if (px-&gt;index == py-&gt;index) &#123; sum += px-&gt;value * py-&gt;value; ++px; ++py; &#125; else &#123; if (px-&gt;index &gt; py-&gt;index) ++py; else ++px; &#125; &#125; return sum;&#125; static double k_function(const svm_node *x, const svm_node *y, const svm_parameter&amp; param);功能类似kernel_function,不过kerel_function用于训练，k_function用于预测。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758double Kernel::k_function(const svm_node *x, const svm_node *y, const svm_parameter&amp; param) //输入数据为两个数据样本，其中一个为训练样本一个为测试样本&#123; switch (param.kernel_type) &#123; case LINEAR: return dot(x, y); case POLY: return powi(param.gamma*dot(x, y) + param.coef0, param.degree); case RBF: &#123; double sum = 0; while (x-&gt;index != -1 &amp;&amp; y-&gt;index != -1) &#123; if (x-&gt;index == y-&gt;index) &#123; double d = x-&gt;value - y-&gt;value; sum += d * d; ++x; ++y; &#125; else &#123; if (x-&gt;index &gt; y-&gt;index) &#123; sum += y-&gt;value * y-&gt;value; ++y; &#125; else &#123; sum += x-&gt;value * x-&gt;value; ++x; &#125; &#125; &#125; while (x-&gt;index != -1) &#123; sum += x-&gt;value * x-&gt;value; ++x; &#125; while (y-&gt;index != -1) &#123; sum += y-&gt;value * y-&gt;value; ++y; &#125; return exp(-param.gamma*sum); &#125; case SIGMOID: return tanh(param.gamma*dot(x, y) + param.coef0); case PRECOMPUTED: //x: test (validation), y: SV return x[(int)(y-&gt;value)].value; default: return 0; // Unreachable &#125;&#125; 其中RBF部分很有讲究。因为存储时，0值不保留。如果所有0值都保留，第一个while就可以都做完了；如果第一个while做不完，在x，y中任意一个出现index＝-1，第一个while就停止，剩下的代码中两个while只会有一个工作，该循环直接把剩下的计算做完。 virtual Qfloat *get_Q(int column, int len);纯虚函数，将来在子类中实现(如class SVR_Q)，计算Q值。相当重要的函数。 1virtual Qfloat *get_Q(int column, int len) const = 0; virtual void swap_index(int i, int j);虚函数，x[i]和x[j]中所存储指针的内容。如果x_square不为空，则交换相应的内容。 12345virtual void swap_index(int i, int j) const // no so const... &#123; swap(x[i], x[j]); if (x_square) swap(x_square[i], x_square[j]); &#125; virtual double *get_QD();纯虚函数，将来在子类中实现(如class SVR_Q),计算Q[i,i]值。 1virtual Qfloat *get_Q(int column, int len) const = 0; double (Kernel::*kernel_function)(int i, int j)；函数指针，根据相应的核函数类型，来决定所使用的函数。在计算矩阵Q时使用。 1double (Kernel::*kernel_function)(int i, int j) const; Solver类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Solver &#123;public: Solver() &#123;&#125;; virtual ~Solver() &#123;&#125;; struct SolutionInfo &#123; double obj; double rho; double upper_bound_p; double upper_bound_n; double r; // for Solver_NU &#125;; void Solve(int l, const QMatrix&amp; Q, const double *p_, const schar *y_, double *alpha_, double Cp, double Cn, double eps, SolutionInfo* si, int shrinking);protected: int active_size; schar *y; double *G; // gradient of objective function enum &#123; LOWER_BOUND, UPPER_BOUND, FREE &#125;; char *alpha_status; // LOWER_BOUND, UPPER_BOUND, FREE double *alpha; const QMatrix *Q; const double *QD; double eps; double Cp, Cn; double *p; int *active_set; double *G_bar; // gradient, if we treat free variables as 0 int l; bool unshrink; // XXX double get_C(int i) &#123; return (y[i] &gt; 0) ? Cp : Cn; &#125; void update_alpha_status(int i) &#123; if (alpha[i] &gt;= get_C(i)) alpha_status[i] = UPPER_BOUND; else if (alpha[i] &lt;= 0) alpha_status[i] = LOWER_BOUND; else alpha_status[i] = FREE; &#125; bool is_upper_bound(int i) &#123; return alpha_status[i] == UPPER_BOUND; &#125; bool is_lower_bound(int i) &#123; return alpha_status[i] == LOWER_BOUND; &#125; bool is_free(int i) &#123; return alpha_status[i] == FREE; &#125; void swap_index(int i, int j); void reconstruct_gradient(); virtual int select_working_set(int &amp;i, int &amp;j); virtual double calculate_rho(); virtual void do_shrinking();private: bool be_shrunk(int i, double Gmax1, double Gmax2);&#125;; 成员变量结构体SolutionInfo为求解优化中的参数信息。 各参数解释如下： Parameter Interpretation SolutionInfo.obj 求解优化过程中的目标函数值 SolutionInfo.rho 判别函数中的b SolutionInfo.upper_bound_p 对于不平衡数据集，该值对应惩罚因子Cp SolutionInfo.upper_bound_n 对于不平衡数据集，该值对应惩罚因子Cn SolutionInfo.r 用于Solver_NU active_size 计算时实际参加运算的样本数目，经过shrink处理后，该数目会小于全部样本总数。 *y 样本所属类别，该值只取+1/-1 。虽然可以处理多类，最终是用两类SVM 完成的。 *G 梯度G=Qα+P *alpha_status α[i]的状态，根据情况分为α[i]≤0,α[i]≥c和0&lt;α[i]&lt;\c,分别对应内部点(非SV)，错分点(BSV)和支持向量(SV)。 *alpha α[i] *Q 对应公式中Q的某一列 *QD 对应公式中的Q[i][i] eps 停止条件的误差限 Cp，Cn 对应不平衡数据的惩罚因子，若不为不平数据或是对于SVR来说Cp=Cn=C *p 对应梯度公式中的p，即SVR中的间隔带epsilon *active_set active对应的index *G_bar sum(C*Q) l 数据样本个数 unshrink 是否被压缩 成员函数double get_C(int i)；返回对应于样本的C。设置不同的Cp 和Cn 是为了处理数据的不平衡。见[1]中的Unbalanced data.对于一般样本数据Cp=Cn。 1234double get_C(int i) &#123; return (y[i] &gt; 0) ? Cp : Cn; &#125; void swap_index(int i, int j);完全交换样本i和样本j的内容，包括所申请的内存的地址。 12345678910111213void Solver::swap_index(int i, int j)&#123; Q-&gt;swap_index(i, j); swap(y[i], y[j]); swap(G[i], G[j]); swap(alpha_status[i], alpha_status[j]); swap(alpha[i], alpha[j]); swap(p[i], p[j]); swap(active_set[i], active_set[j]); swap(G_bar[i], G_bar[j]);&#125;template &lt;class T&gt; static inline void swap(T&amp; x, T&amp; y) &#123; T t = x; x = y; y = t; &#125; void reconstruct_gradient();重新计算梯度。 1234567891011121314151617181920212223242526272829303132333435363738394041void Solver::reconstruct_gradient()&#123; // reconstruct inactive elements of G from G_bar and free variables if (active_size == l) return; int i, j; int nr_free = 0; for (j = active_size; j&lt;l; j++) G[j] = G_bar[j] + p[j]; for (j = 0; j&lt;active_size; j++) if (is_free(j)) nr_free++; if (2 * nr_free &lt; active_size) info("\nWARNING: using -h 0 may be faster\n"); if (nr_free*l &gt; 2 * active_size*(l - active_size)) &#123; for (i = active_size; i&lt;l; i++) &#123; const Qfloat *Q_i = Q-&gt;get_Q(i, active_size); for (j = 0; j&lt;active_size; j++) if (is_free(j)) G[i] += alpha[j] * Q_i[j]; &#125; &#125; else &#123; for (i = 0; i&lt;active_size; i++) if (is_free(i)) &#123; const Qfloat *Q_i = Q-&gt;get_Q(i, l); double alpha_i = alpha[i]; for (j = active_size; j&lt;l; j++) G[j] += alpha_i * Q_i[j]; &#125; &#125;&#125; G_bar[i]在初始化时并未加入p[i]，所以程序首先增加p[i]。Shrink后依然参加运算的样本位于active_size和l-1位置上。在0～active_size之间的alpha[i]如果在区间(0,c)上，才有必要更新相应的active_size和l-1位置上的样本的梯度。 virtual void do_shrinking();对样本集做缩减。当0&lt;α&lt;C时(还有两种情况),程序认为该样本可以不参加下次迭代。(0&lt;α&lt;C时，为内部点)程序会减小active_size，为内部点增加位置。active_size表明了不可以参加下次迭代的样本的最小标签号，在active_size与l之间的元素都对分类没有贡献。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960void Solver::do_shrinking()&#123; int i; double Gmax1 = -INF; // max &#123; -y_i * grad(f)_i | i in I_up(\alpha) &#125; double Gmax2 = -INF; // max &#123; y_i * grad(f)_i | i in I_low(\alpha) &#125; // find maximal violating pair first for (i = 0; i&lt;active_size; i++) &#123; if (y[i] == +1) &#123; if (!is_upper_bound(i)) // &lt; C &#123; if (-G[i] &gt;= Gmax1) Gmax1 = -G[i]; &#125; if (!is_lower_bound(i)) &#123; if (G[i] &gt;= Gmax2) Gmax2 = G[i]; &#125; &#125; else &#123; if (!is_upper_bound(i)) &#123; if (-G[i] &gt;= Gmax2) Gmax2 = -G[i]; &#125; if (!is_lower_bound(i)) &#123; if (G[i] &gt;= Gmax1) Gmax1 = G[i]; &#125; &#125; &#125; //如果程序在缩减一次后没有达到结束条件，就重新构造梯度矢量，并再缩减一次。 if (unshrink == false &amp;&amp; Gmax1 + Gmax2 &lt;= eps * 10) &#123; unshrink = true; reconstruct_gradient(); active_size = l; info("*"); &#125; //程序中active_size--是为了消除交换后的影响，使重新换来的样本也被检查一次。 for (i = 0; i&lt;active_size; i++) if (be_shrunk(i, Gmax1, Gmax2)) &#123; active_size--; while (active_size &gt; i) &#123; if (!be_shrunk(active_size, Gmax1, Gmax2)) &#123; swap_index(i, active_size); break; &#125; active_size--; &#125; &#125;&#125; virtual int select_working_set(int &amp;i, int &amp;j);该函数求解出违反KKT条件最严重的目标对i与j。我们先来了解一下working set的选择原理。参考文献[3]。 选择iSVM的对偶问题为： SVM收敛的充分必要条件(KKT条件)为： 对(1)式求导可以得到：①yi=1，αi&lt;C，由(2)和(3)可得： ②yi=-1，αi&gt;0，由(2)和(3)可得： ③yi=-1，αi&lt;C，由(2)和(3)可得： ④yi=1，αi&gt;0，由(2)和(3)可得： 对式(4)、(5)、(6)、(7)进行约简得到式(8): 可以发现，(4)和(5)都是b大于某个数，(6)和(7)都是b小于某个数。因为b是个常量，那么根据上述条件，我们可以得到以下结论，在合理的αi和αj下，有： 我们就是要从中挑选违反上述条件的αi和αj，来进行重新的迭代和更新，使得所有的αi和αj都满足上述条件。那么我们可以很容易得到违反条件为： 则根据式(8)中关于i的选择就可以明白select_working_set函数中关于选择i的部分了。 选择j当yiyjK(i,j)为半正定矩阵时，当且仅当待优化乘子为“违反对”时，目标函数是严格递减的。LibSVM在做选择的时候，采用的是second order information方法。那么我们挑选出了i之后，剩下的任务就是挑选出既是“违反对”同时使目标函数值最小。补充一下：挑选了“违反对”，自然就使得目标函数自然递减了，那么我们挑选目标函数最小，自然使得迭代速度加快。这是我们希望看到的结果。 使用泰勒展开式： 则优化问题变为： 由约束条件可知： 因为0≤α≤c,所以当α取到极值的时候，d的取值是有限制的，使得最终的α+d的值不会超出α取值范围。 则原优化问题可转换为下述优化问题： 最小值为： 证明如下(可参考文献[3]的Theorem 3)： 结论貌似挺复杂的，其实不然，仔细观察发现式(13)其实就是一个一元二次函数，对其求极值，得该函数的最小值。 下面我们来看一下代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100int Solver::select_working_set(int &amp;out_i, int &amp;out_j)&#123; // return i,j such that // i: maximizes -y_i * grad(f)_i, i in I_up(\alpha) // j: minimizes the decrease of obj value // (if quadratic coefficeint &lt;= 0, replace it with tau) // -y_j*grad(f)_j &lt; -y_i*grad(f)_i, j in I_low(\alpha) double Gmax = -INF; //-yi*G(alphai) double Gmax2 = -INF; //yj*G(alphaj) int Gmax_idx = -1; int Gmin_idx = -1; double obj_diff_min = INF; //寻找working set B中的i for (int t = 0; t&lt;active_size; t++) if (y[t] == +1) &#123; if (!is_upper_bound(t)) //对应于yi=1,alphai&lt;c if (-G[t] &gt;= Gmax) &#123; Gmax = -G[t]; //寻找最大的-yi*G(alphai)，以使违反条件最严重 Gmax_idx = t; &#125; &#125; else &#123; if (!is_lower_bound(t)) //对应于yi=1,alphai&gt;0 if (G[t] &gt;= Gmax) &#123; Gmax = G[t]; Gmax_idx = t; &#125; &#125; int i = Gmax_idx; //得到i const Qfloat *Q_i = NULL; if (i != -1) // NULL Q_i not accessed: Gmax=-INF if i=-1 Q_i = Q-&gt;get_Q(i, active_size); //寻找working set B中的j for (int j = 0; j&lt;active_size; j++) &#123; if (y[j] == +1) &#123; if (!is_lower_bound(j)) &#123; double grad_diff = Gmax + G[j]; //分子(-yi*Gi+yj*Gj) if (G[j] &gt;= Gmax2) //寻找最小的-yj*G(alphaj) Gmax2 = G[j]; if (grad_diff &gt; 0) //保证不满足KKT条件 &#123; double obj_diff; double quad_coef = QD[i] + QD[j] - 2.0*y[i] * Q_i[j]; //分母(Kii+Kjj-2*Kij),注意Kij和Qij的关系(SVR_Q类中会讲) if (quad_coef &gt; 0) obj_diff = -(grad_diff*grad_diff) / quad_coef; else obj_diff = -(grad_diff*grad_diff) / TAU; //当quad_coef小于0时令其等于一个很小很小的值。 if (obj_diff &lt;= obj_diff_min) &#123; Gmin_idx = j; obj_diff_min = obj_diff; &#125; &#125; &#125; &#125; else &#123; if (!is_upper_bound(j)) &#123; double grad_diff = Gmax - G[j]; if (-G[j] &gt;= Gmax2) Gmax2 = -G[j]; if (grad_diff &gt; 0) &#123; double obj_diff; double quad_coef = QD[i] + QD[j] + 2.0*y[i] * Q_i[j]; if (quad_coef &gt; 0) obj_diff = -(grad_diff*grad_diff) / quad_coef; else obj_diff = -(grad_diff*grad_diff) / TAU; if (obj_diff &lt;= obj_diff_min) &#123; Gmin_idx = j; obj_diff_min = obj_diff; &#125; &#125; &#125; &#125; &#125; if (Gmax + Gmax2 &lt; eps || Gmin_idx == -1) //达到停止条件或再没有需要优化的alpha，表示已经完全优化 return 1; out_i = Gmax_idx; out_j = Gmin_idx; return 0;&#125; void Solve(int l, const QMatrix&amp; Q, const double *p_, const schar *y_, double *alpha_, double Cp, double Cn, double eps, SolutionInfo* si, int shrinking);Solve函数用于求解更新alpha，下面讲解一下其求解原理，主要是SMO算法原理。这里主要还是以C-SVC为例，在后面讲解SVR_Q类时会解释如何将其扩展至回归分析。 SVM寻找超平面的公式为： 其对偶问题为： 将其表示为矩阵形式可变换为： 其中C&gt;0为上界，e是数值全为1的行向量，Q是l*l的半正定矩阵，Qij=yi*yj*K(i,j)，K(i,j)=φ(Xi)^T*φ(Xj)为核函数。 当然，这只是LIBSVM中的C-SVC的目标公式，LIBSVM采用的是更加通用的目标公式： 其中p是长度为l的行向量，△为常数。 其求导为： 于是令aij = Kii+Kjj-2*Kij，假设选定的working set B为i和j，将其带入上式。（见文献[2]的Algorithm 1）当aij&gt;0时得： 当aij≤0时，约束同上式，令： 上式加的一项看似复杂，其实就是函数select_working_set中写的，当aij小于0时令其等于一个很小很小的值。 工作集i,j的选择参见select_working_set函数的讲解。 αi,αj的更新参考文献[2]的“5 Unbalanced Data”。 令： 则问题： 可转化为问题： 进而求解出di,dj可以得到更新后的α，即： 其中： 以不考虑非均衡样本为例(即Cp=Cn)。当yi≠yj时： 当yi=yj时： 梯度G的更新G[α(k)] = Q[α(k)] + pG[α(k+1)] = Q[α(k+1)] + p则：G[α(k+1)] = G[α(k)] + Q[α(k+1)-α(k)] G_bar的更新G_bar[i] = {C * sum(Q[i,j]) while α[j]=C} i = 1,2,3,… l因此，若α更新前后状态(alpha_status)不变，如都为C或都小于C，则G_bar不变。否则：①迭代前不为C，迭代后为C，则：G_bar(k+1)[i] = {C * sum(Q[i,j]) while α[j]=C}②迭代前为C，迭代后不为C，则：G_bar(k+1)[i] = G_bar(k)[i] - {C * sum(Q[i,j]) while α[j]=C} 下面我们开始看代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284void Solver::Solve(int l, const QMatrix&amp; Q, const double *p_, const schar *y_, double *alpha_, double Cp, double Cn, double eps, SolutionInfo* si, int shrinking)&#123; this-&gt;l = l; this-&gt;Q = &amp;Q; QD = Q.get_QD(); clone(p, p_, l); clone(y, y_, l); clone(alpha, alpha_, l); this-&gt;Cp = Cp; this-&gt;Cn = Cn; this-&gt;eps = eps; unshrink = false; // initialize alpha_status &#123; alpha_status = new char[l]; for (int i = 0; i&lt;l; i++) update_alpha_status(i); &#125; // initialize active set (for shrinking) &#123; active_set = new int[l]; for (int i = 0; i&lt;l; i++) active_set[i] = i; active_size = l; &#125; // initialize gradient，根据梯度定义公式进行初始化 &#123; G = new double[l]; G_bar = new double[l]; int i; for (i = 0; i&lt;l; i++) &#123; G[i] = p[i]; G_bar[i] = 0; &#125; for (i = 0; i&lt;l; i++) if (!is_lower_bound(i)) &#123; const Qfloat *Q_i = Q.get_Q(i, l); double alpha_i = alpha[i]; int j; for (j = 0; j&lt;l; j++) G[j] += alpha_i * Q_i[j]; if (is_upper_bound(i)) for (j = 0; j&lt;l; j++) G_bar[j] += get_C(i) * Q_i[j]; &#125; &#125; // optimization step int iter = 0; int max_iter = max(10000000, l&gt;INT_MAX / 100 ? INT_MAX : 100 * l); int counter = min(l, 1000) + 1; while (iter &lt; max_iter) &#123; // show progress and do shrinking if (--counter == 0) &#123; counter = min(l, 1000); if (shrinking) do_shrinking(); info("do shrinking.\n"); &#125; int i, j; if (select_working_set(i, j) != 0) &#123; // reconstruct the whole gradient reconstruct_gradient(); // reset active set size and check active_size = l; info("reconstruct G*\n"); if (select_working_set(i, j) != 0) &#123; info("======break======"); break; &#125; else counter = 1; // do shrinking next iteration &#125; ++iter; // update alpha[i] and alpha[j], handle bounds carefully const Qfloat *Q_i = Q.get_Q(i, active_size); const Qfloat *Q_j = Q.get_Q(j, active_size); double C_i = get_C(i); double C_j = get_C(j); double old_alpha_i = alpha[i]; double old_alpha_j = alpha[j]; if (y[i] != y[j]) //# yi,yj异号 &#123; double quad_coef = QD[i] + QD[j] + 2 * Q_i[j]; //最后一个为+号因为Qij为kij*y[i]*y[j] if (quad_coef &lt;= 0) quad_coef = TAU; double delta = (-G[i] - G[j]) / quad_coef; //alpha改变量 double diff = alpha[i] - alpha[j]; //根据此项判断alpha(i)-alpha(j)=constant与约束框(0~c)的交点 alpha[i] += delta; alpha[j] += delta; if (diff &gt; 0) &#123; if (alpha[j] &lt; 0) &#123; alpha[j] = 0; alpha[i] = diff; &#125; &#125; else &#123; if (alpha[i] &lt; 0) &#123; alpha[i] = 0; alpha[j] = -diff; &#125; &#125; if (diff &gt; C_i - C_j) &#123; if (alpha[i] &gt; C_i) &#123; alpha[i] = C_i; alpha[j] = C_i - diff; &#125; &#125; else &#123; if (alpha[j] &gt; C_j) &#123; alpha[j] = C_j; alpha[i] = C_j + diff; &#125; &#125; &#125; else &#123; double quad_coef = QD[i] + QD[j] - 2 * Q_i[j]; if (quad_coef &lt;= 0) quad_coef = TAU; double delta = (G[i] - G[j]) / quad_coef; double sum = alpha[i] + alpha[j]; alpha[i] -= delta; alpha[j] += delta; if (sum &gt; C_i) &#123; if (alpha[i] &gt; C_i) &#123; alpha[i] = C_i; alpha[j] = sum - C_i; &#125; &#125; else &#123; if (alpha[j] &lt; 0) &#123; alpha[j] = 0; alpha[i] = sum; &#125; &#125; if (sum &gt; C_j) &#123; if (alpha[j] &gt; C_j) &#123; alpha[j] = C_j; alpha[i] = sum - C_j; &#125; &#125; else &#123; if (alpha[i] &lt; 0) &#123; alpha[i] = 0; alpha[j] = sum; &#125; &#125; &#125; // update G double delta_alpha_i = alpha[i] - old_alpha_i; double delta_alpha_j = alpha[j] - old_alpha_j; for (int k = 0; k&lt;active_size; k++) &#123; G[k] += Q_i[k] * delta_alpha_i + Q_j[k] * delta_alpha_j; &#125; // update alpha_status and G_bar &#123; bool ui = is_upper_bound(i); bool uj = is_upper_bound(j); update_alpha_status(i); update_alpha_status(j); int k; if (ui != is_upper_bound(i)) &#123; Q_i = Q.get_Q(i, l); if (ui) for (k = 0; k&lt;l; k++) G_bar[k] -= C_i * Q_i[k]; else for (k = 0; k&lt;l; k++) G_bar[k] += C_i * Q_i[k]; &#125; if (uj != is_upper_bound(j)) &#123; Q_j = Q.get_Q(j, l); if (uj) for (k = 0; k&lt;l; k++) G_bar[k] -= C_j * Q_j[k]; else for (k = 0; k&lt;l; k++) G_bar[k] += C_j * Q_j[k]; &#125; &#125; printf("i:%d, j:%d, alpha_i:%f, alpha_j:%f\n", i, j, alpha[i], alpha[j]); &#125; if (iter &gt;= max_iter) &#123; if (active_size &lt; l) &#123; // reconstruct the whole gradient to calculate objective value reconstruct_gradient(); active_size = l; info("*"); &#125; fprintf(stderr, "\nWARNING: reaching max number of iterations\n"); &#125; // calculate rho si-&gt;rho = calculate_rho(); // calculate objective value &#123; double v = 0; int i; for (i = 0; i&lt;l; i++) v += alpha[i] * (G[i] + p[i]); si-&gt;obj = v / 2; //目标值为(alpha^T*Q*alpha + p^T*alpha) &#125; // put back the solution &#123; for (int i = 0; i&lt;l; i++) alpha_[active_set[i]] = alpha[i]; &#125; // juggle everything back /*&#123; for(int i=0;i&lt;l;i++) while(active_set[i] != i) swap_index(i,active_set[i]); // or Q.swap_index(i,active_set[i]); &#125;*/ si-&gt;upper_bound_p = Cp; si-&gt;upper_bound_n = Cn; info("\noptimization finished, #iter = %d\n", iter); /*for (int g = 0; g &lt; l/2; g++) &#123; printf("alpha_%d:%f\n", g, (alpha[g]-alpha[g+l/2])); &#125;*/ delete[] p; delete[] y; delete[] alpha; delete[] alpha_status; delete[] active_set; delete[] G; delete[] G_bar;&#125; virtual double calculate_rho();该函数用于计算判别函数中的b(rho为b的相反数)，参考文献[2]的3.6。这里仅写出结果：当yi=1时：假设0&lt;αi&lt;C，则r1 = G[α](i)为避免出现数值错误，一般将其写成平均值：如果没有这样的αi，则r1必须满足：此时将ri取为范围中点。当yi=-1时，计算过程类似，得到r2。 得到r1、r2后，通过计算得到： 12345678910111213141516171819202122232425262728293031323334353637double Solver::calculate_rho()&#123; double r; int nr_free = 0; double ub = INF, lb = -INF, sum_free = 0; for (int i = 0; i&lt;active_size; i++) &#123; double yG = y[i] * G[i]; if (is_upper_bound(i)) &#123; if (y[i] == -1) ub = min(ub, yG); else lb = max(lb, yG); &#125; else if (is_lower_bound(i)) &#123; if (y[i] == +1) ub = min(ub, yG); else lb = max(lb, yG); &#125; else &#123; ++nr_free; sum_free += yG; &#125; &#125; if (nr_free&gt;0) r = sum_free / nr_free; else r = (ub + lb) / 2; return r;&#125; SVR_Q类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374class SVR_Q : public Kernel&#123;public: SVR_Q(const svm_problem&amp; prob, const svm_parameter&amp; param) :Kernel(prob.l, prob.x, param) &#123; l = prob.l; cache = new Cache(l, (long int)(param.cache_size*(1 &lt;&lt; 20))); QD = new double[2 * l]; sign = new schar[2 * l]; index = new int[2 * l]; for (int k = 0; k&lt;l; k++) &#123; sign[k] = 1; sign[k + l] = -1; index[k] = k; index[k + l] = k; QD[k] = (this-&gt;*kernel_function)(k, k); QD[k + l] = QD[k]; &#125; buffer[0] = new Qfloat[2 * l]; buffer[1] = new Qfloat[2 * l]; next_buffer = 0; &#125; void swap_index(int i, int j) const &#123; swap(sign[i], sign[j]); swap(index[i], index[j]); swap(QD[i], QD[j]); &#125; Qfloat *get_Q(int i, int len) const &#123; Qfloat *data; int j, real_i = index[i]; if (cache-&gt;get_data(real_i, &amp;data, l) &lt; l) &#123; for (j = 0; j&lt;l; j++) data[j] = (Qfloat)(this-&gt;*kernel_function)(real_i, j); &#125; // reorder and copy Qfloat *buf = buffer[next_buffer]; next_buffer = 1 - next_buffer; schar si = sign[i]; for (j = 0; j&lt;len; j++) buf[j] = (Qfloat)si * (Qfloat)sign[j] * data[index[j]]; return buf; &#125; double *get_QD() const &#123; return QD; &#125; ~SVR_Q() &#123; delete cache; delete[] sign; delete[] index; delete[] buffer[0]; delete[] buffer[1]; delete[] QD; &#125;private: int l; Cache *cache; schar *sign; int *index; mutable int next_buffer; Qfloat *buffer[2]; double *QD;&#125;; 成员变量主要参数解释如下： Parameter Interpretation *sign 同SVC_Q中的y，即为公式中的y。sign[i]=1,sign[i+l]=-1,i=1,2,3,…,l 成员函数SVR_Q(const svm_problem&amp; prob, const svm_parameter&amp; param):Kernel(prob.l, prob.x, param);初始化有关SVR的计算参数。与SVC不同的是优化公式中的y并不是SVR样本数据的目标值，优化公式中的l为两倍的SVR数据样本数量，详见solve_epsilon_svr函数解析。 1234567891011121314151617181920212223242526SVR_Q(const svm_problem&amp; prob, const svm_parameter&amp; param) :Kernel(prob.l, prob.x, param) &#123; l = prob.l; //l为样本数据的数量 cache = new Cache(l, (long int)(param.cache_size*(1 &lt;&lt; 20))); //对于SVR而言需要开辟2*l的空间 QD = new double[2 * l]; sign = new schar[2 * l]; index = new int[2 * l]; for (int k = 0; k&lt;l; k++) &#123; //sign[i]=1,sign[i+l]=-1,i=1,2,3,...,l sign[k] = 1; sign[k + l] = -1; index[k] = k; index[k + l] = k; QD[k] = (this-&gt;*kernel_function)(k, k); QD[k + l] = QD[k]; &#125; buffer[0] = new Qfloat[2 * l]; buffer[1] = new Qfloat[2 * l]; next_buffer = 0; &#125; Qfloat *get_Q(int i, int len) const;计算SVR公式中所使用的Q[i]，此处为第i列，不过一般而言Q[i,j]=Q[j,i]。 123456789101112131415161718Qfloat *get_Q(int i, int len) const&#123; Qfloat *data; int j, real_i = index[i]; if (cache-&gt;get_data(real_i, &amp;data, l) &lt; l) &#123; for (j = 0; j&lt;l; j++) data[j] = (Qfloat)(this-&gt;*kernel_function)(real_i, j); //计算得到K[i,j] &#125; // reorder and copy Qfloat *buf = buffer[next_buffer]; next_buffer = 1 - next_buffer; schar si = sign[i]; for (j = 0; j&lt;len; j++) buf[j] = (Qfloat)si * (Qfloat)sign[j] * data[index[j]]; //为了与Solver类中的公式相匹配，此处定义Q[i,j]=sign[i]*sign[j]*K[i,j] return buf;&#125; static void solve_epsilon_svr(const svm_problem *prob, const svm_parameter *param, double *alpha, Solver::SolutionInfo* si)该函数用于计算优化公式中的p，并定义Solver中的y与α，调用Solver类。 epsilon-SVR原始公式为： 其对偶式为： 其中： 决策函数为： 将其化为矩阵形式： 其中y为2l*1的矩阵，yt=1，t=1,…,l; yt=-1，t=l+1,…,2l. 将上式与前述的通用目标公式相比较，记上标t为通用公式的参数，则可知： 12345678910111213141516171819202122232425262728293031323334353637static void solve_epsilon_svr( const svm_problem *prob, const svm_parameter *param, double *alpha, Solver::SolutionInfo* si)&#123; int l = prob-&gt;l; double *alpha2 = new double[2 * l]; double *linear_term = new double[2 * l]; schar *y = new schar[2 * l]; //新定义了y值，对应Solver的y int i; for (i = 0; i&lt;l; i++) &#123; alpha2[i] = 0; linear_term[i] = param-&gt;p - prob-&gt;y[i]; //epsilon*e-z,epsilon为间隔带，e为全为1的行向量，z为样本数据的目标值 y[i] = 1; alpha2[i + l] = 0; linear_term[i + l] = param-&gt;p + prob-&gt;y[i]; //epsilon*e+z y[i + l] = -1; &#125; Solver s; s.Solve(2 * l, SVR_Q(*prob, *param), linear_term, y, alpha2, param-&gt;C, param-&gt;C, param-&gt;eps, si, param-&gt;shrinking); double sum_alpha = 0; for (i = 0; i&lt;l; i++) &#123; alpha[i] = alpha2[i] - alpha2[i + l]; //将alpha[i]-alpha[i+1]得到数据样本x前的最终系数 sum_alpha += fabs(alpha[i]); &#125; info("nu = %f\n", sum_alpha / (param-&gt;C*l)); delete[] alpha2; delete[] linear_term; delete[] y;&#125; static decision_function svm_train_one(const svm_problem *prob, const svm_parameter *param, double Cp, double Cn)根据kernel_type的不同调用不同的求解函数，并计算支持向量的个数与处于边界的支持向量个数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556static decision_function svm_train_one( const svm_problem *prob, const svm_parameter *param, double Cp, double Cn)&#123; double *alpha = Malloc(double, prob-&gt;l); Solver::SolutionInfo si; switch (param-&gt;svm_type) &#123; case C_SVC: solve_c_svc(prob, param, alpha, &amp;si, Cp, Cn); break; case NU_SVC: solve_nu_svc(prob, param, alpha, &amp;si); break; case ONE_CLASS: solve_one_class(prob, param, alpha, &amp;si); break; case EPSILON_SVR: solve_epsilon_svr(prob, param, alpha, &amp;si); break; case NU_SVR: solve_nu_svr(prob, param, alpha, &amp;si); break; &#125; info("obj = %f, rho = %f\n", si.obj, si.rho); // output SVs int nSV = 0; int nBSV = 0; for (int i = 0; i&lt;prob-&gt;l; i++) &#123; if (fabs(alpha[i]) &gt; 0) &#123; ++nSV; if (prob-&gt;y[i] &gt; 0) &#123; if (fabs(alpha[i]) &gt;= si.upper_bound_p) ++nBSV; &#125; else &#123; if (fabs(alpha[i]) &gt;= si.upper_bound_n) ++nBSV; &#125; &#125; &#125; info("nSV = %d, nBSV = %d\n", nSV, nBSV); decision_function f; f.alpha = alpha; f.rho = si.rho; return f;&#125; svm_model *svm_train(const svm_problem *prob, const svm_parameter *param)；根据不同svm_type开辟不同空间，最后返回训练好的svm model。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245svm_model *svm_train(const svm_problem *prob, const svm_parameter *param)&#123; svm_model *model = Malloc(svm_model, 1); model-&gt;param = *param; model-&gt;free_sv = 0; // XXX if (param-&gt;svm_type == ONE_CLASS || param-&gt;svm_type == EPSILON_SVR || param-&gt;svm_type == NU_SVR) &#123; // regression or one-class-svm model-&gt;nr_class = 2; model-&gt;label = NULL; model-&gt;nSV = NULL; model-&gt;probA = NULL; model-&gt;probB = NULL; model-&gt;sv_coef = Malloc(double *, 1); /*if (param-&gt;probability &amp;&amp; (param-&gt;svm_type == EPSILON_SVR || param-&gt;svm_type == NU_SVR)) &#123; model-&gt;probA = Malloc(double, 1); model-&gt;probA[0] = svm_svr_probability(prob, param); &#125;*/ decision_function f = svm_train_one(prob, param, 0, 0); model-&gt;rho = Malloc(double, 1); model-&gt;rho[0] = f.rho; int nSV = 0; int i; for (i = 0; i&lt;prob-&gt;l; i++) if (fabs(f.alpha[i]) &gt; 0) ++nSV; model-&gt;l = nSV; model-&gt;SV = Malloc(svm_node *, nSV); model-&gt;sv_coef[0] = Malloc(double, nSV); model-&gt;sv_indices = Malloc(int, nSV); int j = 0; for (i = 0; i&lt;prob-&gt;l; i++) if (fabs(f.alpha[i]) &gt; 0) &#123; model-&gt;SV[j] = prob-&gt;x[i]; model-&gt;sv_coef[0][j] = f.alpha[i]; model-&gt;sv_indices[j] = i + 1; ++j; &#125; free(f.alpha); &#125; else &#123; // classification int l = prob-&gt;l; int nr_class; int *label = NULL; int *start = NULL; int *count = NULL; int *perm = Malloc(int, l); // group training data of the same class svm_group_classes(prob, &amp;nr_class, &amp;label, &amp;start, &amp;count, perm); if (nr_class == 1) info("WARNING: training data in only one class. See README for details.\n"); svm_node **x = Malloc(svm_node *, l); int i; for (i = 0; i&lt;l; i++) x[i] = prob-&gt;x[perm[i]]; // calculate weighted C double *weighted_C = Malloc(double, nr_class); for (i = 0; i&lt;nr_class; i++) weighted_C[i] = param-&gt;C; for (i = 0; i&lt;param-&gt;nr_weight; i++) &#123; int j; for (j = 0; j&lt;nr_class; j++) if (param-&gt;weight_label[i] == label[j]) break; if (j == nr_class) fprintf(stderr, "WARNING: class label %d specified in weight is not found\n", param-&gt;weight_label[i]); else weighted_C[j] *= param-&gt;weight[i]; &#125; // train k*(k-1)/2 models bool *nonzero = Malloc(bool, l); for (i = 0; i&lt;l; i++) nonzero[i] = false; decision_function *f = Malloc(decision_function, nr_class*(nr_class - 1) / 2); double *probA = NULL, *probB = NULL; if (param-&gt;probability) &#123; probA = Malloc(double, nr_class*(nr_class - 1) / 2); probB = Malloc(double, nr_class*(nr_class - 1) / 2); &#125; int p = 0; for (i = 0; i&lt;nr_class; i++) for (int j = i + 1; j&lt;nr_class; j++) &#123; svm_problem sub_prob; int si = start[i], sj = start[j]; int ci = count[i], cj = count[j]; sub_prob.l = ci + cj; sub_prob.x = Malloc(svm_node *, sub_prob.l); sub_prob.y = Malloc(double, sub_prob.l); int k; for (k = 0; k&lt;ci; k++) &#123; sub_prob.x[k] = x[si + k]; sub_prob.y[k] = +1; &#125; for (k = 0; k&lt;cj; k++) &#123; sub_prob.x[ci + k] = x[sj + k]; sub_prob.y[ci + k] = -1; &#125; if (param-&gt;probability) svm_binary_svc_probability(&amp;sub_prob, param, weighted_C[i], weighted_C[j], probA[p], probB[p]); f[p] = svm_train_one(&amp;sub_prob, param, weighted_C[i], weighted_C[j]); for (k = 0; k&lt;ci; k++) if (!nonzero[si + k] &amp;&amp; fabs(f[p].alpha[k]) &gt; 0) nonzero[si + k] = true; for (k = 0; k&lt;cj; k++) if (!nonzero[sj + k] &amp;&amp; fabs(f[p].alpha[ci + k]) &gt; 0) nonzero[sj + k] = true; free(sub_prob.x); free(sub_prob.y); ++p; &#125; // build output model-&gt;nr_class = nr_class; model-&gt;label = Malloc(int, nr_class); for (i = 0; i&lt;nr_class; i++) model-&gt;label[i] = label[i]; model-&gt;rho = Malloc(double, nr_class*(nr_class - 1) / 2); for (i = 0; i&lt;nr_class*(nr_class - 1) / 2; i++) model-&gt;rho[i] = f[i].rho; if (param-&gt;probability) &#123; model-&gt;probA = Malloc(double, nr_class*(nr_class - 1) / 2); model-&gt;probB = Malloc(double, nr_class*(nr_class - 1) / 2); for (i = 0; i&lt;nr_class*(nr_class - 1) / 2; i++) &#123; model-&gt;probA[i] = probA[i]; model-&gt;probB[i] = probB[i]; &#125; &#125; else &#123; model-&gt;probA = NULL; model-&gt;probB = NULL; &#125; int total_sv = 0; int *nz_count = Malloc(int, nr_class); model-&gt;nSV = Malloc(int, nr_class); for (i = 0; i&lt;nr_class; i++) &#123; int nSV = 0; for (int j = 0; j&lt;count[i]; j++) if (nonzero[start[i] + j]) &#123; ++nSV; ++total_sv; &#125; model-&gt;nSV[i] = nSV; nz_count[i] = nSV; &#125; info("Total nSV = %d\n", total_sv); model-&gt;l = total_sv; model-&gt;SV = Malloc(svm_node *, total_sv); model-&gt;sv_indices = Malloc(int, total_sv); p = 0; for (i = 0; i&lt;l; i++) if (nonzero[i]) &#123; model-&gt;SV[p] = x[i]; model-&gt;sv_indices[p++] = perm[i] + 1; &#125; int *nz_start = Malloc(int, nr_class); nz_start[0] = 0; for (i = 1; i&lt;nr_class; i++) nz_start[i] = nz_start[i - 1] + nz_count[i - 1]; model-&gt;sv_coef = Malloc(double *, nr_class - 1); for (i = 0; i&lt;nr_class - 1; i++) model-&gt;sv_coef[i] = Malloc(double, total_sv); p = 0; for (i = 0; i&lt;nr_class; i++) for (int j = i + 1; j&lt;nr_class; j++) &#123; // classifier (i,j): coefficients with // i are in sv_coef[j-1][nz_start[i]...], // j are in sv_coef[i][nz_start[j]...] int si = start[i]; int sj = start[j]; int ci = count[i]; int cj = count[j]; int q = nz_start[i]; int k; for (k = 0; k&lt;ci; k++) if (nonzero[si + k]) model-&gt;sv_coef[j - 1][q++] = f[p].alpha[k]; q = nz_start[j]; for (k = 0; k&lt;cj; k++) if (nonzero[sj + k]) model-&gt;sv_coef[i][q++] = f[p].alpha[ci + k]; ++p; &#125; free(label); free(probA); free(probB); free(count); free(perm); free(start); free(x); free(weighted_C); free(nonzero); for (i = 0; i&lt;nr_class*(nr_class - 1) / 2; i++) free(f[i].alpha); free(f); free(nz_count); free(nz_start); &#125; return model;&#125; double svm_predict_values(const svm_model *model, const svm_node *x, double* dec_values)该函数用于预测单个测试样本数据，因此对于一组测试样本需要调用n次。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475double svm_predict_values(const svm_model *model, const svm_node *x, double* dec_values)&#123; int i; if (model-&gt;param.svm_type == ONE_CLASS || model-&gt;param.svm_type == EPSILON_SVR || model-&gt;param.svm_type == NU_SVR) &#123; double *sv_coef = model-&gt;sv_coef[0]; double sum = 0; for (i = 0; i&lt;model-&gt;l; i++) sum += sv_coef[i] * Kernel::k_function(x, model-&gt;SV[i], model-&gt;param); //对应决策公式的前半部分，即αi*K(xi,x) sum -= model-&gt;rho[0]; //加上决策函数的常数项 *dec_values = sum; if (model-&gt;param.svm_type == ONE_CLASS) return (sum&gt;0) ? 1 : -1; else return sum; &#125; else &#123; int nr_class = model-&gt;nr_class; int l = model-&gt;l; double *kvalue = Malloc(double, l); for (i = 0; i&lt;l; i++) kvalue[i] = Kernel::k_function(x, model-&gt;SV[i], model-&gt;param); int *start = Malloc(int, nr_class); start[0] = 0; for (i = 1; i&lt;nr_class; i++) start[i] = start[i - 1] + model-&gt;nSV[i - 1]; int *vote = Malloc(int, nr_class); for (i = 0; i&lt;nr_class; i++) vote[i] = 0; int p = 0; for (i = 0; i&lt;nr_class; i++) for (int j = i + 1; j&lt;nr_class; j++) &#123; double sum = 0; int si = start[i]; int sj = start[j]; int ci = model-&gt;nSV[i]; int cj = model-&gt;nSV[j]; int k; double *coef1 = model-&gt;sv_coef[j - 1]; double *coef2 = model-&gt;sv_coef[i]; for (k = 0; k&lt;ci; k++) sum += coef1[si + k] * kvalue[si + k]; for (k = 0; k&lt;cj; k++) sum += coef2[sj + k] * kvalue[sj + k]; sum -= model-&gt;rho[p]; dec_values[p] = sum; if (dec_values[p] &gt; 0) ++vote[i]; else ++vote[j]; p++; &#125; int vote_max_idx = 0; for (i = 1; i&lt;nr_class; i++) if (vote[i] &gt; vote[vote_max_idx]) vote_max_idx = i; free(kvalue); free(start); free(vote); return model-&gt;label[vote_max_idx]; &#125;&#125; Reference:[1] Smola A J, Schölkopf B. A tutorial on support vector regression[J]. Statistics &amp; Computing, 2004, volume 14(3):199-222(24).[2] Chang C C, Lin C J. LIBSVM: A library for support vector machines[M]. ACM, 2011.[3] Fan R E, Chen P H, Lin C J, et al. Working Set Selection Using Second Order Information for Training Support Vector Machines[J]. Journal of Machine Learning Research, 2005, 6(4):1889-1918.[4] Svm O F. Sequential Minimal Optimization for SVM[J]. 2007.[5] LibSVM中select_working_set函数：http://blog.csdn.net/le_zhou/article/details/40505465[6] libsvm最新源代码（版本3.21）理解解析（三）：http://blog.csdn.net/xiaoqiangqiangjie/article/details/53886907[7] LibSVM源码剖析（java版）：http://makaidong.com/bentuwuying/21760_40631.html[8] LibSVM-2.6 程序代码注释,上交]]></content>
      <categories>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>Machine learning</tag>
        <tag>Artificial Intelligence</tag>
        <tag>Support Vector Machine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[P问题、NP问题、NPC问题、NP-hard问题详解]]></title>
    <url>%2F2017%2F09%2F13%2FP%E9%97%AE%E9%A2%98%E3%80%81NP%E9%97%AE%E9%A2%98%E3%80%81NPC%E9%97%AE%E9%A2%98%E3%80%81NP-hard%E9%97%AE%E9%A2%98%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[要理解P问题、NP问题、NPC问题、NP-hard问题，需要先弄懂几个概念： 什么是多项式时间？ 什么是确定性算法？什么是非确定性算法？ 什么是规约/约化？ 多项式时间（Polynomial time）什么是时间复杂度？ 时间复杂度并不是表示一个程序解决问题需要花多少时间，而是当程序所处理的问题规模扩大后，程序需要的时间长度对应增长得有多快。也就是说，对于某一个程序，其处理某一个特定数据的效率不能衡量该程序的好坏，而应该看当这个数据的规模变大到数百倍后，程序运行时间是否还是一样，或者也跟着慢了数百倍，或者变慢了数万倍。 不管数据有多大，程序处理所花的时间始终是那么多的，我们就说这个程序很好，具有$O(1)$的时间复杂度，也称常数级复杂度；数据规模变得有多大，花的时间也跟着变得有多长，比如找n个数中的最大值这个程序的时间复杂度就是$O(n)$，为线性级复杂度，而像冒泡排序、插入排序等，数据扩大2倍，时间变慢4倍的，时间复杂度是$O(n^2)$，为平方级复杂度。还有一些穷举类的算法，所需时间长度成几何阶数上涨，这就是$O(a^n)$的指数级复杂度，甚至$O(n!)$的阶乘级复杂度。 不会存在 $O(2*n^2)$ 的复杂度，因为前面的那个”2”是系数，根本不会影响到整个程序的时间增长。同样地，$O(n^3+n^2)$ 的复杂度也就是$O(n^3)$的复杂度。因此，我们会说，一个$O(0.01*n^3)$的程序的效率比O(100*n^2)的效率低，尽管在n很小的时候，前者优于后者，但后者时间随数据规模增长得慢，最终$O(n^3)$的复杂度将远远超过$O(n^2)$。我们也说，$O(n^{100})$的复杂度小于$O(1.01^n)$的复杂度。 容易看出，前面的几类复杂度被分为两种级别，其中后者的复杂度无论如何都远远大于前者。像$O(1)$,$O(\ln(n))$,$O(n^a)$等，我们把它叫做多项式级复杂度，因为它的规模n出现在底数的位置；另一种像是$O(a^n)$和$O(n!)$等，它是非多项式级的复杂度，其复杂度计算机往往不能承受。当我们在解决一个问题时，我们选择的算法通常都需要是多项式级的复杂度，非多项式级的复杂度需要的时间太多，往往会超时，除非是数据规模非常小。 确定性算法与非确定性算法确定性算法：设A是求解问题B的一个解决算法，在算法的整个执行过程中，每一步都能得到一个确定的解，这样的算法就是确定性算法。 非确定性算法：设A是求解问题B的一个解决算法，它将问题分解成两部分，分别为猜测阶段和验证阶段，其中 猜测阶段：在这个阶段，对问题的一个特定的输入实例x产生一个任意字符串y，在算法的每一次运行时，y的值可能不同，因此，猜测以一种非确定的形式工作。 验证阶段：在这个阶段，用一个确定性算法（有限时间内）验证。①检查在猜测阶段产生的y是否是合适的形式，如果不是，则算法停下来并得到no；② 如果y是合适的形式，则验证它是否是问题的解，如果是，则算法停下来并得到yes，否则算法停下来并得到no。它是验证所猜测的解的正确性。 规约/约化问题A可以约化为问题B，称为“问题A可规约为问题B”，可以理解为问题B的解一定就是问题A的解，因此解决A不会难于解决B。由此可知问题B的时间复杂度一定大于等于问题A。 《算法导论》中有一个例子：现在有两个问题：求解一个一元一次方程和求解一个一元二次方程。那么我们说，前者可以规约为后者，意即知道如何解一个一元二次方程那么一定能解出一元一次方程。我们可以写出两个程序分别对应两个问题，那么我们能找到一个“规则”，按照这个规则把解一元一次方程程序的输入数据变一下，用在解一元二次方程的程序上，两个程序总能得到一样的结果。这个规则即是：两个方程的对应项系数不变，一元二次方程的二次项系数为0。 从规约的定义中我们看到，一个问题规约为另一个问题，时间复杂度增加了，问题的应用范围也增大了。通过对某些问题的不断规约，我们能够不断寻找复杂度更高，但应用范围更广的算法来代替复杂度虽然低，但只能用于很小的一类问题的算法。存在这样一个NP问题，所有的NP问题都可以约化成它。换句话说，只要解决了这个问题，那么所有的NP问题都解决了。这种问题的存在难以置信，并且更加不可思议的是，这种问题不只一个，它有很多个，它是一类问题。这一类问题就是传说中的NPC问题，也就是NP-完全问题。 P类问题、NP类问题、NPC问题、NP难问题 P类问题：能在多项式时间内可解的问题。 NP类问题：在多项式时间内“可验证”的问题。也就是说，不能判定这个问题到底有没有解，而是猜出一个解来在多项式时间内证明这个解是否正确。即该问题的猜测过程是不确定的，而对其某一个解的验证则能够在多项式时间内完成。P类问题属于NP问题，但NP类问题不一定属于P类问题。 NPC问题：存在这样一个NP问题，所有的NP问题都可以约化成它。换句话说，只要解决了这个问题，那么所有的NP问题都解决了。其定义要满足2个条件： 它是一个NP问题； 所有NP问题都能规约到它。 NP难问题：NP-Hard问题是这样一种问题，它满足NPC问题定义的第二条但不一定要满足第一条（就是说，NP-Hard问题要比 NPC问题的范围广，NP-Hard问题没有限定属于NP），即所有的NP问题都能约化到它，但是他不一定是一个NP问题。NP-Hard问题同样难以找到多项式的算法，但它不列入我们的研究范围，因为它不一定是NP问题。即使NPC问题发现了多项式级的算法，NP-Hard问题有可能仍然无法得到多项式级的算法。事实上，由于NP-Hard放宽了限定条件，它将有可能比所有的NPC问题的时间复杂度更高从而更难以解决。以上四个问题之间的关系如下图所示： P=NP？ 此处会再次从不同的角度来讨论P与NP的定义。 “P=NP?” 通常被认为是计算机科学最重要的问题。在很早的时候，就有个数学家毫不客气的指出，P=NP? 是个愚蠢的问题，并且为了嘲笑它，专门在4月1号写了一篇“论文”，称自己证明了 P=NP。 首先，我们要搞清楚什么是“P=NP?” 为此，我们必须先了解一下什么是“算法复杂度”。为此，我们又必须先了解什么是“算法”。我们可以简单的把“算法”想象成一台机器，就跟绞肉机似的。我们给它一些“输入”，它就给我们一些“输出”。比如，绞肉机的输入是肉末，输出是肉渣。牛的输入是草，输出是奶。“加法器”的输入是两个整数，输出是这两个整数的和。“算法理论”所讨论的问题，就是如何设计这些机器，让它们更加有效的工作。就像是说如何培育出优质的奶牛，吃进相同数量的草，更快的产出更多的奶。 世界上的计算问题，都需要“算法”经过一定时间的工作（也叫“计算”），才能得到结果。计算所需要的时间，往往跟“输入”的大小有关系。你的牛吃越是多的草，它就需要越是长时间才能把它们都变成奶。这种草和奶的转换速度，通常被叫做“算法复杂度”。算法复杂度通常被表示为一个函数f(n)，其中n是输入的大小。比如，如果我们的算法复杂度为n^2，那么当输入10个东西的时候，它需要100个单元的时间才能完成计算。当输入100 个东西的时候，它需要10000个单元的时间才能完成。当输入1000个数据的时候，它需要1000000个单元的时间。所谓的“P时间”，多项式时间，就是说这个复杂度函数f(n)是一个多项式。“P=NP?”中的“P”，就是指所有这些复杂度为多项式的算法的“集合”，也就是“所有”的复杂度为多项式的算法。为了简要的描述以下的内容，我定义一些术语： “f(n)时间算法”=“能够在f(n)时间之内，解决某个问题的算法” 当f(n)是个多项式（比如$n^2$）的时候，这就是“多项式时间算法”（P时间算法）。当f(n)是个指数函数（比如$2^n$）的时候，这就是“指数时间算法”（EXPTIME算法）。很多人认为NP问题就是需要指数时间的问题，而NP跟EXPTIME，其实是风马牛不相及的。很显然，P不等于EXPTIME，但是P是否等于NP，却没有一个结论。 现在我来解释一下什么是NP。通常的计算机，都是确定性（deterministic）的。它们在同一个时刻，只有一种行为。如果用程序来表示，那么它们遇到一个条件判断（分支）的时候，只能一次探索其中一条路径。比如： 123456if (x == 0) &#123; one();&#125;else &#123; two();&#125; 在这里，根据x的值是否为零，one()和two()这两个操作，只有一个会发生。然而，有人幻想出来一种机器，叫做“非确定性计算机”（nondeterministic computer），它可以同时运行这程序的两个分支，one()和two()。这有什么用处呢？它的用处就在于，当你不知道x的大小的时候，根据one()和two()是否“运行成功”，你可以推断出x是否为零。这种方式可以同时探索多种可能性。这不是普通的“并行计算”，因为每当遇到一个分支点，非确定性计算机就会产生新的计算单元，用以同时探索这些路径。这机器就像有“分身术”一样。当这种分支点存在于循环（或者递归）里面的时候，它就会反复的产生新的计算单元，新的计算单元又产生更多的计算单元，就跟细胞分裂一样。一般的计算机都没有 这种“超能力”，它们只有固定数目的计算单元。所以他只能先探索一条路径，失败之后，再回过头来探索另外一条。所以，它们似乎要多花一些时间才能得到结果。到这里，基本的概念都有了定义，于是我们可以圆满的给出P和NP的定义。P和NP是这样两个“问题的集合”： P = “确定性计算机”能够在“多项式时间”解决的所有问题NP = “非确定性计算机”能够在“多项式时间”解决的所有问题（注意它们的区别，仅在于“确定性”或者是“非确定性”。） “P=NP?”问题的目标，就是想要知道P和NP这两个集合是否相等。为了证明两个集合（A和 B）相等，一般都要证明两个方向： A 包含 B； B 包含 A。 上一个标题中我们已经说过NP包含了P。因为任何一个非确定性机器，都能被当成一个确定性的机器来用。你只要不使用它的“超能力”，在每个分支点只探索一条路径就行。所以“P=NP?”问题的关键，就在于P是否也包含了NP。也就是说，如果只使用确定性计算机，能否在多项式时间之内，解决所有非确定性计算机能在多项式时间内解决的问题。 我们来细看一下什么是多项式时间（Polynomial time）。我们都知道，$n^2$是多项式，$n^{1000000}$ 也是多项式。多项式与多项式之间，却有天壤之别。把解决问题所需要的时间，用“多项式”这么笼统的概念来描述，其实是非常不准确的做法。在实际的大规模应用中，$n^2$的算法都嫌慢。能找到“多项式时间”的算法，根本不能说明任何问题。对此，理论家们喜欢说，就算再大的多项式(比如 $n^{1000000}$)，也不能和再小的指数函数（比如 $1.0001^n$）相比。因为总是“存在”一个M，当n&gt;M的时候，$1.0001^n$会超过$n^{1000000}$。可是问题的关键，却不在于M的“存在”，而在于它的“大小”。如果你的输入必须达到天文数字才能让指数函数超过多项式的话，那么还不如就用指数复杂度的算法。所以，“P=NP?”这问题的错误就在于，它并没有针对我们的实际需要，而是首先假设了我们有“无穷大”的输入，有“无穷多”的时间和耐心，可以让多项式时间的算法“最终”得到优势。 Reference:[1] 对于“NP难问题”的理解：http://blog.csdn.net/u010021014/article/details/77839858[2] 谈“P=NP?”：http://yinwang0.lofter.com/post/183ec2_4f6312]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>NP-hard</tag>
      </tags>
  </entry>
</search>
